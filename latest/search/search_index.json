{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Sorix","text":"<p>Sorix is a minimalist and high-performance library for Machine Learning and Deep Learning, designed to run neural networks directly on NumPy with minimal resource usage.</p> <p>Inspired by the PyTorch API, Sorix maintains a clear and intuitive interface that allows for rapid adoption without compromising efficiency. Its architecture facilitates a smooth transition from research prototype to production.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p> High Performance</p> <p>Executes optimized neural networks on NumPy with optional GPU acceleration via CuPy.</p> </li> <li> <p> PyTorch-like API</p> <p>Expressive and familiar syntax based on PyTorch design principles, ensuring a short learning curve.</p> </li> <li> <p> Lightweight</p> <p>Ideal for environments with limited computational resources or where low overhead is required.</p> </li> <li> <p> Production Ready</p> <p>Develop models that are ready for real-world deployment without the need to rewrite in other frameworks.</p> </li> </ul>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"#standard-cpu","title":"\ud83d\udcbb Standard (CPU)","text":"<p>For general use on CPU environments.</p> pipPoetryuv <pre><code>pip install sorix\n</code></pre> <pre><code>poetry add sorix\n</code></pre> <pre><code>uv add sorix\n</code></pre>"},{"location":"#gpu-accelerated","title":"\ud83d\ude80 GPU Accelerated","text":"<p>Requires CuPy v13+ and CUDA.</p> pipPoetryuv <pre><code>pip install \"sorix[cp13]\"\n</code></pre> <pre><code>poetry add \"sorix[cp13]\"\n</code></pre> <pre><code>uv add \"sorix[cp13]\"\n</code></pre>"},{"location":"#quick-start","title":"\u26a1 Quick Start","text":""},{"location":"#1-autograd-engine","title":"1. Autograd Engine","text":"<p>Sorix features a simple but powerful autograd engine for automatic differentiation.</p> <pre><code>from sorix import tensor\n\n# Create tensors with gradient tracking\nx = tensor([2.0], requires_grad=True)\nw = tensor([3.0], requires_grad=True)\nb = tensor([1.0], requires_grad=True)\n\n# Define a simple function: y = w*x + b\ny = w * x + b\n\n# Compute gradients via backpropagation\ny.backward()\n\nprint(f\"dy/dx: {x.grad}\") # \u2192 3.0\nprint(f\"dy/dw: {w.grad}\") # \u2192 2.0\n</code></pre>"},{"location":"#2-linear-regression-training-loop","title":"2. Linear Regression (Training Loop)","text":"<p>Building and training models is as intuitive and powerful as in PyTorch.</p> <pre><code>import numpy as np\nfrom sorix import tensor\nfrom sorix.nn import Linear, MSELoss\nfrom sorix.optim import SGD\n\n# 1. Prepare data\nX = np.linspace(-1, 1, 100).reshape(-1, 1)\ny = 3 * X + 2 + 0.1 * np.random.randn(*X.shape)\nX_t, y_t = tensor(X), tensor(y)\n\n# 2. Define model, loss, and optimizer\nmodel = Linear(1, 1)\ncriterion = MSELoss()\noptimizer = SGD(model.parameters(), lr=0.1)\n\n# 3. Training loop\nfor epoch in range(100):\n    y_pred = model(X_t)\n    loss = criterion(y_pred, y_t)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n\n# 4. Final parameters\nprint(f\"Learned: y = {model.W.item():.2f}x + {model.b.item():.2f}\")\n</code></pre>"},{"location":"#explore-the-documentation","title":"\ud83d\udcc2 Explore the Documentation","text":"<ul> <li> <p> Learn Basics</p> <p>Understand Tensors, Graphs and Autograd.</p> <p> Start Learning</p> </li> <li> <p> Examples</p> <p>Real-world models: Linear/Logistic Regression, MNIST, and more.</p> <p> View Examples</p> </li> <li> <p> API Reference</p> <p>Detailed documentation for every class and method.</p> <p> Browse API</p> </li> </ul>"},{"location":"#project-status","title":"\ud83d\udea7 Project Status","text":"<p>Sorix is under active development. We are constantly working on extending key functionalities:</p> <ul> <li>Integration of more essential neural network layers.</li> <li>Optimization of GPU support via CuPy.</li> <li>Extension of the <code>autograd</code> engine.</li> </ul>"},{"location":"#important-links","title":"\ud83d\udd17 Important Links","text":"Resource Link PyPI Package View on PyPI Source Code GitHub Repository"},{"location":"api/","title":"sorix","text":""},{"location":"api/#sorix","title":"sorix","text":""},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>sorix</li> <li>_version</li> <li>clustering<ul> <li>k_means</li> </ul> </li> <li>cuda<ul> <li>cuda</li> </ul> </li> <li>cupy<ul> <li>cupy</li> </ul> </li> <li>metrics<ul> <li>metrics</li> </ul> </li> <li>model_selection<ul> <li>train_test</li> </ul> </li> <li>nn<ul> <li>layers</li> <li>loss</li> <li>net</li> </ul> </li> <li>optim<ul> <li>optim</li> </ul> </li> <li>preprocessing<ul> <li>encoders</li> <li>scalers</li> <li>transformers</li> </ul> </li> <li>tensor</li> <li>utils<ul> <li>data<ul> <li>dataloader</li> <li>dataset</li> </ul> </li> <li>math</li> <li>utils</li> </ul> </li> </ul>"},{"location":"api/_version/","title":"_version","text":""},{"location":"api/_version/#sorix._version","title":"sorix._version","text":""},{"location":"api/tensor/","title":"tensor","text":""},{"location":"api/tensor/#sorix.tensor","title":"sorix.tensor","text":""},{"location":"api/tensor/#sorix.tensor.tensor","title":"tensor","text":"<pre><code>tensor(data, device='cpu', requires_grad=False)\n</code></pre> <p>Factory function to create a Sorix Tensor.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def tensor(data, device='cpu', requires_grad=False):\n    \"\"\"\n    Factory function to create a Sorix Tensor.\n    \"\"\"\n    return Tensor(data, device=device, requires_grad=requires_grad)\n</code></pre>"},{"location":"api/clustering/","title":"Index","text":""},{"location":"api/clustering/#sorix.clustering","title":"sorix.clustering","text":""},{"location":"api/clustering/k_means/","title":"k_means","text":""},{"location":"api/clustering/k_means/#sorix.clustering.k_means","title":"sorix.clustering.k_means","text":""},{"location":"api/clustering/k_means/#sorix.clustering.k_means.Kmeans","title":"Kmeans","text":"<pre><code>Kmeans(n_clusters)\n</code></pre> <p>K-means clustering</p> <p>Parameters: n_clusters (int): number of clusters</p> Source code in <code>sorix/clustering/k_means.py</code> <pre><code>def __init__(self, n_clusters:int):\n    \"\"\"\n    Parameters:\n    n_clusters (int): number of clusters\n    \"\"\"\n    self.n_clusters = n_clusters\n    self._centroids = None\n    self.features_names = None\n    self.labels = None\n</code></pre>"},{"location":"api/clustering/k_means/#sorix.clustering.k_means.Kmeans.fit","title":"fit","text":"<pre><code>fit(features, eps=0.001, max_iters=1000)\n</code></pre> <p>Fit the model.</p> PARAMETER DESCRIPTION <code>features</code> <p>Features to predict.</p> <p> TYPE: <code>tensor</code> </p> <code>eps</code> <p>Stop criterion, by default 0.001</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_iters</code> <p>Maximum number of iterations, by default 1000</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>sorix/clustering/k_means.py</code> <pre><code>def fit(self, \n          features: Tensor, \n          eps:float=0.001,\n          max_iters:int=1000) -&gt; None:\n\n    \"\"\"\n    Fit the model.\n\n    Parameters:\n        features (tensor): Features to predict.\n        eps (float, optional): Stop criterion, by default 0.001\n        max_iters (int, optional): Maximum number of iterations, by default 1000\n\n    \"\"\"\n\n    features_train = self._data_preprocessing_train(features)\n    iters = 0\n    while True:\n        iters += 1\n        distances = self._distances(features_train, self._centroids)\n        self.labels = self._new_labels(distances)\n        centroids_before = self._centroids\n        self._centroids = self._new_centroids(features_train, self.labels)\n        moviment = self._moviment(centroids_before, self._centroids)\n        if (moviment &lt; eps) or (iters &gt; max_iters):\n            break\n</code></pre>"},{"location":"api/clustering/k_means/#sorix.clustering.k_means.Kmeans.predict","title":"predict","text":"<pre><code>predict(features)\n</code></pre> <p>Predict the labels of features</p> PARAMETER DESCRIPTION <code>features</code> <p>Features to predict.</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>labels</code> <p>Labels of features</p> <p> TYPE: <code>tensor</code> </p> Source code in <code>sorix/clustering/k_means.py</code> <pre><code>def predict(self, features: Tensor) -&gt; Tensor:\n\n    \"\"\"\n    Predict the labels of features\n\n    Parameters:\n        features (tensor): Features to predict.\n\n    Returns:\n        labels (tensor): Labels of features\n    \"\"\"\n\n    distances = self._distances(features, self._centroids)\n    labels = self._new_labels(distances)\n    return tensor(labels)\n</code></pre>"},{"location":"api/clustering/k_means/#sorix.clustering.k_means.Kmeans.get_distances","title":"get_distances","text":"<pre><code>get_distances(features)\n</code></pre> <p>Get distances between features and centroids</p> PARAMETER DESCRIPTION <code>features</code> <p>Features to predict.</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>distances</code> <p>Distances betwen features and centroids</p> <p> TYPE: <code>tensor</code> </p> Source code in <code>sorix/clustering/k_means.py</code> <pre><code>def get_distances(self, features: Tensor) -&gt; Tensor:\n\n    \"\"\"\n    Get distances between features and centroids\n\n    Parameters:\n        features (tensor): Features to predict.\n\n    Returns:\n        distances (tensor): Distances betwen features and centroids\n    \"\"\"\n\n    return tensor(self._distances(features, self._centroids))\n</code></pre>"},{"location":"api/clustering/k_means/#sorix.clustering.k_means.Kmeans.get_inertia","title":"get_inertia","text":"<pre><code>get_inertia(features)\n</code></pre> <p>Get inertia of features for k-centroids</p> PARAMETER DESCRIPTION <code>features</code> <p>Features to predict.</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>inertia</code> <p>Inertia of features</p> <p> TYPE: <code>float</code> </p> Source code in <code>sorix/clustering/k_means.py</code> <pre><code>def get_inertia(self, features: Tensor) -&gt; float:\n\n    \"\"\"\n    Get inertia of features for k-centroids\n\n    Parameters:\n        features (tensor): Features to predict.\n\n    Returns:\n        inertia (float): Inertia of features\n    \"\"\"\n\n    distances = self._distances(features, self._centroids)\n    labels = self._new_labels(distances)\n\n    return smat.sum((features - self.centroids[labels])**2).item()\n</code></pre>"},{"location":"api/cuda/","title":"Index","text":""},{"location":"api/cuda/#sorix.cuda","title":"sorix.cuda","text":""},{"location":"api/cuda/cuda/","title":"cuda","text":""},{"location":"api/cuda/cuda/#sorix.cuda.cuda","title":"sorix.cuda.cuda","text":""},{"location":"api/cupy/","title":"Index","text":""},{"location":"api/cupy/#sorix.cupy","title":"sorix.cupy","text":""},{"location":"api/cupy/cupy/","title":"cupy","text":""},{"location":"api/cupy/cupy/#sorix.cupy.cupy","title":"sorix.cupy.cupy","text":""},{"location":"api/metrics/","title":"Index","text":""},{"location":"api/metrics/#sorix.metrics","title":"sorix.metrics","text":""},{"location":"api/metrics/metrics/","title":"metrics","text":""},{"location":"api/metrics/metrics/#sorix.metrics.metrics","title":"sorix.metrics.metrics","text":""},{"location":"api/metrics/metrics/#sorix.metrics.metrics.regression_report","title":"regression_report","text":"<pre><code>regression_report(y_true, y_pred)\n</code></pre> <p>Reporte de regresi\u00f3n con columnas alineadas y rango uniforme.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def regression_report(y_true: np.ndarray, y_pred: np.ndarray) -&gt; str:\n    \"\"\"\n    Reporte de regresi\u00f3n con columnas alineadas y rango uniforme.\n    \"\"\"\n    metrics = {\n        \"R2\":   (r2_score(y_true, y_pred), \"[0,   1]\"),\n        \"MAE\":  (mean_absolute_error(y_true, y_pred), \"[0,  \u221e)\"),\n        \"MSE\":  (mean_squared_error(y_true, y_pred), \"[0,  \u221e)\"),\n        \"RMSE\": (root_mean_squared_error(y_true, y_pred), \"[0,  \u221e)\"),\n        \"MAPE\": (mean_absolute_percentage_error(y_true, y_pred) * 100, \"[0, 100]\"),\n    }\n\n    # Forzar todos los rangos a la misma longitud (8 caracteres)\n    fixed_width = 8\n    for k, (val, rng) in metrics.items():\n        metrics[k] = (val, rng.ljust(fixed_width))\n\n    col_metric = 6\n    col_score = 9\n    col_range = fixed_width\n\n    header = f\"{'Metric':&lt;{col_metric}} | {'Score':&gt;{col_score}} | {'Range':&gt;{col_range}}\"\n    lines = [header, \"-\" * len(header)]\n\n    for name, (value, rng) in metrics.items():\n        lines.append(f\"{name:&lt;{col_metric}} | {value:&gt;{col_score}.4f} | {rng:&gt;{col_range}}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.classification_report","title":"classification_report","text":"<pre><code>classification_report(y_true, y_pred)\n</code></pre> <p>Reporte de clasificaci\u00f3n similar a sklearn.metrics.classification_report.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def classification_report(y_true: np.ndarray, y_pred: np.ndarray) -&gt; str:\n    \"\"\"\n    Reporte de clasificaci\u00f3n similar a sklearn.metrics.classification_report.\n    \"\"\"\n    y_true, y_pred = _get_classification_data(y_true, y_pred)\n    classes = sorted(np.unique(y_true))\n    report = {}\n\n    total_true = len(y_true)\n\n    # M\u00e9tricas por clase\n    for c in classes:\n        true_pos = np.sum((y_true == c) &amp; (y_pred == c))\n        pred_pos = np.sum(y_pred == c)\n        actual_pos = np.sum(y_true == c)\n\n        precision = true_pos / pred_pos if pred_pos &gt; 0 else 0.0\n        recall = true_pos / actual_pos if actual_pos &gt; 0 else 0.0\n        f1 = (2 * precision * recall / (precision + recall)\n              if (precision + recall) &gt; 0 else 0.0)\n\n        report[c] = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1,\n            \"support\": actual_pos\n        }\n\n    # Promedio macro\n    macro_precision = np.mean([report[c][\"precision\"] for c in classes])\n    macro_recall = np.mean([report[c][\"recall\"] for c in classes])\n    macro_f1 = np.mean([report[c][\"f1\"] for c in classes])\n\n    # Promedio ponderado\n    weights = np.array([report[c][\"support\"] for c in classes])\n    weighted_precision = np.average([report[c][\"precision\"] for c in classes], weights=weights)\n    weighted_recall = np.average([report[c][\"recall\"] for c in classes], weights=weights)\n    weighted_f1 = np.average([report[c][\"f1\"] for c in classes], weights=weights)\n\n    # Estilo sklearn: ancho fijo y espacio inicial\n    header = f\"{'':&lt;12}{'precision':&gt;9}{'recall':&gt;9}{'f1-score':&gt;9}{'support':&gt;9}\"\n    lines = [header]\n\n    # L\u00edneas para cada clase\n    for c in classes:\n        lines.append(f\"{str(c):&lt;12}{report[c]['precision']:&gt;9.2f}{report[c]['recall']:&gt;9.2f}{report[c]['f1']:&gt;9.2f}{report[c]['support']:&gt;9}\")\n\n    lines.append(\"\")\n\n    # Fila de accuracy (sklearn tambi\u00e9n la incluye)\n    accuracy = np.sum(y_true == y_pred) / total_true\n    lines.append(f\"{'accuracy':&lt;12}{'':&gt;9}{'':&gt;9}{accuracy:&gt;9.2f}{total_true:&gt;9}\")\n\n    # L\u00edneas de promedios con alineaci\u00f3n correcta\n    lines.append(f\"{'macro avg':&lt;12}{macro_precision:&gt;9.2f}{macro_recall:&gt;9.2f}{macro_f1:&gt;9.2f}{total_true:&gt;9}\")\n    lines.append(f\"{'weighted avg':&lt;12}{weighted_precision:&gt;9.2f}{weighted_recall:&gt;9.2f}{weighted_f1:&gt;9.2f}{total_true:&gt;9}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/model_selection/","title":"Index","text":""},{"location":"api/model_selection/#sorix.model_selection","title":"sorix.model_selection","text":""},{"location":"api/model_selection/train_test/","title":"train_test","text":""},{"location":"api/model_selection/train_test/#sorix.model_selection.train_test","title":"sorix.model_selection.train_test","text":""},{"location":"api/model_selection/train_test/#sorix.model_selection.train_test.train_test_split","title":"train_test_split","text":"<pre><code>train_test_split(X, Y=None, test_size=0.2, random_state=42, shuffle=True)\n</code></pre> <p>Method to split the data into train and test sets.</p> PARAMETER DESCRIPTION <code>X</code> <p>Features.</p> <p> TYPE: <code>DataFrame</code> </p> <code>Y</code> <p>Labels.</p> <p> TYPE: <code>Series</code> DEFAULT: <code>None</code> </p> <code>test_size</code> <p>Proportion of the dataset to include in the test split. Default is 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>random_state</code> <p>Random state for reproducibility. Default is 42.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>shuffle</code> <p>Whether to shuffle the data before splitting. Default is True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>(X_train, X_test, y_train, y_test)</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>sorix/model_selection/train_test.py</code> <pre><code>def train_test_split(X: pd.DataFrame,\n                     Y: pd.Series = None,\n                     test_size: float = 0.2,\n                     random_state: int = 42,\n                     shuffle: bool = True) -&gt; tuple:\n    \"\"\"\n    Method to split the data into train and test sets.\n\n    Args:\n        X (pd.DataFrame): Features.\n        Y (pd.Series): Labels.\n        test_size (float, optional): Proportion of the dataset to include in the test split. Default is 0.2.\n        random_state (int, optional): Random state for reproducibility. Default is 42.\n        shuffle (bool, optional): Whether to shuffle the data before splitting. Default is True.\n\n    Returns:\n        tuple: (X_train, X_test, y_train, y_test)\n    \"\"\"\n\n    # Validate test_size\n    if not (0 &lt; test_size &lt; 1):\n        raise ValueError(\"test_size must be between 0 and 1.\")\n\n    # Set random seed for reproducibility\n    np.random.seed(random_state)\n\n    # Shuffle the data if specified\n    if shuffle:\n        indices = np.random.permutation(len(X))\n        X = X.iloc[indices]\n        if Y is not None:\n            Y = Y.iloc[indices]\n\n    # Calculate the split index\n    split_index = int(len(X) * (1 - test_size))\n\n    # Split the data\n    X_train = X.iloc[:split_index]\n    X_test = X.iloc[split_index:]\n    if Y is not None:\n        Y_train = Y.iloc[:split_index]\n        Y_test = Y.iloc[split_index:]\n        return X_train, X_test, Y_train, Y_test\n\n    return X_train, X_test\n</code></pre>"},{"location":"api/nn/","title":"Index","text":""},{"location":"api/nn/#sorix.nn","title":"sorix.nn","text":""},{"location":"api/nn/layers/","title":"layers","text":""},{"location":"api/nn/layers/#sorix.nn.layers","title":"sorix.nn.layers","text":""},{"location":"api/nn/loss/","title":"loss","text":""},{"location":"api/nn/loss/#sorix.nn.loss","title":"sorix.nn.loss","text":""},{"location":"api/nn/net/","title":"net","text":""},{"location":"api/nn/net/#sorix.nn.net","title":"sorix.nn.net","text":""},{"location":"api/nn/net/#sorix.nn.net.Module","title":"Module","text":"<pre><code>Module()\n</code></pre> Source code in <code>sorix/nn/net.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.device = 'cpu'\n    self.training = True\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.parameters","title":"parameters","text":"<pre><code>parameters()\n</code></pre> <p>Returns an iterator over module parameters.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def parameters(self):\n    \"\"\"\n    Returns an iterator over module parameters.\n    \"\"\"\n    params = []\n    visited = set()\n\n    def _gather_params(obj):\n        if id(obj) in visited:\n            return\n        visited.add(id(obj))\n\n        if isinstance(obj, Tensor):\n            if obj.requires_grad:\n                params.append(obj)\n        elif hasattr(obj, \"parameters\") and callable(obj.parameters) and obj is not self:\n            # If the object has its own parameters() method, use it\n            params.extend(obj.parameters())\n        elif hasattr(obj, \"__dict__\"):\n            # Recurse into attributes\n            for k, v in obj.__dict__.items():\n                if not k.startswith('_'):\n                    _gather_params(v)\n        elif isinstance(obj, (list, tuple)):\n            for item in obj:\n                _gather_params(item)\n        elif isinstance(obj, dict):\n            for item in obj.values():\n                _gather_params(item)\n\n    _gather_params(self)\n    return params\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.to","title":"to","text":"<pre><code>to(device)\n</code></pre> <p>Mueve TODOS los tensores/capas/subredes al device.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def to(self, device):\n    \"\"\"Mueve TODOS los tensores/capas/subredes al device.\"\"\"\n    self.device = device\n\n    def _apply(obj):\n        if hasattr(obj, \"to\") and callable(obj.to) and obj is not self:\n            return obj.to(device)\n\n        if isinstance(obj, Tensor):\n            return obj.to(device)\n\n        if isinstance(obj, list):\n            return [_apply(v) for v in obj]\n        if isinstance(obj, tuple):\n            return tuple(_apply(v) for v in obj)\n        if isinstance(obj, dict):\n            return {k: _apply(v) for k, v in obj.items()}\n\n        if hasattr(obj, \"__dict__\") and obj is not self:\n             # Try to move attributes of non-Module objects\n             for k, v in obj.__dict__.items():\n                 if not k.startswith('_'):\n                     setattr(obj, k, _apply(v))\n\n        return obj\n\n    for k, v in self.__dict__.items():\n        if not k.startswith('_'):\n            setattr(self, k, _apply(v))\n\n    return self\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.state_dict","title":"state_dict","text":"<pre><code>state_dict()\n</code></pre> <p>Returns a dictionary containing a whole state of the module.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def state_dict(self):\n    \"\"\"\n    Returns a dictionary containing a whole state of the module.\n    \"\"\"\n    state = {}\n\n    def _get_state(obj, prefix):\n        for name, val in obj.__dict__.items():\n            if name.startswith('_') or name == 'device' or name == 'training':\n                continue\n\n            key = prefix + name\n            if isinstance(val, Tensor):\n                state[key] = val\n            elif hasattr(val, 'state_dict') and callable(val.state_dict) and val is not self:\n                sub_state = val.state_dict()\n                for sk, sv in sub_state.items():\n                    state[key + '.' + sk] = sv\n            elif hasattr(val, '__dict__'):\n                 for kn, kv in val.__dict__.items():\n                     if isinstance(kv, Tensor):\n                         state[key + '.' + kn] = kv\n\n    _get_state(self, \"\")\n    return state\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict)\n</code></pre> <p>Copies parameters and buffers from state_dict into this module and its descendants.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def load_state_dict(self, state_dict):\n    \"\"\"\n    Copies parameters and buffers from state_dict into this module and its descendants.\n    \"\"\"\n    own_state = self.state_dict()\n    for name, param in state_dict.items():\n        if name in own_state:\n            if isinstance(param, Tensor):\n                own_state[name].data = param.data\n                own_state[name].to(own_state[name].device) # Ensure device consistency\n            else:\n                pass\n        else:\n            pass\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.weights","title":"weights","text":"<pre><code>weights()\n</code></pre> <p>Deprecated: use state_dict() instead.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def weights(self):\n    \"\"\"Deprecated: use state_dict() instead.\"\"\"\n    return self.state_dict()\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.load_weights","title":"load_weights","text":"<pre><code>load_weights(weights)\n</code></pre> <p>Deprecated: use load_state_dict() instead.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def load_weights(self, weights):\n    \"\"\"Deprecated: use load_state_dict() instead.\"\"\"\n    self.load_state_dict(weights)\n</code></pre>"},{"location":"api/optim/","title":"Index","text":""},{"location":"api/optim/#sorix.optim","title":"sorix.optim","text":""},{"location":"api/optim/optim/","title":"optim","text":""},{"location":"api/optim/optim/#sorix.optim.optim","title":"sorix.optim.optim","text":""},{"location":"api/preprocessing/","title":"Index","text":""},{"location":"api/preprocessing/#sorix.preprocessing","title":"sorix.preprocessing","text":""},{"location":"api/preprocessing/encoders/","title":"encoders","text":""},{"location":"api/preprocessing/encoders/#sorix.preprocessing.encoders","title":"sorix.preprocessing.encoders","text":""},{"location":"api/preprocessing/scalers/","title":"scalers","text":""},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers","title":"sorix.preprocessing.scalers","text":""},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.BaseScaler","title":"BaseScaler","text":"<pre><code>BaseScaler()\n</code></pre> <p>Clase base para todos los escaladores, implementando m\u00e9todos comunes.</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def __init__(self):\n    self.numerical_features: List[str] = []\n    self.n_features: int = 0\n</code></pre>"},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.BaseScaler.prepros","title":"prepros","text":"<pre><code>prepros(X)\n</code></pre> <p>Valida y registra nombres de columnas.</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def prepros(self, X: Union[np.ndarray, pd.DataFrame]):\n    \"\"\"Valida y registra nombres de columnas.\"\"\"\n    if isinstance(X, pd.DataFrame):\n        self.numerical_features = list(X.columns)\n        X = X.to_numpy()\n    elif isinstance(X, np.ndarray):\n        self.numerical_features = [f\"F{i}\" for i in range(X.shape[1])] if X.ndim &gt; 1 else [\"F0\"]\n    else:\n        raise TypeError(\"La entrada debe ser un ndarray de NumPy o un DataFrame de Pandas.\")\n\n    self.n_features = X.shape[1] if X.ndim &gt; 1 else 1\n    return X\n</code></pre>"},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.MinMaxScaler","title":"MinMaxScaler","text":"<pre><code>MinMaxScaler()\n</code></pre> <p>               Bases: <code>BaseScaler</code></p> <p>Escala las caracter\u00edsticas a un rango [0, 1].</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.min: Optional[np.ndarray] = None\n    self.max: Optional[np.ndarray] = None\n</code></pre>"},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.StandardScaler","title":"StandardScaler","text":"<pre><code>StandardScaler()\n</code></pre> <p>               Bases: <code>BaseScaler</code></p> <p>Estandariza eliminando la media y escalando a varianza unitaria.</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.mean: Optional[np.ndarray] = None\n    self.std: Optional[np.ndarray] = None\n</code></pre>"},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.RobustScaler","title":"RobustScaler","text":"<pre><code>RobustScaler()\n</code></pre> <p>               Bases: <code>BaseScaler</code></p> <p>Escala usando mediana e IQR (robusto a outliers).</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.median: Optional[np.ndarray] = None\n    self.q1: Optional[np.ndarray] = None\n    self.q3: Optional[np.ndarray] = None\n</code></pre>"},{"location":"api/preprocessing/transformers/","title":"transformers","text":""},{"location":"api/preprocessing/transformers/#sorix.preprocessing.transformers","title":"sorix.preprocessing.transformers","text":""},{"location":"api/utils/","title":"Index","text":""},{"location":"api/utils/#sorix.utils","title":"sorix.utils","text":""},{"location":"api/utils/math/","title":"math","text":""},{"location":"api/utils/math/#sorix.utils.math","title":"sorix.utils.math","text":""},{"location":"api/utils/utils/","title":"utils","text":""},{"location":"api/utils/utils/#sorix.utils.utils","title":"sorix.utils.utils","text":""},{"location":"api/utils/utils/#sorix.utils.utils.save","title":"save","text":"<pre><code>save(obj, f)\n</code></pre> <p>Saves an object to a file using pickle.  Tensors will be automatically moved to CPU during serialization.</p> Source code in <code>sorix/utils/utils.py</code> <pre><code>def save(obj, f):\n    \"\"\"\n    Saves an object to a file using pickle. \n    Tensors will be automatically moved to CPU during serialization.\n    \"\"\"\n    if isinstance(f, str):\n        with open(f, 'wb') as file:\n            pickle.dump(obj, file)\n    else:\n        pickle.dump(obj, f)\n</code></pre>"},{"location":"api/utils/utils/#sorix.utils.utils.load","title":"load","text":"<pre><code>load(f)\n</code></pre> <p>Loads an object from a file using pickle.</p> Source code in <code>sorix/utils/utils.py</code> <pre><code>def load(f):\n    \"\"\"\n    Loads an object from a file using pickle.\n    \"\"\"\n    if isinstance(f, str):\n        with open(f, 'rb') as file:\n            return pickle.load(file)\n    else:\n        return pickle.load(f)\n</code></pre>"},{"location":"api/utils/data/","title":"Index","text":""},{"location":"api/utils/data/#sorix.utils.data","title":"sorix.utils.data","text":""},{"location":"api/utils/data/dataloader/","title":"dataloader","text":""},{"location":"api/utils/data/dataloader/#sorix.utils.data.dataloader","title":"sorix.utils.data.dataloader","text":""},{"location":"api/utils/data/dataset/","title":"dataset","text":""},{"location":"api/utils/data/dataset/#sorix.utils.data.dataset","title":"sorix.utils.data.dataset","text":""},{"location":"examples/clustering/Iris-dataset/","title":"Iris Dataset","text":"In\u00a0[1]: Copied! <pre>#Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> #Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[2]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sorix.clustering import Kmeans\nfrom sorix import tensor\n</pre> import pandas as pd import matplotlib.pyplot as plt from sorix.clustering import Kmeans from sorix import tensor In\u00a0[3]: Copied! <pre>data=pd.read_csv(\"../data/Iris.csv\")\ndata.head()\n</pre> data=pd.read_csv(\"../data/Iris.csv\") data.head() Out[3]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[4]: Copied! <pre>data['labels'] = data.Species.map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\ndata.head()\n</pre> data['labels'] = data.Species.map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}) data.head() Out[4]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species labels 0 1 5.1 3.5 1.4 0.2 Iris-setosa 0 1 2 4.9 3.0 1.4 0.2 Iris-setosa 0 2 3 4.7 3.2 1.3 0.2 Iris-setosa 0 3 4 4.6 3.1 1.5 0.2 Iris-setosa 0 4 5 5.0 3.6 1.4 0.2 Iris-setosa 0 In\u00a0[5]: Copied! <pre>plt.figure(figsize=(12,8))\nplt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=data['labels'])\n</pre> plt.figure(figsize=(12,8)) plt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=data['labels']) Out[5]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f3928970830&gt;</pre> In\u00a0[6]: Copied! <pre>X=data[['PetalLengthCm','PetalWidthCm','PetalLengthCm','PetalWidthCm']]\nX = tensor(X)\nX\n</pre> X=data[['PetalLengthCm','PetalWidthCm','PetalLengthCm','PetalWidthCm']] X = tensor(X) X Out[6]: <pre>Tensor(\n[[1.4 0.2 1.4 0.2]\n [1.4 0.2 1.4 0.2]\n [1.3 0.2 1.3 0.2]\n [1.5 0.2 1.5 0.2]\n [1.4 0.2 1.4 0.2]\n [1.7 0.4 1.7 0.4]\n [1.4 0.3 1.4 0.3]\n [1.5 0.2 1.5 0.2]\n [1.4 0.2 1.4 0.2]\n [1.5 0.1 1.5 0.1]\n [1.5 0.2 1.5 0.2]\n [1.6 0.2 1.6 0.2]\n [1.4 0.1 1.4 0.1]\n [1.1 0.1 1.1 0.1]\n [1.2 0.2 1.2 0.2]\n [1.5 0.4 1.5 0.4]\n [1.3 0.4 1.3 0.4]\n [1.4 0.3 1.4 0.3]\n [1.7 0.3 1.7 0.3]\n [1.5 0.3 1.5 0.3]\n [1.7 0.2 1.7 0.2]\n [1.5 0.4 1.5 0.4]\n [1.  0.2 1.  0.2]\n [1.7 0.5 1.7 0.5]\n [1.9 0.2 1.9 0.2]\n [1.6 0.2 1.6 0.2]\n [1.6 0.4 1.6 0.4]\n [1.5 0.2 1.5 0.2]\n [1.4 0.2 1.4 0.2]\n [1.6 0.2 1.6 0.2]\n [1.6 0.2 1.6 0.2]\n [1.5 0.4 1.5 0.4]\n [1.5 0.1 1.5 0.1]\n [1.4 0.2 1.4 0.2]\n [1.5 0.1 1.5 0.1]\n [1.2 0.2 1.2 0.2]\n [1.3 0.2 1.3 0.2]\n [1.5 0.1 1.5 0.1]\n [1.3 0.2 1.3 0.2]\n [1.5 0.2 1.5 0.2]\n [1.3 0.3 1.3 0.3]\n [1.3 0.3 1.3 0.3]\n [1.3 0.2 1.3 0.2]\n [1.6 0.6 1.6 0.6]\n [1.9 0.4 1.9 0.4]\n [1.4 0.3 1.4 0.3]\n [1.6 0.2 1.6 0.2]\n [1.4 0.2 1.4 0.2]\n [1.5 0.2 1.5 0.2]\n [1.4 0.2 1.4 0.2]\n [4.7 1.4 4.7 1.4]\n [4.5 1.5 4.5 1.5]\n [4.9 1.5 4.9 1.5]\n [4.  1.3 4.  1.3]\n [4.6 1.5 4.6 1.5]\n [4.5 1.3 4.5 1.3]\n [4.7 1.6 4.7 1.6]\n [3.3 1.  3.3 1. ]\n [4.6 1.3 4.6 1.3]\n [3.9 1.4 3.9 1.4]\n [3.5 1.  3.5 1. ]\n [4.2 1.5 4.2 1.5]\n [4.  1.  4.  1. ]\n [4.7 1.4 4.7 1.4]\n [3.6 1.3 3.6 1.3]\n [4.4 1.4 4.4 1.4]\n [4.5 1.5 4.5 1.5]\n [4.1 1.  4.1 1. ]\n [4.5 1.5 4.5 1.5]\n [3.9 1.1 3.9 1.1]\n [4.8 1.8 4.8 1.8]\n [4.  1.3 4.  1.3]\n [4.9 1.5 4.9 1.5]\n [4.7 1.2 4.7 1.2]\n [4.3 1.3 4.3 1.3]\n [4.4 1.4 4.4 1.4]\n [4.8 1.4 4.8 1.4]\n [5.  1.7 5.  1.7]\n [4.5 1.5 4.5 1.5]\n [3.5 1.  3.5 1. ]\n [3.8 1.1 3.8 1.1]\n [3.7 1.  3.7 1. ]\n [3.9 1.2 3.9 1.2]\n [5.1 1.6 5.1 1.6]\n [4.5 1.5 4.5 1.5]\n [4.5 1.6 4.5 1.6]\n [4.7 1.5 4.7 1.5]\n [4.4 1.3 4.4 1.3]\n [4.1 1.3 4.1 1.3]\n [4.  1.3 4.  1.3]\n [4.4 1.2 4.4 1.2]\n [4.6 1.4 4.6 1.4]\n [4.  1.2 4.  1.2]\n [3.3 1.  3.3 1. ]\n [4.2 1.3 4.2 1.3]\n [4.2 1.2 4.2 1.2]\n [4.2 1.3 4.2 1.3]\n [4.3 1.3 4.3 1.3]\n [3.  1.1 3.  1.1]\n [4.1 1.3 4.1 1.3]\n [6.  2.5 6.  2.5]\n [5.1 1.9 5.1 1.9]\n [5.9 2.1 5.9 2.1]\n [5.6 1.8 5.6 1.8]\n [5.8 2.2 5.8 2.2]\n [6.6 2.1 6.6 2.1]\n [4.5 1.7 4.5 1.7]\n [6.3 1.8 6.3 1.8]\n [5.8 1.8 5.8 1.8]\n [6.1 2.5 6.1 2.5]\n [5.1 2.  5.1 2. ]\n [5.3 1.9 5.3 1.9]\n [5.5 2.1 5.5 2.1]\n [5.  2.  5.  2. ]\n [5.1 2.4 5.1 2.4]\n [5.3 2.3 5.3 2.3]\n [5.5 1.8 5.5 1.8]\n [6.7 2.2 6.7 2.2]\n [6.9 2.3 6.9 2.3]\n [5.  1.5 5.  1.5]\n [5.7 2.3 5.7 2.3]\n [4.9 2.  4.9 2. ]\n [6.7 2.  6.7 2. ]\n [4.9 1.8 4.9 1.8]\n [5.7 2.1 5.7 2.1]\n [6.  1.8 6.  1.8]\n [4.8 1.8 4.8 1.8]\n [4.9 1.8 4.9 1.8]\n [5.6 2.1 5.6 2.1]\n [5.8 1.6 5.8 1.6]\n [6.1 1.9 6.1 1.9]\n [6.4 2.  6.4 2. ]\n [5.6 2.2 5.6 2.2]\n [5.1 1.5 5.1 1.5]\n [5.6 1.4 5.6 1.4]\n [6.1 2.3 6.1 2.3]\n [5.6 2.4 5.6 2.4]\n [5.5 1.8 5.5 1.8]\n [4.8 1.8 4.8 1.8]\n [5.4 2.1 5.4 2.1]\n [5.6 2.4 5.6 2.4]\n [5.1 2.3 5.1 2.3]\n [5.1 1.9 5.1 1.9]\n [5.9 2.3 5.9 2.3]\n [5.7 2.5 5.7 2.5]\n [5.2 2.3 5.2 2.3]\n [5.  1.9 5.  1.9]\n [5.2 2.  5.2 2. ]\n [5.4 2.3 5.4 2.3]\n [5.1 1.8 5.1 1.8]], shape=(150, 4), device=cpu, requires_grad=False)</pre> In\u00a0[7]: Copied! <pre>model= Kmeans(n_clusters=3)\nmodel.fit(X)\n</pre> model= Kmeans(n_clusters=3) model.fit(X) In\u00a0[8]: Copied! <pre>pred_labels = model.predict(X)\nplt.figure(figsize=(12,8))\nplt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=pred_labels)\n\nfor centroid in model.centroids:\n    plt.scatter(centroid[0],centroid[1], s=150)\n</pre> pred_labels = model.predict(X) plt.figure(figsize=(12,8)) plt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=pred_labels)  for centroid in model.centroids:     plt.scatter(centroid[0],centroid[1], s=150) In\u00a0[9]: Copied! <pre>pred_labels = model.predict(X)\nplt.figure(figsize=(12,8))\nplt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=pred_labels)\n\nfor centroid in model.centroids:\n    plt.scatter(centroid[0],centroid[1], s=150)\n</pre> pred_labels = model.predict(X) plt.figure(figsize=(12,8)) plt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=pred_labels)  for centroid in model.centroids:     plt.scatter(centroid[0],centroid[1], s=150) In\u00a0[10]: Copied! <pre>inertias = []\n\nfor i in range(2,11):\n    model = Kmeans(n_clusters=i)\n    model.fit(X)\n    inertias.append(model.get_inertia(X))\n\nimport matplotlib.pyplot as plt\nplt.plot(range(2,11), inertias, marker='o')\n</pre> inertias = []  for i in range(2,11):     model = Kmeans(n_clusters=i)     model.fit(X)     inertias.append(model.get_inertia(X))  import matplotlib.pyplot as plt plt.plot(range(2,11), inertias, marker='o') Out[10]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f3928142490&gt;]</pre>"},{"location":"examples/clustering/Iris-dataset/#iris-dataset","title":"Iris Dataset\u00b6","text":""},{"location":"examples/clustering/RandomDataset%2BGPU/","title":"Random Dataset + GPU","text":"In\u00a0[1]: Copied! <pre>#Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> #Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport sorix\nfrom sorix.clustering import Kmeans\n</pre> import numpy as np import matplotlib.pyplot as plt import sorix from sorix.clustering import Kmeans In\u00a0[3]: Copied! <pre>device = 'gpu' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'gpu' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 13.6.0\n</pre> Out[3]: <pre>'gpu'</pre> In\u00a0[4]: Copied! <pre># Semilla para reproducibilidad\nnp.random.seed(42)\n\n# Par\u00e1metros\nn_clusters = 3\nsamples_per_cluster = 1000000\n\n# Centros de los cl\u00fasteres (puedes cambiarlos)\ncentros = np.array([\n    [2, 2],\n    [8, 3],\n    [3, 8]\n])\n\n# Desviaci\u00f3n est\u00e1ndar de los cl\u00fasteres\nstd = 0.8\n\n# Genera los puntos\nX = []\ny = []\nfor label, centro in enumerate(centros):\n    puntos = np.random.randn(samples_per_cluster, 2) * std + centro\n    X.append(puntos)\n    y.append(np.full(samples_per_cluster, label))\n\n# Unimos todos los puntos y etiquetas\nX = np.vstack(X)\ny = np.concatenate(y)\n\n# Visualizaci\u00f3n r\u00e1pida\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"Set1\", s=40, edgecolor='k')\nplt.title(\"Dataset sint\u00e9tico para clustering\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.show()\n</pre> # Semilla para reproducibilidad np.random.seed(42)  # Par\u00e1metros n_clusters = 3 samples_per_cluster = 1000000  # Centros de los cl\u00fasteres (puedes cambiarlos) centros = np.array([     [2, 2],     [8, 3],     [3, 8] ])  # Desviaci\u00f3n est\u00e1ndar de los cl\u00fasteres std = 0.8  # Genera los puntos X = [] y = [] for label, centro in enumerate(centros):     puntos = np.random.randn(samples_per_cluster, 2) * std + centro     X.append(puntos)     y.append(np.full(samples_per_cluster, label))  # Unimos todos los puntos y etiquetas X = np.vstack(X) y = np.concatenate(y)  # Visualizaci\u00f3n r\u00e1pida plt.figure(figsize=(6, 6)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"Set1\", s=40, edgecolor='k') plt.title(\"Dataset sint\u00e9tico para clustering\") plt.xlabel(\"X1\") plt.ylabel(\"X2\") plt.show() In\u00a0[5]: Copied! <pre>X_train = sorix.tensor(X, device=device)\nX_train\n</pre> X_train = sorix.tensor(X, device=device) X_train Out[5]: <pre>Tensor(\n[[2.39737132 1.88938856]\n [2.51815083 3.21842389]\n [1.8126773  1.81269043]\n ...\n [3.56752298 8.4546417 ]\n [2.8109625  7.1025456 ]\n [4.49822372 7.9792028 ]], shape=(3000000, 2), device=gpu, requires_grad=False)</pre> In\u00a0[6]: Copied! <pre>kmeans = Kmeans(n_clusters=n_clusters)\nkmeans.fit(X_train)\n</pre> kmeans = Kmeans(n_clusters=n_clusters) kmeans.fit(X_train)"},{"location":"examples/clustering/RandomDataset%2BGPU/#random-dataset-gpu","title":"Random Dataset + GPU\u00b6","text":""},{"location":"examples/nn/1-regression/","title":"Regression","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport joblib\n\nimport sorix\nfrom sorix.nn import ReLU,Linear\nfrom sorix.optim import RMSprop\nfrom sorix import tensor,Tensor\nfrom sorix.nn import Module\nfrom sorix.nn import  MSELoss\nfrom sorix.preprocessing import MinMaxScaler\nfrom sorix.model_selection import train_test_split\nfrom sorix.metrics import r2_score\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import joblib  import sorix from sorix.nn import ReLU,Linear from sorix.optim import RMSprop from sorix import tensor,Tensor from sorix.nn import Module from sorix.nn import  MSELoss from sorix.preprocessing import MinMaxScaler from sorix.model_selection import train_test_split from sorix.metrics import r2_score In\u00a0[3]: Copied! <pre>device  = 'gpu' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device  = 'gpu' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 13.6.0\n</pre> Out[3]: <pre>'gpu'</pre> In\u00a0[4]: Copied! <pre># Datos\npoints = 10000\nx1 = np.linspace(1, 20*np.pi, points)\nx2 = np.linspace(1, 20*np.pi, points)\n\n# Definimos la salida\ny = 20*np.log(x1+1) + -1*x2 + 3*np.random.randn(points)\n\n\ndata = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\ndata.head()\n\nplt.figure(figsize=(20,7))\nplt.scatter(data['x2'], data['y'], s=20)\nplt.show()\n</pre> # Datos points = 10000 x1 = np.linspace(1, 20*np.pi, points) x2 = np.linspace(1, 20*np.pi, points)  # Definimos la salida y = 20*np.log(x1+1) + -1*x2 + 3*np.random.randn(points)   data = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y}) data.head()  plt.figure(figsize=(20,7)) plt.scatter(data['x2'], data['y'], s=20) plt.show()  In\u00a0[5]: Copied! <pre>independent_variables = ['x1', 'x2']\ndependent_variable = ['y']\n</pre> independent_variables = ['x1', 'x2'] dependent_variable = ['y'] In\u00a0[6]: Copied! <pre># Train test split and normalization\ndf_train, df_test = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n\nX_train = df_train[independent_variables]\nY_train = df_train[dependent_variable]\n\nX_test = df_test[independent_variables]\nY_test = df_test[dependent_variable]\n\nX_train\n</pre> # Train test split and normalization df_train, df_test = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)  X_train = df_train[independent_variables] Y_train = df_train[dependent_variable]  X_test = df_test[independent_variables] Y_test = df_test[dependent_variable]  X_train Out[6]: x1 x2 6252 39.661141 39.661141 4684 29.964936 29.964936 1731 11.704164 11.704164 4742 30.323597 30.323597 4521 28.956976 28.956976 ... ... ... 1638 11.129070 11.129070 5891 37.428788 37.428788 7427 46.927110 46.927110 608 4.759753 4.759753 6907 43.711532 43.711532 <p>8000 rows \u00d7 2 columns</p> In\u00a0[7]: Copied! <pre>scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train\n</pre> scaler = MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) X_train Out[7]: <pre>array([[0.62526253, 0.62526253],\n       [0.46844684, 0.46844684],\n       [0.17311731, 0.17311731],\n       ...,\n       [0.74277428, 0.74277428],\n       [0.06080608, 0.06080608],\n       [0.69076908, 0.69076908]], shape=(8000, 2))</pre> In\u00a0[8]: Copied! <pre>plt.Figure(figsize=(20,7))\nplt.scatter(X_train[:,0], Y_train)\nplt.show()\n</pre> plt.Figure(figsize=(20,7)) plt.scatter(X_train[:,0], Y_train) plt.show() In\u00a0[9]: Copied! <pre>X_train = tensor(X_train).to(device)\nY_train = tensor(Y_train).to(device)\n\nX_test = tensor(X_test).to(device)\nY_test = tensor(Y_test).to(device)\n\nprint(f\"X_train shape: {X_train.shape}, device: {X_train.device}\")\nprint(f\"Y_train shape: {Y_train.shape}. device: {Y_train.device}\")\nprint(f\"X_test shape: {X_test.shape}, device: {X_test.device}\")\nprint(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\")\n</pre>   X_train = tensor(X_train).to(device) Y_train = tensor(Y_train).to(device)  X_test = tensor(X_test).to(device) Y_test = tensor(Y_test).to(device)  print(f\"X_train shape: {X_train.shape}, device: {X_train.device}\") print(f\"Y_train shape: {Y_train.shape}. device: {Y_train.device}\") print(f\"X_test shape: {X_test.shape}, device: {X_test.device}\") print(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\") <pre>X_train shape: (8000, 2), device: gpu\nY_train shape: (8000, 1). device: gpu\nX_test shape: (2000, 2), device: gpu\nY_test shape: (2000, 1), device: gpu\n</pre> In\u00a0[10]: Copied! <pre>class Network(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = Linear(2, 64)\n        self.relu = ReLU()\n        self.fc2 = Linear(64, 32)\n        self.fc3 = Linear(32, 1)\n\n    def forward(self, x: tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n    \n\nnet = Network().to(device)\n</pre> class Network(Module):     def __init__(self):         super().__init__()         self.fc1 = Linear(2, 64)         self.relu = ReLU()         self.fc2 = Linear(64, 32)         self.fc3 = Linear(32, 1)      def forward(self, x: tensor) -&gt; Tensor:         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         x = self.relu(x)         x = self.fc3(x)         return x       net = Network().to(device) In\u00a0[11]: Copied! <pre>criterion = MSELoss()\noptimizer = RMSprop(net.parameters(), lr=1e-2)\n</pre> criterion = MSELoss() optimizer = RMSprop(net.parameters(), lr=1e-2) In\u00a0[12]: Copied! <pre>Y_pred = net(X_train)\nY_pred\n</pre> Y_pred = net(X_train) Y_pred Out[12]: <pre>Tensor(\n[[0.64082439]\n [0.4801058 ]\n [0.17742595]\n ...\n [0.76126084]\n [0.06231945]\n [0.7079613 ]], shape=(8000, 1), device=gpu, requires_grad=True)</pre> In\u00a0[13]: Copied! <pre># %%\n# Bucle de entrenamiento mejorado\nfor epoch in range(1000 + 1):\n    Y_pred = net(X_train)\n    loss = criterion(Y_train,Y_pred)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        r2_train = r2_score(Y_pred, Y_train)\n        with sorix.no_grad():\n            Y_pred = net(X_test)\n            r2_test = r2_score(Y_test, Y_pred)\n        print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | R2_Train: {100*r2_train:5.2f} % | R2_Test: {100*r2_test:5.2f} %\")\n        if r2_test &gt; 0.95:  # Mejoramos el criterio de parada\n            print(f\"Entrenamiento completado en {epoch} epochs!\")\n            break\n</pre> # %% # Bucle de entrenamiento mejorado for epoch in range(1000 + 1):     Y_pred = net(X_train)     loss = criterion(Y_train,Y_pred)      optimizer.zero_grad()     loss.backward()     optimizer.step()      if epoch % 10 == 0:         r2_train = r2_score(Y_pred, Y_train)         with sorix.no_grad():             Y_pred = net(X_test)             r2_test = r2_score(Y_test, Y_pred)         print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | R2_Train: {100*r2_train:5.2f} % | R2_Test: {100*r2_test:5.2f} %\")         if r2_test &gt; 0.95:  # Mejoramos el criterio de parada             print(f\"Entrenamiento completado en {epoch} epochs!\")             break <pre>[gpu] Epoch     0 | Loss: 1138.1861 | R2_Train: -1299129.78 % | R2_Test: -1822.18 %\n[gpu] Epoch    10 | Loss: 328.8072 | R2_Train: -140.71 % | R2_Test: -518.61 %\n[gpu] Epoch    20 | Loss: 270.1417 | R2_Train: -139.37 % | R2_Test: -404.99 %\n[gpu] Epoch    30 | Loss: 189.0735 | R2_Train: -186.87 % | R2_Test: -244.60 %\n[gpu] Epoch    40 | Loss: 101.9436 | R2_Train: -969.65 % | R2_Test: -85.13 %\n[gpu] Epoch    50 | Loss: 53.4110 | R2_Train: -12914.39 % | R2_Test: -0.95 %\n[gpu] Epoch    60 | Loss: 42.0682 | R2_Train: -424.93 % | R2_Test: 20.39 %\n[gpu] Epoch    70 | Loss: 39.8960 | R2_Train: -198.40 % | R2_Test: 23.65 %\n[gpu] Epoch    80 | Loss: 37.8964 | R2_Train: -150.46 % | R2_Test: 27.14 %\n</pre> <pre>[gpu] Epoch    90 | Loss: 36.2701 | R2_Train: -128.69 % | R2_Test: 30.53 %\n[gpu] Epoch   100 | Loss: 34.8717 | R2_Train: -110.98 % | R2_Test: 33.51 %\n[gpu] Epoch   110 | Loss: 33.3838 | R2_Train: -93.64 % | R2_Test: 36.46 %\n</pre> <pre>[gpu] Epoch   120 | Loss: 31.7458 | R2_Train: -76.23 % | R2_Test: 39.65 %\n[gpu] Epoch   130 | Loss: 30.1210 | R2_Train: -58.93 % | R2_Test: 42.85 %\n[gpu] Epoch   140 | Loss: 28.7416 | R2_Train: -42.44 % | R2_Test: 45.80 %\n[gpu] Epoch   150 | Loss: 28.2170 | R2_Train: -31.84 % | R2_Test: 44.87 %\n[gpu] Epoch   160 | Loss: 24.8055 | R2_Train: -8.66 % | R2_Test: 51.85 %\n[gpu] Epoch   170 | Loss: 23.0389 | R2_Train:  5.91 % | R2_Test: 56.42 %\n[gpu] Epoch   180 | Loss: 22.5441 | R2_Train: 15.97 % | R2_Test: 58.06 %\n[gpu] Epoch   190 | Loss: 21.5454 | R2_Train: 25.79 % | R2_Test: 59.71 %\n[gpu] Epoch   200 | Loss: 20.2370 | R2_Train: 34.77 % | R2_Test: 61.95 %\n[gpu] Epoch   210 | Loss: 19.3087 | R2_Train: 41.25 % | R2_Test: 63.80 %\n</pre> <pre>[gpu] Epoch   220 | Loss: 18.6570 | R2_Train: 46.05 % | R2_Test: 65.19 %\n[gpu] Epoch   230 | Loss: 18.0563 | R2_Train: 49.80 % | R2_Test: 66.35 %\n[gpu] Epoch   240 | Loss: 17.6191 | R2_Train: 52.58 % | R2_Test: 67.26 %\n</pre> <pre>[gpu] Epoch   250 | Loss: 17.2403 | R2_Train: 54.74 % | R2_Test: 68.09 %\n[gpu] Epoch   260 | Loss: 16.8828 | R2_Train: 56.52 % | R2_Test: 68.73 %\n[gpu] Epoch   270 | Loss: 16.6399 | R2_Train: 57.86 % | R2_Test: 69.20 %\n[gpu] Epoch   280 | Loss: 16.4310 | R2_Train: 58.97 % | R2_Test: 69.61 %\n[gpu] Epoch   290 | Loss: 16.2363 | R2_Train: 59.91 % | R2_Test: 69.97 %\n[gpu] Epoch   300 | Loss: 16.0600 | R2_Train: 60.68 % | R2_Test: 70.30 %\n[gpu] Epoch   310 | Loss: 15.9042 | R2_Train: 61.35 % | R2_Test: 70.83 %\n[gpu] Epoch   320 | Loss: 14.7440 | R2_Train: 64.17 % | R2_Test: 71.97 %\n[gpu] Epoch   330 | Loss: 15.3534 | R2_Train: 63.07 % | R2_Test: 71.86 %\n[gpu] Epoch   340 | Loss: 15.8306 | R2_Train: 62.31 % | R2_Test: 71.03 %\n[gpu] Epoch   350 | Loss: 15.4940 | R2_Train: 63.21 % | R2_Test: 71.23 %\n</pre> <pre>[gpu] Epoch   360 | Loss: 15.1690 | R2_Train: 64.06 % | R2_Test: 71.79 %\n[gpu] Epoch   370 | Loss: 15.1224 | R2_Train: 64.30 % | R2_Test: 72.00 %\n[gpu] Epoch   380 | Loss: 15.1242 | R2_Train: 64.46 % | R2_Test: 72.01 %\n</pre> <pre>[gpu] Epoch   390 | Loss: 15.0235 | R2_Train: 64.81 % | R2_Test: 72.12 %\n[gpu] Epoch   400 | Loss: 14.9155 | R2_Train: 65.14 % | R2_Test: 72.29 %\n[gpu] Epoch   410 | Loss: 14.8526 | R2_Train: 65.39 % | R2_Test: 72.41 %\n[gpu] Epoch   420 | Loss: 14.7821 | R2_Train: 65.63 % | R2_Test: 72.52 %\n[gpu] Epoch   430 | Loss: 14.7178 | R2_Train: 65.85 % | R2_Test: 72.63 %\n[gpu] Epoch   440 | Loss: 14.6586 | R2_Train: 66.05 % | R2_Test: 72.73 %\n[gpu] Epoch   450 | Loss: 14.5944 | R2_Train: 66.25 % | R2_Test: 72.83 %\n[gpu] Epoch   460 | Loss: 14.5292 | R2_Train: 66.43 % | R2_Test: 72.93 %\n[gpu] Epoch   470 | Loss: 14.4627 | R2_Train: 66.61 % | R2_Test: 73.04 %\n[gpu] Epoch   480 | Loss: 14.4131 | R2_Train: 66.77 % | R2_Test: 73.14 %\n[gpu] Epoch   490 | Loss: 14.3567 | R2_Train: 66.92 % | R2_Test: 73.23 %\n</pre> <pre>[gpu] Epoch   500 | Loss: 14.3069 | R2_Train: 67.06 % | R2_Test: 73.31 %\n[gpu] Epoch   510 | Loss: 14.2508 | R2_Train: 67.20 % | R2_Test: 73.39 %\n[gpu] Epoch   520 | Loss: 14.2015 | R2_Train: 67.33 % | R2_Test: 73.48 %\n</pre> <pre>[gpu] Epoch   530 | Loss: 14.1617 | R2_Train: 67.46 % | R2_Test: 73.55 %\n[gpu] Epoch   540 | Loss: 14.1081 | R2_Train: 67.60 % | R2_Test: 73.63 %\n[gpu] Epoch   550 | Loss: 14.0635 | R2_Train: 67.73 % | R2_Test: 73.71 %\n[gpu] Epoch   560 | Loss: 14.0127 | R2_Train: 67.86 % | R2_Test: 73.79 %\n[gpu] Epoch   570 | Loss: 13.9710 | R2_Train: 67.96 % | R2_Test: 73.86 %\n[gpu] Epoch   580 | Loss: 13.9265 | R2_Train: 68.06 % | R2_Test: 73.93 %\n[gpu] Epoch   590 | Loss: 13.8908 | R2_Train: 68.16 % | R2_Test: 73.99 %\n[gpu] Epoch   600 | Loss: 13.8366 | R2_Train: 68.29 % | R2_Test: 74.08 %\n[gpu] Epoch   610 | Loss: 13.7948 | R2_Train: 68.38 % | R2_Test: 74.15 %\n[gpu] Epoch   620 | Loss: 13.7529 | R2_Train: 68.47 % | R2_Test: 74.22 %\n[gpu] Epoch   630 | Loss: 13.7080 | R2_Train: 68.55 % | R2_Test: 74.29 %\n</pre> <pre>[gpu] Epoch   640 | Loss: 13.6633 | R2_Train: 68.62 % | R2_Test: 74.36 %\n[gpu] Epoch   650 | Loss: 13.6156 | R2_Train: 68.67 % | R2_Test: 74.46 %\n[gpu] Epoch   660 | Loss: 13.6150 | R2_Train: 68.73 % | R2_Test: 74.49 %\n</pre> <pre>[gpu] Epoch   670 | Loss: 13.5845 | R2_Train: 68.84 % | R2_Test: 74.54 %\n[gpu] Epoch   680 | Loss: 13.5451 | R2_Train: 68.96 % | R2_Test: 74.59 %\n[gpu] Epoch   690 | Loss: 13.5109 | R2_Train: 69.05 % | R2_Test: 74.65 %\n[gpu] Epoch   700 | Loss: 13.4858 | R2_Train: 69.12 % | R2_Test: 74.70 %\n[gpu] Epoch   710 | Loss: 13.4486 | R2_Train: 69.21 % | R2_Test: 74.76 %\n[gpu] Epoch   720 | Loss: 13.4161 | R2_Train: 69.31 % | R2_Test: 74.82 %\n[gpu] Epoch   730 | Loss: 13.3732 | R2_Train: 69.40 % | R2_Test: 74.90 %\n[gpu] Epoch   740 | Loss: 13.3621 | R2_Train: 69.42 % | R2_Test: 74.92 %\n[gpu] Epoch   750 | Loss: 13.3308 | R2_Train: 69.47 % | R2_Test: 74.96 %\n[gpu] Epoch   760 | Loss: 13.2843 | R2_Train: 69.55 % | R2_Test: 75.05 %\n[gpu] Epoch   770 | Loss: 13.2727 | R2_Train: 69.58 % | R2_Test: 75.06 %\n</pre> <pre>[gpu] Epoch   780 | Loss: 13.2403 | R2_Train: 69.66 % | R2_Test: 75.12 %\n[gpu] Epoch   790 | Loss: 13.2261 | R2_Train: 69.71 % | R2_Test: 75.15 %\n[gpu] Epoch   800 | Loss: 13.2058 | R2_Train: 69.76 % | R2_Test: 75.18 %\n</pre> <pre>[gpu] Epoch   810 | Loss: 13.1846 | R2_Train: 69.81 % | R2_Test: 75.21 %\n[gpu] Epoch   820 | Loss: 13.1705 | R2_Train: 69.85 % | R2_Test: 75.25 %\n[gpu] Epoch   830 | Loss: 13.1428 | R2_Train: 69.90 % | R2_Test: 75.33 %\n[gpu] Epoch   840 | Loss: 13.1033 | R2_Train: 69.96 % | R2_Test: 75.35 %\n[gpu] Epoch   850 | Loss: 13.0605 | R2_Train: 70.07 % | R2_Test: 75.43 %\n[gpu] Epoch   860 | Loss: 13.0477 | R2_Train: 70.11 % | R2_Test: 75.47 %\n[gpu] Epoch   870 | Loss: 13.0619 | R2_Train: 70.09 % | R2_Test: 75.45 %\n[gpu] Epoch   880 | Loss: 13.0346 | R2_Train: 70.15 % | R2_Test: 75.48 %\n[gpu] Epoch   890 | Loss: 13.0101 | R2_Train: 70.21 % | R2_Test: 75.52 %\n[gpu] Epoch   900 | Loss: 12.9967 | R2_Train: 70.24 % | R2_Test: 75.54 %\n[gpu] Epoch   910 | Loss: 12.9852 | R2_Train: 70.26 % | R2_Test: 75.56 %\n</pre> <pre>[gpu] Epoch   920 | Loss: 12.9691 | R2_Train: 70.30 % | R2_Test: 75.59 %\n[gpu] Epoch   930 | Loss: 12.9508 | R2_Train: 70.33 % | R2_Test: 75.62 %\n[gpu] Epoch   940 | Loss: 12.9273 | R2_Train: 70.36 % | R2_Test: 75.65 %\n</pre> <pre>[gpu] Epoch   950 | Loss: 12.9148 | R2_Train: 70.39 % | R2_Test: 75.67 %\n[gpu] Epoch   960 | Loss: 12.9004 | R2_Train: 70.42 % | R2_Test: 75.69 %\n[gpu] Epoch   970 | Loss: 12.8814 | R2_Train: 70.46 % | R2_Test: 75.72 %\n[gpu] Epoch   980 | Loss: 12.8643 | R2_Train: 70.50 % | R2_Test: 75.75 %\n[gpu] Epoch   990 | Loss: 12.8469 | R2_Train: 70.54 % | R2_Test: 75.77 %\n[gpu] Epoch  1000 | Loss: 12.8292 | R2_Train: 70.59 % | R2_Test: 75.80 %\n</pre> In\u00a0[14]: Copied! <pre># %%\n# Visualizaci\u00f3n final\n\nwith sorix.no_grad():\n    y_pred = net(X_test)\n\nr2 = r2_score(Y_test, y_pred)\n\nplt.figure(figsize=(20,8))\nplt.scatter(X_test[:,0],Y_test,s=50)\nplt.scatter(X_test[:,0],y_pred)\nplt.title(f'Polinomic Regression on Test Data(Accuracy:{r2*100:.2f}%)')\n</pre> # %% # Visualizaci\u00f3n final  with sorix.no_grad():     y_pred = net(X_test)  r2 = r2_score(Y_test, y_pred)  plt.figure(figsize=(20,8)) plt.scatter(X_test[:,0],Y_test,s=50) plt.scatter(X_test[:,0],y_pred) plt.title(f'Polinomic Regression on Test Data(Accuracy:{r2*100:.2f}%)') Out[14]: <pre>Text(0.5, 1.0, 'Polinomic Regression on Test Data(Accuracy:75.80%)')</pre> In\u00a0[15]: Copied! <pre>sorix.save(net.state_dict(),\"regression_weights.sor\")\n</pre> sorix.save(net.state_dict(),\"regression_weights.sor\") In\u00a0[16]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"regression_weights.sor\"))\n\nif X_test.device == 'cpu':\n    out = net2(X_test)\nif X_test.device == 'gpu':\n    out = net2(X_test.to('cpu'))\n\nout\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"regression_weights.sor\"))  if X_test.device == 'cpu':     out = net2(X_test) if X_test.device == 'gpu':     out = net2(X_test.to('cpu'))  out Out[16]: <pre>Tensor(\n[[28.10235298]\n [32.96587243]\n [35.82963861]\n ...\n [38.65923082]\n [35.33820828]\n [32.88064684]], shape=(2000, 1), device=cpu, requires_grad=True)</pre> In\u00a0[17]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"regression_weights.sor\"))\nnet2.to('gpu')\n\nif X_test.device == 'cpu':\n    with sorix.no_grad():\n        out = net2(X_test.to('gpu'))\nif X_test.device == 'gpu':\n    with sorix.no_grad():\n        out = net2(X_test)\n\nout\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"regression_weights.sor\")) net2.to('gpu')  if X_test.device == 'cpu':     with sorix.no_grad():         out = net2(X_test.to('gpu')) if X_test.device == 'gpu':     with sorix.no_grad():         out = net2(X_test)  out Out[17]: <pre>Tensor(\n[[28.10235298]\n [32.96587243]\n [35.82963861]\n ...\n [38.65923082]\n [35.33820828]\n [32.88064684]], shape=(2000, 1), device=gpu, requires_grad=False)</pre>"},{"location":"examples/nn/1-regression/#regression","title":"Regression\u00b6","text":""},{"location":"examples/nn/1-regression/#datos","title":"Datos\u00b6","text":""},{"location":"examples/nn/1-regression/#preprocessing","title":"Preprocessing\u00b6","text":""},{"location":"examples/nn/1-regression/#save-and-load-model","title":"Save and Load Model\u00b6","text":""},{"location":"examples/nn/1-regression/#cpu","title":"CPU\u00b6","text":""},{"location":"examples/nn/1-regression/#gpu","title":"GPU\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/","title":"Classification Binary","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[2]: Copied! <pre>import joblib\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\nimport sorix\nfrom sorix.nn import ReLU,Linear,Sigmoid\nfrom sorix.optim import SGDMomentum,RMSprop,Adam,SGD\nfrom sorix import tensor,Tensor\nfrom sorix.nn import Module\nfrom sorix.nn import BCEWithLogitsLoss\nfrom sorix.metrics import confusion_matrix,classification_report\nfrom sorix.model_selection import train_test_split\n</pre> import joblib import seaborn as sns import numpy as np import pandas as pd import matplotlib.pyplot as plt   import sorix from sorix.nn import ReLU,Linear,Sigmoid from sorix.optim import SGDMomentum,RMSprop,Adam,SGD from sorix import tensor,Tensor from sorix.nn import Module from sorix.nn import BCEWithLogitsLoss from sorix.metrics import confusion_matrix,classification_report from sorix.model_selection import train_test_split In\u00a0[3]: Copied! <pre>device = 'gpu' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'gpu' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 13.6.0\n</pre> Out[3]: <pre>'gpu'</pre> In\u00a0[4]: Copied! <pre>r1 = 0.5  \nr2 = 1.5  \n\nnum_points = 10000\nthetas = np.linspace(0, 2 * np.pi, num_points) \n\n\nx1 = r1 * np.cos(thetas) + 0.1 * np.random.randn(num_points)\ny1 = r1 * np.sin(thetas) + 0.1 * np.random.randn(num_points)\n\n\nx2 = r2 * np.cos(thetas) + 0.1 * np.random.randn(num_points)\ny2 = r2 * np.sin(thetas) + 0.1 * np.random.randn(num_points)\n\n\n\nplt.figure(figsize=(8, 7)) \n\n\nplt.scatter(x1, y1, s=50, label=\"Clase A\", alpha=0.8) # Puntos de la Clase A\nplt.scatter(x2, y2, s=50, label=\"Clase B\", alpha=0.8) # Puntos de la Clase B\n\nplt.xlabel(\"Caracter\u00edstica X\") \nplt.ylabel(\"Caracter\u00edstica Y\")\nplt.title(\"Dataset de C\u00edrculos con 3 Clases\")\nplt.legend() \nplt.grid(True, linestyle='--', alpha=0.6)\nplt.axis('equal')\nplt.show() \n</pre> r1 = 0.5   r2 = 1.5    num_points = 10000 thetas = np.linspace(0, 2 * np.pi, num_points)    x1 = r1 * np.cos(thetas) + 0.1 * np.random.randn(num_points) y1 = r1 * np.sin(thetas) + 0.1 * np.random.randn(num_points)   x2 = r2 * np.cos(thetas) + 0.1 * np.random.randn(num_points) y2 = r2 * np.sin(thetas) + 0.1 * np.random.randn(num_points)    plt.figure(figsize=(8, 7))    plt.scatter(x1, y1, s=50, label=\"Clase A\", alpha=0.8) # Puntos de la Clase A plt.scatter(x2, y2, s=50, label=\"Clase B\", alpha=0.8) # Puntos de la Clase B  plt.xlabel(\"Caracter\u00edstica X\")  plt.ylabel(\"Caracter\u00edstica Y\") plt.title(\"Dataset de C\u00edrculos con 3 Clases\") plt.legend()  plt.grid(True, linestyle='--', alpha=0.6) plt.axis('equal') plt.show()   In\u00a0[5]: Copied! <pre>df = pd.DataFrame(\n    {\"x\": x1.tolist()+x2.tolist(),\n    \"y\": y1.tolist()+ y2.tolist(),\n    \"labels\":['A' for _ in range(num_points)]+['B' for _ in range(num_points)]\n    })\n\nlabels = df[\"labels\"].unique()\nlabels2id = {label: i for i, label in enumerate(labels)}\nid2labels = {v: k for k, v in labels2id.items()}\n\ndf[\"labels\"] = df[\"labels\"].map(labels2id)\ndf.head()\n</pre> df = pd.DataFrame(     {\"x\": x1.tolist()+x2.tolist(),     \"y\": y1.tolist()+ y2.tolist(),     \"labels\":['A' for _ in range(num_points)]+['B' for _ in range(num_points)]     })  labels = df[\"labels\"].unique() labels2id = {label: i for i, label in enumerate(labels)} id2labels = {v: k for k, v in labels2id.items()}  df[\"labels\"] = df[\"labels\"].map(labels2id) df.head() Out[5]: x y labels 0 0.413433 -0.004978 0 1 0.309648 0.171411 0 2 0.517652 -0.135341 0 3 0.453722 -0.043269 0 4 0.458163 0.212586 0 In\u00a0[6]: Copied! <pre>df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n\nX_train = tensor(df_train[[\"x\",\"y\"]].values).to(device)\nY_train = tensor(df_train[[\"labels\"]].values).to(device)\n\nX_test = tensor(df_test[[\"x\",\"y\"]].values).to(device)\nY_test = tensor(df_test[[\"labels\"]].values).to(device)\n\n\nprint(f\"X_train shape: {X_train.shape}, device: {X_train.device}\")\nprint(f\"Y_train shape: {Y_train.shape}, device: {Y_train.device}\")\nprint(f\"X_test shape: {X_test.shape}, device: {X_test.device}\")\nprint(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\")\n</pre> df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)   X_train = tensor(df_train[[\"x\",\"y\"]].values).to(device) Y_train = tensor(df_train[[\"labels\"]].values).to(device)  X_test = tensor(df_test[[\"x\",\"y\"]].values).to(device) Y_test = tensor(df_test[[\"labels\"]].values).to(device)   print(f\"X_train shape: {X_train.shape}, device: {X_train.device}\") print(f\"Y_train shape: {Y_train.shape}, device: {Y_train.device}\") print(f\"X_test shape: {X_test.shape}, device: {X_test.device}\") print(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\")  <pre>X_train shape: (16000, 2), device: gpu\nY_train shape: (16000, 1), device: gpu\nX_test shape: (4000, 2), device: gpu\nY_test shape: (4000, 1), device: gpu\n</pre> In\u00a0[7]: Copied! <pre>class Network(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = Linear(2, 4)\n        self.relu = ReLU()\n        self.fc2 = Linear(4, 1)\n\n    def forward(self, x: tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n\nnet = Network().to(device)\nnet.parameters()\n</pre> class Network(Module):     def __init__(self):         super().__init__()         self.fc1 = Linear(2, 4)         self.relu = ReLU()         self.fc2 = Linear(4, 1)      def forward(self, x: tensor) -&gt; Tensor:         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         return x   net = Network().to(device) net.parameters() Out[7]: <pre>[Tensor(\n [[-0.17466629 -1.1154755  -0.40821885 -0.16068657]\n  [-1.31859936  1.08693522  0.56908629 -1.04859129]], shape=(2, 4), device=gpu, requires_grad=True),\n Tensor(\n [[0. 0. 0. 0.]], shape=(1, 4), device=gpu, requires_grad=True),\n Tensor(\n [[-1.25809527]\n  [ 0.50759795]\n  [-1.16201892]\n  [ 0.05934924]], shape=(4, 1), device=gpu, requires_grad=True),\n Tensor(\n [[0.]], shape=(1, 1), device=gpu, requires_grad=True)]</pre> In\u00a0[8]: Copied! <pre>criterion = BCEWithLogitsLoss()\noptimizer = RMSprop(net.parameters(), lr=1e-2)\n</pre> criterion = BCEWithLogitsLoss() optimizer = RMSprop(net.parameters(), lr=1e-2) In\u00a0[9]: Copied! <pre>logits = net(X_train)\nlogits\n</pre> logits = net(X_train) logits Out[9]: <pre>Tensor(\n[[ 0.        ]\n [-0.06757853]\n [-0.35889663]\n ...\n [ 0.        ]\n [-2.57556281]\n [-0.07809887]], shape=(16000, 1), device=gpu, requires_grad=True)</pre> In\u00a0[10]: Copied! <pre># Bucle de entrenamiento mejorado\nfor epoch in range(1000 + 1):\n    logits = net(X_train)\n    loss = criterion(logits, Y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        probs = sorix.sigmoid(logits)\n        preds = (probs &gt; 0.5).astype('uint8')\n        acc_train = (preds == Y_train).mean()\n        with sorix.no_grad():\n            logits = net(X_test)\n            probs = sorix.sigmoid(logits)\n            preds = (probs &gt; 0.5).astype('uint8')\n            acc_test = (preds == Y_test).mean()\n\n        # Usamos una f-string para formatear y alinear la salida\n        print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")\n\n        if acc_test.item() &gt; 0.98:\n            break\n</pre> # Bucle de entrenamiento mejorado for epoch in range(1000 + 1):     logits = net(X_train)     loss = criterion(logits, Y_train)      optimizer.zero_grad()     loss.backward()     optimizer.step()      if epoch % 10 == 0:         probs = sorix.sigmoid(logits)         preds = (probs &gt; 0.5).astype('uint8')         acc_train = (preds == Y_train).mean()         with sorix.no_grad():             logits = net(X_test)             probs = sorix.sigmoid(logits)             preds = (probs &gt; 0.5).astype('uint8')             acc_test = (preds == Y_test).mean()          # Usamos una f-string para formatear y alinear la salida         print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")          if acc_test.item() &gt; 0.98:             break  <pre>[gpu] Epoch     0 | Loss: 0.9078 | Acc Train: 50.20% | Acc Test: 42.60%\n[gpu] Epoch    10 | Loss: 0.7468 | Acc Train: 48.02% | Acc Test: 47.90%\n[gpu] Epoch    20 | Loss: 0.6646 | Acc Train: 59.32% | Acc Test: 59.08%\n[gpu] Epoch    30 | Loss: 0.6044 | Acc Train: 66.77% | Acc Test: 65.18%\n[gpu] Epoch    40 | Loss: 0.5514 | Acc Train: 82.98% | Acc Test: 82.38%\n[gpu] Epoch    50 | Loss: 0.5004 | Acc Train: 86.17% | Acc Test: 84.75%\n[gpu] Epoch    60 | Loss: 0.4553 | Acc Train: 86.78% | Acc Test: 85.35%\n[gpu] Epoch    70 | Loss: 0.4158 | Acc Train: 86.88% | Acc Test: 92.70%\n[gpu] Epoch    80 | Loss: 0.3804 | Acc Train: 95.09% | Acc Test: 94.83%\n[gpu] Epoch    90 | Loss: 0.3480 | Acc Train: 96.16% | Acc Test: 95.75%\n[gpu] Epoch   100 | Loss: 0.3175 | Acc Train: 97.07% | Acc Test: 96.73%\n[gpu] Epoch   110 | Loss: 0.2885 | Acc Train: 97.56% | Acc Test: 97.52%\n[gpu] Epoch   120 | Loss: 0.2612 | Acc Train: 98.01% | Acc Test: 97.95%\n</pre> <pre>[gpu] Epoch   130 | Loss: 0.2356 | Acc Train: 98.45% | Acc Test: 98.35%\n</pre> In\u00a0[11]: Copied! <pre>with sorix.no_grad():\n    logits = net(X_test)\n    probs = sorix.sigmoid(logits)\n    preds = (probs &gt; 0.5).astype('uint8')\n    acc_test = (preds == Y_test).mean()\n   \n\ny_pred_labels = [id2labels[y.item()] for y in preds]\n\ndf_test['pred_labels'] = y_pred_labels\n\nfor label in df_test['pred_labels'].unique():\n    x = df_test[df_test['pred_labels'] == label]['x']\n    y = df_test[df_test['pred_labels'] == label]['y']\n\n    plt.scatter(x,y,s=50,label=label)\n\nplt.title(f\"Circles Tetst Dataset: Accuracy: {100*acc_test.item():.2f}%\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n</pre> with sorix.no_grad():     logits = net(X_test)     probs = sorix.sigmoid(logits)     preds = (probs &gt; 0.5).astype('uint8')     acc_test = (preds == Y_test).mean()      y_pred_labels = [id2labels[y.item()] for y in preds]  df_test['pred_labels'] = y_pred_labels  for label in df_test['pred_labels'].unique():     x = df_test[df_test['pred_labels'] == label]['x']     y = df_test[df_test['pred_labels'] == label]['y']      plt.scatter(x,y,s=50,label=label)  plt.title(f\"Circles Tetst Dataset: Accuracy: {100*acc_test.item():.2f}%\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend() Out[11]: <pre>&lt;matplotlib.legend.Legend at 0x7f140838efd0&gt;</pre> In\u00a0[12]: Copied! <pre>sns.heatmap(confusion_matrix(Y_test, preds), annot=True, cmap=\"Blues\", fmt=\".0f\")\n</pre> sns.heatmap(confusion_matrix(Y_test, preds), annot=True, cmap=\"Blues\", fmt=\".0f\") Out[12]: <pre>&lt;Axes: &gt;</pre> In\u00a0[13]: Copied! <pre>print(classification_report(Y_test, preds))\n</pre> print(classification_report(Y_test, preds)) <pre>            precision   recall f1-score  support\n0                0.97     1.00     0.98     1956\n1                1.00     0.97     0.98     2044\n\naccuracy                           0.98     4000\nmacro avg        0.98     0.98     0.98     4000\nweighted avg     0.98     0.98     0.98     4000\n</pre> In\u00a0[14]: Copied! <pre>sorix.save(net.state_dict(),\"model_weights.sor\")\n</pre> sorix.save(net.state_dict(),\"model_weights.sor\") In\u00a0[15]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"model_weights.sor\"))\n\nif X_test.device == 'cpu':\n    with sorix.no_grad():\n        out = net2(X_test)\nif X_test.device == 'gpu':\n    with sorix.no_grad():\n        out = net2(X_test.to('cpu'))\n\nout\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"model_weights.sor\"))  if X_test.device == 'cpu':     with sorix.no_grad():         out = net2(X_test) if X_test.device == 'gpu':     with sorix.no_grad():         out = net2(X_test.to('cpu'))  out Out[15]: <pre>Tensor(\n[[-1.38734515]\n [ 0.40306571]\n [-1.05274762]\n ...\n [-1.3077026 ]\n [-1.01548182]\n [ 0.22951189]], shape=(4000, 1), device=cpu, requires_grad=False)</pre> In\u00a0[16]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"model_weights.sor\"))\nnet2.to('gpu')\n\nif X_test.device == 'cpu':\n    with sorix.no_grad():\n        out = net2(X_test.to('gpu'))\nif X_test.device == 'gpu':\n    with sorix.no_grad():\n        out = net2(X_test)\n\nout\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"model_weights.sor\")) net2.to('gpu')  if X_test.device == 'cpu':     with sorix.no_grad():         out = net2(X_test.to('gpu')) if X_test.device == 'gpu':     with sorix.no_grad():         out = net2(X_test)  out Out[16]: <pre>Tensor(\n[[-1.38734515]\n [ 0.40306571]\n [-1.05274762]\n ...\n [-1.3077026 ]\n [-1.01548182]\n [ 0.22951189]], shape=(4000, 1), device=gpu, requires_grad=False)</pre>"},{"location":"examples/nn/2.1-classification-binary/#classification-binary","title":"Classification Binary\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#datos","title":"Datos\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#training","title":"Training\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#prediction","title":"Prediction\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#save-and-load-model","title":"Save and Load Model\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#cpu","title":"CPU\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#gpu","title":"GPU\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/","title":"Classification Multiclass","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line to install GPU version with Cupy backend\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the following line to install GPU version with Cupy backend #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[2]: Copied! <pre>import joblib\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\nimport sorix\nfrom sorix.nn import ReLU,Linear\nfrom sorix.optim import SGDMomentum,RMSprop,Adam,SGD\nfrom sorix import tensor,Tensor\nfrom sorix.nn import Module\nfrom sorix.nn import CrossEntropyLoss\nfrom sorix.metrics import confusion_matrix,classification_report\nfrom sorix.model_selection import train_test_split\n</pre> import joblib import seaborn as sns import numpy as np import pandas as pd import matplotlib.pyplot as plt   import sorix from sorix.nn import ReLU,Linear from sorix.optim import SGDMomentum,RMSprop,Adam,SGD from sorix import tensor,Tensor from sorix.nn import Module from sorix.nn import CrossEntropyLoss from sorix.metrics import confusion_matrix,classification_report from sorix.model_selection import train_test_split In\u00a0[3]: Copied! <pre>device = 'gpu' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'gpu' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 13.6.0\n</pre> Out[3]: <pre>'gpu'</pre> In\u00a0[4]: Copied! <pre>r1 = 0.5  \nr2 = 1.5  \nr3 = 1  \n\nnum_points = 10000\nthetas = np.linspace(0, 2 * np.pi, num_points) \n\n\nx1 = r1 * np.cos(thetas) + 0.1 * np.random.randn(num_points)\ny1 = r1 * np.sin(thetas) + 0.1 * np.random.randn(num_points)\n\n\nx2 = r2 * np.cos(thetas) + 0.1 * np.random.randn(num_points)\ny2 = r2 * np.sin(thetas) + 0.1 * np.random.randn(num_points)\n\n\nx3 = 5 + r3 * np.cos(thetas) + 0.1 * np.random.randn(num_points)\ny3 = 5 + r3 * np.sin(thetas) + 0.1 * np.random.randn(num_points)\n\n\nplt.figure(figsize=(8, 7)) \n\n\nplt.scatter(x1, y1, s=50, label=\"Clase A\", alpha=0.8) # Puntos de la Clase A\nplt.scatter(x2, y2, s=50, label=\"Clase B\", alpha=0.8) # Puntos de la Clase B\nplt.scatter(x3, y3, s=50, label=\"Clase C\", alpha=0.8) # Puntos de la Clase C\n\nplt.xlabel(\"Caracter\u00edstica X\") \nplt.ylabel(\"Caracter\u00edstica Y\")\nplt.title(\"Dataset de C\u00edrculos con 3 Clases\")\nplt.legend() \nplt.grid(True, linestyle='--', alpha=0.6)\nplt.axis('equal')\nplt.show() \n</pre> r1 = 0.5   r2 = 1.5   r3 = 1    num_points = 10000 thetas = np.linspace(0, 2 * np.pi, num_points)    x1 = r1 * np.cos(thetas) + 0.1 * np.random.randn(num_points) y1 = r1 * np.sin(thetas) + 0.1 * np.random.randn(num_points)   x2 = r2 * np.cos(thetas) + 0.1 * np.random.randn(num_points) y2 = r2 * np.sin(thetas) + 0.1 * np.random.randn(num_points)   x3 = 5 + r3 * np.cos(thetas) + 0.1 * np.random.randn(num_points) y3 = 5 + r3 * np.sin(thetas) + 0.1 * np.random.randn(num_points)   plt.figure(figsize=(8, 7))    plt.scatter(x1, y1, s=50, label=\"Clase A\", alpha=0.8) # Puntos de la Clase A plt.scatter(x2, y2, s=50, label=\"Clase B\", alpha=0.8) # Puntos de la Clase B plt.scatter(x3, y3, s=50, label=\"Clase C\", alpha=0.8) # Puntos de la Clase C  plt.xlabel(\"Caracter\u00edstica X\")  plt.ylabel(\"Caracter\u00edstica Y\") plt.title(\"Dataset de C\u00edrculos con 3 Clases\") plt.legend()  plt.grid(True, linestyle='--', alpha=0.6) plt.axis('equal') plt.show()   In\u00a0[5]: Copied! <pre>df = pd.DataFrame(\n    {\"x\": x1.tolist()+x2.tolist()+x3.tolist(),\n    \"y\": y1.tolist()+y2.tolist()+y3.tolist(),\n    \"labels\":['A' for _ in range(num_points)]+['B' for _ in range(num_points)]+['C' for _ in range(num_points)]\n    })\n\nlabels = df[\"labels\"].unique()\nlabels2id = {label: i for i, label in enumerate(labels)}\nid2labels = {v: k for k, v in labels2id.items()}\n\ndf[\"labels\"] = df[\"labels\"].map(labels2id)\ndf.head()\n</pre> df = pd.DataFrame(     {\"x\": x1.tolist()+x2.tolist()+x3.tolist(),     \"y\": y1.tolist()+y2.tolist()+y3.tolist(),     \"labels\":['A' for _ in range(num_points)]+['B' for _ in range(num_points)]+['C' for _ in range(num_points)]     })  labels = df[\"labels\"].unique() labels2id = {label: i for i, label in enumerate(labels)} id2labels = {v: k for k, v in labels2id.items()}  df[\"labels\"] = df[\"labels\"].map(labels2id) df.head() Out[5]: x y labels 0 0.404847 -0.077124 0 1 0.479746 -0.063604 0 2 0.577334 0.074238 0 3 0.644964 0.120343 0 4 0.539449 -0.040765 0 In\u00a0[6]: Copied! <pre>df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n\nX_train = tensor(df_train[[\"x\",\"y\"]].values).to(device)\nY_train = tensor(df_train[[\"labels\"]].values).to(device)\nX_test = tensor(df_test[[\"x\",\"y\"]].values).to(device)\nY_test = tensor(df_test[[\"labels\"]].values).to(device)\n\nprint(f\"X_train shape: {X_train.shape}, device: {X_train.device}\")\nprint(f\"Y_train shape: {Y_train.shape}, device: {Y_train.device}\")\nprint(f\"X_test shape: {X_test.shape}, device: {X_test.device}\")\nprint(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\")\n</pre> df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)   X_train = tensor(df_train[[\"x\",\"y\"]].values).to(device) Y_train = tensor(df_train[[\"labels\"]].values).to(device) X_test = tensor(df_test[[\"x\",\"y\"]].values).to(device) Y_test = tensor(df_test[[\"labels\"]].values).to(device)  print(f\"X_train shape: {X_train.shape}, device: {X_train.device}\") print(f\"Y_train shape: {Y_train.shape}, device: {Y_train.device}\") print(f\"X_test shape: {X_test.shape}, device: {X_test.device}\") print(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\")  <pre>X_train shape: (24000, 2), device: gpu\nY_train shape: (24000, 1), device: gpu\nX_test shape: (6000, 2), device: gpu\nY_test shape: (6000, 1), device: gpu\n</pre> In\u00a0[7]: Copied! <pre>X_train.grad\n</pre> X_train.grad In\u00a0[8]: Copied! <pre>class Network(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = Linear(2, 4)\n        self.relu = ReLU()\n        self.fc2 = Linear(4, 4)\n        self.fc3 = Linear(4, 3)\n\n    def forward(self, x: tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n\nnet = Network().to(device)\nnet.parameters()\n</pre> class Network(Module):     def __init__(self):         super().__init__()         self.fc1 = Linear(2, 4)         self.relu = ReLU()         self.fc2 = Linear(4, 4)         self.fc3 = Linear(4, 3)      def forward(self, x: tensor) -&gt; Tensor:         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         x = self.relu(x)         x = self.fc3(x)         return x   net = Network().to(device) net.parameters() Out[8]: <pre>[Tensor(\n [[ 0.82184812  1.65849734 -0.21329843 -0.22710424]\n  [-1.28396598  1.08825148 -0.06972136 -0.03704953]], shape=(2, 4), device=gpu, requires_grad=True),\n Tensor(\n [[0. 0. 0. 0.]], shape=(1, 4), device=gpu, requires_grad=True),\n Tensor(\n [[ 0.30648765  0.21343028 -0.09919398  0.93407362]\n  [-1.13397806  0.14940732  0.28791216 -0.29160606]\n  [ 0.43589576  0.46887451  0.78475726 -0.0788846 ]\n  [-0.15354739  0.38126807 -0.13964173 -1.07575263]], shape=(4, 4), device=gpu, requires_grad=True),\n Tensor(\n [[0. 0. 0. 0.]], shape=(1, 4), device=gpu, requires_grad=True),\n Tensor(\n [[ 0.60982541 -1.29703549  0.28402848]\n  [ 1.79498594  0.37703977 -0.49080712]\n  [-0.31916267  0.26207058 -0.01511212]\n  [ 0.53475129  1.02536398  0.9294297 ]], shape=(4, 3), device=gpu, requires_grad=True),\n Tensor(\n [[0. 0. 0.]], shape=(1, 3), device=gpu, requires_grad=True)]</pre> In\u00a0[9]: Copied! <pre>criterion = CrossEntropyLoss()\noptimizer = RMSprop(net.parameters(), lr=1e-2)\n</pre> criterion = CrossEntropyLoss() optimizer = RMSprop(net.parameters(), lr=1e-2) In\u00a0[10]: Copied! <pre>logits = net(X_train)\nlogits\n</pre> logits = net(X_train) logits Out[10]: <pre>Tensor(\n[[ 0.10213395  0.07634896 -0.04500389]\n [ 2.682305    2.00512379 -1.18192011]\n [ 2.45102165  1.83223079 -1.08000834]\n ...\n [ 2.11946305  1.58437828 -0.93391169]\n [ 2.09147357  1.56345509 -0.92157852]\n [ 0.33371207  0.17142169 -0.12276829]], shape=(24000, 3), device=gpu, requires_grad=True)</pre> In\u00a0[11]: Copied! <pre>preds = sorix.argmax(logits, axis=1, keepdims=True)\npreds\n</pre> preds = sorix.argmax(logits, axis=1, keepdims=True) preds Out[11]: <pre>Tensor(\n[[0]\n [0]\n [0]\n ...\n [0]\n [0]\n [0]], shape=(24000, 1), device=gpu, requires_grad=False)</pre> In\u00a0[12]: Copied! <pre># Bucle de entrenamiento mejorado\nfor epoch in range(10000 + 1):\n    logits = net(X_train)\n    loss = criterion(logits, Y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        preds = sorix.argmax(logits, axis=1, keepdims=True)\n        acc_train = (preds == Y_train).mean()\n        with sorix.no_grad():\n            logits = net(X_test)\n            preds = sorix.argmax(logits, axis=1, keepdims=True)\n            acc_test = (preds == Y_test).mean()\n\n        # Usamos una f-string para formatear y alinear la salida\n        print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")\n\n        if acc_test.item() &gt;= 0.98:  # Mejoramos el criterio de parada\n            print(f\"Entrenamiento completado en {epoch} epochs!\")\n            break\n</pre> # Bucle de entrenamiento mejorado for epoch in range(10000 + 1):     logits = net(X_train)     loss = criterion(logits, Y_train)      optimizer.zero_grad()     loss.backward()     optimizer.step()      if epoch % 10 == 0:         preds = sorix.argmax(logits, axis=1, keepdims=True)         acc_train = (preds == Y_train).mean()         with sorix.no_grad():             logits = net(X_test)             preds = sorix.argmax(logits, axis=1, keepdims=True)             acc_test = (preds == Y_test).mean()          # Usamos una f-string para formatear y alinear la salida         print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")          if acc_test.item() &gt;= 0.98:  # Mejoramos el criterio de parada             print(f\"Entrenamiento completado en {epoch} epochs!\")             break  <pre>[gpu] Epoch     0 | Loss: 2.0361 | Acc Train: 33.60% | Acc Test: 20.95%\n[gpu] Epoch    10 | Loss: 0.8269 | Acc Train: 77.52% | Acc Test: 78.42%\n[gpu] Epoch    20 | Loss: 0.5957 | Acc Train: 79.42% | Acc Test: 80.10%\n[gpu] Epoch    30 | Loss: 0.4900 | Acc Train: 80.76% | Acc Test: 81.73%\n[gpu] Epoch    40 | Loss: 0.4191 | Acc Train: 83.40% | Acc Test: 83.92%\n[gpu] Epoch    50 | Loss: 0.3621 | Acc Train: 85.90% | Acc Test: 86.37%\n[gpu] Epoch    60 | Loss: 0.3075 | Acc Train: 88.31% | Acc Test: 88.43%\n</pre> <pre>[gpu] Epoch    70 | Loss: 0.2532 | Acc Train: 94.07% | Acc Test: 93.72%\n[gpu] Epoch    80 | Loss: 0.2040 | Acc Train: 96.85% | Acc Test: 96.33%\n[gpu] Epoch    90 | Loss: 0.1629 | Acc Train: 98.18% | Acc Test: 97.83%\n</pre> <pre>[gpu] Epoch   100 | Loss: 0.1300 | Acc Train: 99.03% | Acc Test: 99.02%\nEntrenamiento completado en 100 epochs!\n</pre> In\u00a0[13]: Copied! <pre>with sorix.no_grad():\n    logits = net(X_test)\n\npreds = sorix.argmax(logits, axis=1, keepdims=True)\nacc = (preds == Y_test).mean()   \n\n\ny_pred_labels = [id2labels[y.item()] for y in preds]\n\ndf_test['pred_labels'] = y_pred_labels\n\nfor label in df_test['pred_labels'].unique():\n    x = df_test[df_test['pred_labels'] == label]['x']\n    y = df_test[df_test['pred_labels'] == label]['y']\n\n    plt.scatter(x,y,s=50,label=label)\n\nplt.title(f\"Circles Tetst Dataset: Accuracy: {100*acc.item():.2f}%\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n</pre> with sorix.no_grad():     logits = net(X_test)  preds = sorix.argmax(logits, axis=1, keepdims=True) acc = (preds == Y_test).mean()      y_pred_labels = [id2labels[y.item()] for y in preds]  df_test['pred_labels'] = y_pred_labels  for label in df_test['pred_labels'].unique():     x = df_test[df_test['pred_labels'] == label]['x']     y = df_test[df_test['pred_labels'] == label]['y']      plt.scatter(x,y,s=50,label=label)  plt.title(f\"Circles Tetst Dataset: Accuracy: {100*acc.item():.2f}%\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend() Out[13]: <pre>&lt;matplotlib.legend.Legend at 0x7f87e8bc7b10&gt;</pre> In\u00a0[14]: Copied! <pre>sns.heatmap(confusion_matrix(Y_test, preds), annot=True, cmap=\"Blues\", fmt=\".0f\")\n</pre> sns.heatmap(confusion_matrix(Y_test, preds), annot=True, cmap=\"Blues\", fmt=\".0f\") Out[14]: <pre>&lt;Axes: &gt;</pre> In\u00a0[15]: Copied! <pre>print(classification_report(Y_test, preds))\n</pre> print(classification_report(Y_test, preds)) <pre>            precision   recall f1-score  support\n0                0.97     1.00     0.98     1917\n1                1.00     0.97     0.98     1989\n2                1.00     1.00     1.00     2094\n\naccuracy                           0.99     6000\nmacro avg        0.99     0.99     0.99     6000\nweighted avg     0.99     0.99     0.99     6000\n</pre> In\u00a0[16]: Copied! <pre>sorix.save(net.state_dict(),\"model_weights.sor\")\n</pre> sorix.save(net.state_dict(),\"model_weights.sor\") In\u00a0[17]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"model_weights.sor\"))\n\nif X_test.device == 'cpu':\n    with sorix.no_grad():\n        logits = net2(X_test)\nif X_test.device == 'gpu':\n    with sorix.no_grad():\n        logits = net2(X_test.to('cpu'))\n\nlogits\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"model_weights.sor\"))  if X_test.device == 'cpu':     with sorix.no_grad():         logits = net2(X_test) if X_test.device == 'gpu':     with sorix.no_grad():         logits = net2(X_test.to('cpu'))  logits Out[17]: <pre>Tensor(\n[[-21.3093084    5.52856196  10.63325664]\n [-23.51549906   6.26606373  12.12745669]\n [-20.4343828    5.23671019  10.2695404 ]\n ...\n [  4.05182728   2.2906377   -2.86907778]\n [  1.50833004   1.9528017   -1.5003316 ]\n [-26.48189176   7.87412363  13.98874776]], shape=(6000, 3), device=cpu, requires_grad=False)</pre> In\u00a0[18]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"model_weights.sor\"))\nnet2.to('gpu')\n\nif X_test.device == 'cpu':\n    with sorix.no_grad():\n        logits = net2(X_test.to('gpu'))\nif X_test.device == 'gpu':\n    with sorix.no_grad():\n        logits = net2(X_test)\n\nlogits\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"model_weights.sor\")) net2.to('gpu')  if X_test.device == 'cpu':     with sorix.no_grad():         logits = net2(X_test.to('gpu')) if X_test.device == 'gpu':     with sorix.no_grad():         logits = net2(X_test)  logits Out[18]: <pre>Tensor(\n[[-21.3093084    5.52856196  10.63325664]\n [-23.51549906   6.26606373  12.12745669]\n [-20.4343828    5.23671019  10.2695404 ]\n ...\n [  4.05182728   2.2906377   -2.86907778]\n [  1.50833004   1.9528017   -1.5003316 ]\n [-26.48189176   7.87412363  13.98874776]], shape=(6000, 3), device=gpu, requires_grad=False)</pre>"},{"location":"examples/nn/2.2-classification-multiclass/#classification-multiclass","title":"Classification Multiclass\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#datos","title":"Datos\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#training","title":"Training\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#prediction","title":"Prediction\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#save-and-load-model","title":"Save and Load Model\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#cpu","title":"CPU\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#gpu","title":"GPU\u00b6","text":""},{"location":"examples/nn/3-Iris_dataset/","title":"Iris Dataset","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line to install GPU version with Cupy backend\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the following line to install GPU version with Cupy backend #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom typing import Union\n\nimport sorix\nfrom sorix.model_selection import train_test_split\nfrom sorix.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\nfrom sorix.nn import Module\nfrom sorix import tensor,Tensor\nfrom sorix.nn import CrossEntropyLoss\nfrom sorix.metrics import accuracy_score\nfrom sorix.optim import RMSprop\nfrom sorix.nn import Linear,ReLU\nfrom sorix.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from typing import Union  import sorix from sorix.model_selection import train_test_split from sorix.preprocessing import StandardScaler,MinMaxScaler,RobustScaler from sorix.nn import Module from sorix import tensor,Tensor from sorix.nn import CrossEntropyLoss from sorix.metrics import accuracy_score from sorix.optim import RMSprop from sorix.nn import Linear,ReLU from sorix.metrics import confusion_matrix, classification_report import seaborn as sns In\u00a0[3]: Copied! <pre>device = \"gpu\" if sorix.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"gpu\" if sorix.cuda.is_available() else \"cpu\" device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 13.6.0\n</pre> Out[3]: <pre>'gpu'</pre> In\u00a0[4]: Copied! <pre>df  = pd.read_csv(\"../data/Iris.csv\")\ndf.head()\n</pre> df  = pd.read_csv(\"../data/Iris.csv\") df.head() Out[4]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[5]: Copied! <pre>labels = df[\"Species\"].unique()\n\nlabels2id = {label: i for i, label in enumerate(labels)}\nid2labels = {i: label for i, label in enumerate(labels)}\n</pre> labels = df[\"Species\"].unique()  labels2id = {label: i for i, label in enumerate(labels)} id2labels = {i: label for i, label in enumerate(labels)} In\u00a0[6]: Copied! <pre>df['labels'] = df['Species'].map(labels2id)\ndf.head()\n</pre> df['labels'] = df['Species'].map(labels2id) df.head() Out[6]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species labels 0 1 5.1 3.5 1.4 0.2 Iris-setosa 0 1 2 4.9 3.0 1.4 0.2 Iris-setosa 0 2 3 4.7 3.2 1.3 0.2 Iris-setosa 0 3 4 4.6 3.1 1.5 0.2 Iris-setosa 0 4 5 5.0 3.6 1.4 0.2 Iris-setosa 0 In\u00a0[7]: Copied! <pre>df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n</pre> df_train, df_test = train_test_split(df, test_size=0.2, random_state=42) In\u00a0[8]: Copied! <pre>features = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\nlabels = [\"labels\"]\n\nX_train = df_train[features]\ny_train = df_train[labels]\n\nX_test = df_test[features]\ny_test = df_test[labels]\n</pre> features = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"] labels = [\"labels\"]  X_train = df_train[features] y_train = df_train[labels]  X_test = df_test[features] y_test = df_test[labels] In\u00a0[9]: Copied! <pre>plt.scatter(X_train['SepalLengthCm'], X_train['SepalWidthCm'], c=y_train['labels'])\n</pre> plt.scatter(X_train['SepalLengthCm'], X_train['SepalWidthCm'], c=y_train['labels']) Out[9]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f1bde52cc20&gt;</pre> In\u00a0[10]: Copied! <pre>scaler = StandardScaler()\n\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</pre> scaler = StandardScaler()  scaler.fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) In\u00a0[11]: Copied! <pre>X_train_scaled.shape, y_train.shape, X_test_scaled.shape, y_test.shape\n</pre> X_train_scaled.shape, y_train.shape, X_test_scaled.shape, y_test.shape Out[11]: <pre>((120, 4), (120, 1), (30, 4), (30, 1))</pre> In\u00a0[12]: Copied! <pre>X_train_tensor = tensor(X_train_scaled).to(device)\nY_train_tensor = tensor(y_train).to(device)\n\nX_test_tensor = tensor(X_test_scaled).to(device)\nY_test_tensor = tensor(y_test).to(device)\n\nprint(f\"X_train shape: {X_train_tensor.shape}, device: {X_train_tensor.device}\")\nprint(f\"Y_train shape: {Y_train_tensor.shape}, device: {Y_train_tensor.device}\")\nprint(f\"X_test shape: {X_test_tensor.shape}, device: {X_test_tensor.device}\")\nprint(f\"Y_test shape: {Y_test_tensor.shape}, device: {Y_test_tensor.device}\")\n</pre>  X_train_tensor = tensor(X_train_scaled).to(device) Y_train_tensor = tensor(y_train).to(device)  X_test_tensor = tensor(X_test_scaled).to(device) Y_test_tensor = tensor(y_test).to(device)  print(f\"X_train shape: {X_train_tensor.shape}, device: {X_train_tensor.device}\") print(f\"Y_train shape: {Y_train_tensor.shape}, device: {Y_train_tensor.device}\") print(f\"X_test shape: {X_test_tensor.shape}, device: {X_test_tensor.device}\") print(f\"Y_test shape: {Y_test_tensor.shape}, device: {Y_test_tensor.device}\") <pre>X_train shape: (120, 4), device: gpu\nY_train shape: (120, 1), device: gpu\nX_test shape: (30, 4), device: gpu\nY_test shape: (30, 1), device: gpu\n</pre> In\u00a0[13]: Copied! <pre>class Net(Module):\n\n    def __init__(self):\n        super().__init__()\n        self.fc1 = Linear(4, 8)\n        self.fc2 = Linear(8, 4)\n        self.fc3 = Linear(4, 3)\n        self.relu = ReLU()\n\n    def forward(self, x: tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n    \n\nnet = Net().to(device)\ncriterion = CrossEntropyLoss()\noptimizer = RMSprop(net.parameters(), lr=1e-2)\n</pre> class Net(Module):      def __init__(self):         super().__init__()         self.fc1 = Linear(4, 8)         self.fc2 = Linear(8, 4)         self.fc3 = Linear(4, 3)         self.relu = ReLU()      def forward(self, x: tensor) -&gt; Tensor:         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         x = self.relu(x)         x = self.fc3(x)         return x       net = Net().to(device) criterion = CrossEntropyLoss() optimizer = RMSprop(net.parameters(), lr=1e-2)  In\u00a0[14]: Copied! <pre>logits = net(X_train_tensor)\nlogits\n</pre> logits = net(X_train_tensor) logits Out[14]: <pre>Tensor(\n[[ 3.18380978e-02  5.91686011e-02 -7.86397258e-03]\n [ 3.13370189e+00  5.74086680e+00 -7.45613291e-01]\n [-6.27524348e-02 -1.36923843e+00  1.58636660e+00]\n [ 3.68812564e-03 -3.08446115e-01  4.18230145e-01]\n [-2.65776481e-02 -2.79021984e-01  2.54321874e-01]\n [ 2.34201064e+00  4.24755242e+00 -5.85250000e-01]\n [ 8.82216612e-03  2.42461533e-03  1.40565944e-02]\n [ 8.13916040e-02 -1.05761434e+00  1.66882102e+00]\n [-2.64715363e-02 -1.49514679e-01  9.24898379e-02]\n [-5.54240800e-04 -1.86377690e-02 -4.84749180e-02]\n [ 3.60032862e-02 -8.28358582e-01  1.21232245e+00]\n [ 2.06400811e+00  3.68548420e+00 -5.50437948e-01]\n [ 2.88001810e+00  5.23662288e+00 -7.55856643e-01]\n [ 2.21337037e+00  3.97225899e+00 -5.78986076e-01]\n [ 3.18288497e+00  5.80945827e+00 -6.86235563e-01]\n [ 1.45123404e-02 -5.24926661e-01  7.39887746e-01]\n [ 2.46062219e-02 -1.00287910e+00  1.40291057e+00]\n [-5.21423889e-02 -1.27879189e-01 -4.72920095e-02]\n [-5.65749499e-03 -3.95742808e-02  3.27227906e-02]\n [ 2.88399240e-02 -9.18760099e-01  1.30674498e+00]\n [ 2.19221983e+00  3.93667512e+00 -5.17301589e-01]\n [ 8.97615143e-03 -5.72666369e-01  7.83763092e-01]\n [ 2.29760221e+00  4.15590948e+00 -5.26909131e-01]\n [ 2.39446354e-02 -8.61873020e-01  1.21521531e+00]\n [-3.89442531e-02 -1.43634851e+00  1.75593016e+00]\n [ 6.55337482e-02 -1.01456620e+00  1.55805239e+00]\n [-5.04178142e-02 -6.63503502e-01  7.00384967e-01]\n [ 4.32011034e-02 -1.16846244e+00  1.68417142e+00]\n [ 1.77281699e+00  3.15398152e+00 -4.43245821e-01]\n [ 2.02470355e+00  3.62813610e+00 -4.86150803e-01]\n [ 3.13682894e+00  5.67645242e+00 -7.30882106e-01]\n [ 4.04362143e+00  7.43207182e+00 -7.80373087e-01]\n [-5.05755063e-02 -4.29726045e-01  3.92407754e-01]\n [ 2.54596953e+00  4.60337036e+00 -5.74363639e-01]\n [ 2.30134079e+00  4.11463910e+00 -5.71982947e-01]\n [ 2.10389717e-02 -6.10826779e-01  8.75143434e-01]\n [-9.43019079e-04 -4.67122428e-01  6.11088075e-01]\n [ 2.78381897e+00  5.05903670e+00 -6.53340541e-01]\n [ 2.97422691e+00  5.40743412e+00 -6.47727969e-01]\n [ 3.95251825e+00  7.24417626e+00 -8.03992651e-01]\n [-8.61207935e-03 -5.44252648e-01  6.86330622e-01]\n [-1.79378130e-03 -4.57687597e-01  5.95774960e-01]\n [-3.87303388e-02 -5.21625127e-01  5.53716040e-01]\n [ 3.30825144e+00  6.05562176e+00 -7.02709813e-01]\n [ 3.14721004e+00  5.74610686e+00 -7.31635687e-01]\n [-9.77670637e-02 -2.30847619e-01 -8.92878209e-02]\n [-3.39432617e-02 -3.70475442e-01  3.71288840e-01]\n [ 6.65224077e-03 -7.32668980e-01  9.86244694e-01]\n [-4.89626870e-02 -3.82594040e-01  3.35932833e-01]\n [ 6.60773600e-02 -1.46109642e+00  2.14713684e+00]\n [-1.15041957e-02 -8.57852346e-02  7.35272521e-02]\n [-5.57456036e-02 -1.26605881e+00  1.47460513e+00]\n [-2.70143263e-02 -2.49851595e-01  2.36320594e-01]\n [ 2.80564138e+00  5.09444178e+00 -6.36834249e-01]\n [-9.94160232e-02 -1.16481897e+00  1.19232599e+00]\n [ 8.81469102e-02  1.41017481e-01 -8.47095100e-02]\n [ 2.64496757e+00  4.79361711e+00 -6.51898150e-01]\n [ 2.70862071e+00  4.91054511e+00 -6.09610640e-01]\n [ 2.91926935e+00  5.33392078e+00 -6.28574510e-01]\n [-5.69323985e-02 -1.83996184e-01 -4.85782905e-02]\n [ 1.91095213e-02 -5.71609063e-01  8.16979265e-01]\n [ 2.44987667e+00  4.43411204e+00 -5.48691084e-01]\n [ 1.87454059e+00  3.34866663e+00 -5.28320442e-01]\n [ 1.95499243e+00  3.52732925e+00 -4.56857223e-01]\n [-1.39832830e-02 -3.81138186e-02 -1.24192526e-02]\n [ 2.60212343e+00  4.71550162e+00 -6.14192968e-01]\n [-1.74164007e-03 -3.97214845e-03 -1.60025342e-03]\n [ 8.84678060e-03 -1.56237154e+00  2.08487383e+00]\n [ 2.25639285e+00  4.04456660e+00 -5.41551924e-01]\n [-2.13781898e-02 -1.49540828e-01  1.23650844e-01]\n [ 1.83769909e-02 -4.92952112e-01  7.11036525e-01]\n [ 4.01361063e+00  7.36482474e+00 -8.13147379e-01]\n [ 5.43918675e-03 -5.39444572e-01  7.27994258e-01]\n [-8.61207935e-03 -5.44252648e-01  6.86330622e-01]\n [ 1.84389474e-01  2.88705419e-01 -1.94539601e-01]\n [-2.61323815e-02 -1.82796486e-01  1.51148954e-01]\n [ 1.42086486e-02 -1.02002887e+00  1.38995516e+00]\n [ 1.62906732e-02 -5.67342081e-05  1.75329344e-02]\n [ 3.15816851e+00  5.78813876e+00 -6.55640182e-01]\n [-7.37799968e-02 -2.10869817e-01 -6.48543182e-02]\n [ 1.97121306e-02 -6.98977696e-01  9.86538614e-01]\n [ 2.45276117e+00  4.42872683e+00 -5.85644840e-01]\n [ 2.34624305e+00  4.22006752e+00 -5.94801894e-01]\n [-4.58528486e-02 -1.23924876e-01 -4.07969137e-02]\n [-1.55761181e-02 -6.71523843e-01  8.29920855e-01]\n [ 2.21337037e+00  3.97225899e+00 -5.78986076e-01]\n [ 1.37889246e-02 -8.47158922e-01  1.16118201e+00]\n [ 2.59161257e+00  4.69414642e+00 -5.87087230e-01]\n [ 2.24495151e+00  4.06386255e+00 -5.36941886e-01]\n [-5.86844469e-03 -5.57119983e-01  7.12622170e-01]\n [-5.34681434e-02 -1.62438713e-01 -4.63365385e-02]\n [ 5.75974337e-02 -1.15981790e+00  1.72196819e+00]\n [-7.66682190e-03 -3.74621630e-01  4.66478526e-01]\n [-1.93847931e-02 -1.35596982e-01  1.12121094e-01]\n [-1.66016856e-02 -1.43525840e+00  1.83079902e+00]\n [ 1.07662058e-02 -2.58288525e-03  1.17626335e-02]\n [ 1.71668502e+00  3.02509819e+00 -4.54053234e-01]\n [ 2.12903580e+00  3.77705160e+00 -5.44621398e-01]\n [-1.16346978e-02 -9.66011551e-02  8.10755049e-02]\n [-4.86084238e-02 -9.53046861e-01  1.08734016e+00]\n [ 2.04438521e+00  3.65162262e+00 -4.99071310e-01]\n [ 2.66797020e+00  4.84329054e+00 -5.88822106e-01]\n [ 1.93382889e+00  3.42756950e+00 -4.99609946e-01]\n [-3.85869267e-02 -2.69916257e-01  2.23185690e-01]\n [ 1.90211133e-02 -1.12409319e+00  1.54324435e+00]\n [ 2.50570142e+00  4.51384533e+00 -6.00848052e-01]\n [ 4.05301879e-02 -9.79425837e-01  1.42644956e+00]\n [ 1.15126869e-02 -1.21833117e+00  1.64153376e+00]\n [ 2.28844849e+00  4.10618534e+00 -5.43055479e-01]\n [-1.89513015e-02 -2.51221783e-01  2.65658676e-01]\n [-4.04414436e-02 -4.00419450e-01  3.88475901e-01]\n [ 2.39035714e-02 -5.41035511e-01  7.93144490e-01]\n [-7.62890243e-02 -5.34633184e-01  4.42555579e-01]\n [ 3.62928218e-02 -1.00766436e+00  1.44911470e+00]\n [ 3.26473523e+00  5.96281248e+00 -6.88166392e-01]\n [ 4.13973548e-02 -1.01511884e+00  1.47635059e+00]\n [ 5.31529891e-03 -2.65784306e-01  3.67682960e-01]\n [ 3.65343759e-02 -8.05372267e-01  1.18390708e+00]\n [-8.60919465e-02 -2.00252296e-01 -8.98781582e-02]\n [-2.30541372e-02 -6.82637978e-02 -2.01015588e-02]], shape=(120, 3), device=gpu, requires_grad=True)</pre> In\u00a0[15]: Copied! <pre># Bucle de entrenamiento mejorado\nfor epoch in range(1000 + 1):\n    logits = net(X_train_tensor)\n    loss = criterion(logits, Y_train_tensor)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        preds = sorix.argmax(logits, axis=1, keepdims=True)\n        acc_train = (preds== Y_train_tensor).mean()\n        with sorix.no_grad():\n            logits = net(X_test_tensor)\n            preds = sorix.argmax(logits, axis=1, keepdims=True)\n            acc_test = (preds == Y_test_tensor).mean()\n\n        # Usamos una f-string para formatear y alinear la salida\n        print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")\n\n        if acc_test.item() &gt;= 0.96:  # Mejoramos el criterio de parada\n            print(f\"Entrenamiento completado en {epoch} epochs!\")\n            break\n</pre> # Bucle de entrenamiento mejorado for epoch in range(1000 + 1):     logits = net(X_train_tensor)     loss = criterion(logits, Y_train_tensor)      optimizer.zero_grad()     loss.backward()     optimizer.step()      if epoch % 10 == 0:         preds = sorix.argmax(logits, axis=1, keepdims=True)         acc_train = (preds== Y_train_tensor).mean()         with sorix.no_grad():             logits = net(X_test_tensor)             preds = sorix.argmax(logits, axis=1, keepdims=True)             acc_test = (preds == Y_test_tensor).mean()          # Usamos una f-string para formatear y alinear la salida         print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")          if acc_test.item() &gt;= 0.96:  # Mejoramos el criterio de parada             print(f\"Entrenamiento completado en {epoch} epochs!\")             break  <pre>[gpu] Epoch     0 | Loss: 1.3640 | Acc Train: 34.17% | Acc Test: 50.00%\n</pre> <pre>[gpu] Epoch    10 | Loss: 0.4880 | Acc Train: 85.83% | Acc Test: 93.33%\n[gpu] Epoch    20 | Loss: 0.2957 | Acc Train: 90.83% | Acc Test: 93.33%\n[gpu] Epoch    30 | Loss: 0.1825 | Acc Train: 94.17% | Acc Test: 100.00%\nEntrenamiento completado en 30 epochs!\n</pre> In\u00a0[16]: Copied! <pre>with sorix.no_grad():\n    logits = net(X_test_tensor)\n    preds = sorix.argmax(logits, axis=1, keepdims=True)\n</pre> with sorix.no_grad():     logits = net(X_test_tensor)     preds = sorix.argmax(logits, axis=1, keepdims=True) In\u00a0[17]: Copied! <pre>sns.heatmap(confusion_matrix(Y_test_tensor, preds), annot=True, cmap=\"Blues\")\n</pre> sns.heatmap(confusion_matrix(Y_test_tensor, preds), annot=True, cmap=\"Blues\") Out[17]: <pre>&lt;Axes: &gt;</pre> In\u00a0[18]: Copied! <pre>print(classification_report(Y_test_tensor, preds))\n</pre> print(classification_report(Y_test_tensor, preds)) <pre>            precision   recall f1-score  support\n0                1.00     1.00     1.00        7\n1                1.00     1.00     1.00       11\n2                1.00     1.00     1.00       12\n\naccuracy                           1.00       30\nmacro avg        1.00     1.00     1.00       30\nweighted avg     1.00     1.00     1.00       30\n</pre>"},{"location":"examples/nn/3-Iris_dataset/#iris-dataset","title":"Iris Dataset\u00b6","text":""},{"location":"examples/nn/4-digit-recognizer/","title":"MNIST Dataset","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line to install GPU version with Cupy backend\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the following line to install GPU version with Cupy backend #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sorix\nfrom sorix import tensor\nfrom sorix.nn import Module,Linear, CrossEntropyLoss,ReLU,BatchNorm1d\nfrom sorix.optim import SGDMomentum, RMSprop, Adam\nfrom sorix.model_selection import train_test_split\nfrom sorix.utils.data import Dataset, DataLoader\nfrom sorix.metrics import confusion_matrix,classification_report\nfrom datetime import datetime\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns  import sorix from sorix import tensor from sorix.nn import Module,Linear, CrossEntropyLoss,ReLU,BatchNorm1d from sorix.optim import SGDMomentum, RMSprop, Adam from sorix.model_selection import train_test_split from sorix.utils.data import Dataset, DataLoader from sorix.metrics import confusion_matrix,classification_report from datetime import datetime In\u00a0[3]: Copied! <pre>device = \"gpu\" if sorix.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"gpu\" if sorix.cuda.is_available() else \"cpu\" device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 13.6.0\n</pre> Out[3]: <pre>'gpu'</pre> In\u00a0[4]: Copied! <pre>data = pd.read_csv(\"../data/digit-recognizer/train.csv\")\n</pre> data = pd.read_csv(\"../data/digit-recognizer/train.csv\") In\u00a0[5]: Copied! <pre>data.head()\n</pre> data.head() Out[5]: label pixel0 pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ... pixel774 pixel775 pixel776 pixel777 pixel778 pixel779 pixel780 pixel781 pixel782 pixel783 0 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 <p>5 rows \u00d7 785 columns</p> In\u00a0[6]: Copied! <pre>data_train,data_test = train_test_split(data,test_size=0.2)\n</pre> data_train,data_test = train_test_split(data,test_size=0.2) In\u00a0[7]: Copied! <pre>X_train = data_train.drop(\"label\",axis=1).values\nY_train = data_train[[\"label\"]].values\n\nX_test = data_test.drop(\"label\",axis=1).values\nY_test = data_test[[\"label\"]].values\n\ntrain_dataset = Dataset(X_train,Y_train)    \ntest_dataset = Dataset(X_test,Y_test)\n\ntrain_dataloader = DataLoader(train_dataset,batch_size=64)\ntest_dataloader = DataLoader(test_dataset,batch_size=64)\n</pre> X_train = data_train.drop(\"label\",axis=1).values Y_train = data_train[[\"label\"]].values  X_test = data_test.drop(\"label\",axis=1).values Y_test = data_test[[\"label\"]].values  train_dataset = Dataset(X_train,Y_train)     test_dataset = Dataset(X_test,Y_test)  train_dataloader = DataLoader(train_dataset,batch_size=64) test_dataloader = DataLoader(test_dataset,batch_size=64) In\u00a0[8]: Copied! <pre>for x,y in train_dataloader:\n    print(x.shape,y.shape)\n    break\n</pre> for x,y in train_dataloader:     print(x.shape,y.shape)     break <pre>(64, 784) (64, 1)\n</pre> In\u00a0[9]: Copied! <pre>class Model(Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = Linear(784,128,bias=False)\n        self.bn1 = BatchNorm1d(128)\n        self.linear2 = Linear(128,64)\n        self.linear3 = Linear(64,10)\n        self.relu = ReLU()\n        \n    def forward(self,x):\n        x = self.linear1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        x = self.relu(x)\n        x = self.linear3(x)\n        return x\n    \n\nmodel = Model()\nmodel.to(device)\nloss_fn = CrossEntropyLoss()\n\noptimizer = RMSprop(model.parameters(), lr=1e-3)\n</pre> class Model(Module):     def __init__(self):         super().__init__()         self.linear1 = Linear(784,128,bias=False)         self.bn1 = BatchNorm1d(128)         self.linear2 = Linear(128,64)         self.linear3 = Linear(64,10)         self.relu = ReLU()              def forward(self,x):         x = self.linear1(x)         x = self.bn1(x)         x = self.relu(x)         x = self.linear2(x)         x = self.relu(x)         x = self.linear3(x)         return x       model = Model() model.to(device) loss_fn = CrossEntropyLoss()  optimizer = RMSprop(model.parameters(), lr=1e-3)  In\u00a0[10]: Copied! <pre>for X,Y in train_dataloader:\n    X_tensor = tensor(X, device=device)\n    Y_tensor = tensor(Y, device=device)\n    print(X_tensor.shape,Y_tensor.shape)\n    break\n</pre> for X,Y in train_dataloader:     X_tensor = tensor(X, device=device)     Y_tensor = tensor(Y, device=device)     print(X_tensor.shape,Y_tensor.shape)     break <pre>(64, 784) (64, 1)\n</pre> In\u00a0[11]: Copied! <pre>start = datetime.now()\n\nepochs = 100\n\nfor epoch in range(epochs+1):\n    model.train()\n    total_train_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    for x, y in train_dataloader:\n        x = tensor(x, device=device)\n        y = tensor(y, device=device)\n\n        # Forward\n        logits = model(x)\n        loss = loss_fn(logits, y)\n\n        # Backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Acumular loss y accuracy\n        total_train_loss += loss.data.get() * len(y)  # ponderar por tama\u00f1o del batch\n        preds = sorix.argmax(logits,axis=1,keepdims=True)\n        correct_train += (preds == y).sum().item()\n        total_train += len(y)\n\n    avg_train_loss = total_train_loss / total_train\n    avg_train_acc = correct_train / total_train\n\n    # --- Validaci\u00f3n/Test ---\n    if epoch % 5 == 0:\n        with sorix.no_grad():\n            model.eval()\n            total_test_loss = 0.0\n            correct_test = 0\n            total_test = 0\n\n            for x, y in test_dataloader:\n                x = tensor(x, device=device)\n                y = tensor(y, device=device)\n\n                logits = model(x)\n                loss = loss_fn(logits,y)\n\n                total_test_loss += loss.data.get() * len(y)\n                preds = sorix.argmax(logits,axis=1,keepdims=True)\n                correct_test += (preds == y).sum().item()\n                total_test += len(y)\n\n            avg_test_loss = total_test_loss / total_test\n            avg_test_acc = correct_test / total_test\n            \n            print(f\"[{device}] [{epoch:3d}/{epochs:3d}] | Loss: {avg_test_loss:.4f} | Acc Train: {100*avg_train_acc:.2f}% | Acc Test: {100*avg_test_acc:.2f}%\")\n\n        if avg_test_acc &gt; 0.96:\n            break\nend = datetime.now()\n\ndelta = end-start\ntiempo = delta.total_seconds()\nprint(f\"Tiempo:{tiempo} segundos = {tiempo/60:.2f} min \")\n</pre> start = datetime.now()  epochs = 100  for epoch in range(epochs+1):     model.train()     total_train_loss = 0.0     correct_train = 0     total_train = 0      for x, y in train_dataloader:         x = tensor(x, device=device)         y = tensor(y, device=device)          # Forward         logits = model(x)         loss = loss_fn(logits, y)          # Backprop         optimizer.zero_grad()         loss.backward()         optimizer.step()          # Acumular loss y accuracy         total_train_loss += loss.data.get() * len(y)  # ponderar por tama\u00f1o del batch         preds = sorix.argmax(logits,axis=1,keepdims=True)         correct_train += (preds == y).sum().item()         total_train += len(y)      avg_train_loss = total_train_loss / total_train     avg_train_acc = correct_train / total_train      # --- Validaci\u00f3n/Test ---     if epoch % 5 == 0:         with sorix.no_grad():             model.eval()             total_test_loss = 0.0             correct_test = 0             total_test = 0              for x, y in test_dataloader:                 x = tensor(x, device=device)                 y = tensor(y, device=device)                  logits = model(x)                 loss = loss_fn(logits,y)                  total_test_loss += loss.data.get() * len(y)                 preds = sorix.argmax(logits,axis=1,keepdims=True)                 correct_test += (preds == y).sum().item()                 total_test += len(y)              avg_test_loss = total_test_loss / total_test             avg_test_acc = correct_test / total_test                          print(f\"[{device}] [{epoch:3d}/{epochs:3d}] | Loss: {avg_test_loss:.4f} | Acc Train: {100*avg_train_acc:.2f}% | Acc Test: {100*avg_test_acc:.2f}%\")          if avg_test_acc &gt; 0.96:             break end = datetime.now()  delta = end-start tiempo = delta.total_seconds() print(f\"Tiempo:{tiempo} segundos = {tiempo/60:.2f} min \") <pre>[gpu] [  0/100] | Loss: 0.2306 | Acc Train: 87.92% | Acc Test: 93.55%\n</pre> <pre>[gpu] [  5/100] | Loss: 0.1223 | Acc Train: 97.28% | Acc Test: 96.46%\nTiempo:10.612358 segundos = 0.18 min \n</pre> In\u00a0[12]: Copied! <pre>all_preds = np.array([])\nall_targets = np.array([])\n\nwith sorix.no_grad():\n    model.eval()\n    for x, y in test_dataloader:\n        x = tensor(x, device=device)\n        y = tensor(y, device=device)\n        # Predicciones\n        logits = model(x)\n        preds = sorix.argmax(logits,axis=1,keepdims=True).cpu()\n        # Guardar predicciones y targets\n        all_preds = np.append(all_preds,preds)\n        all_targets = np.append(all_targets,y.cpu())\n</pre> all_preds = np.array([]) all_targets = np.array([])  with sorix.no_grad():     model.eval()     for x, y in test_dataloader:         x = tensor(x, device=device)         y = tensor(y, device=device)         # Predicciones         logits = model(x)         preds = sorix.argmax(logits,axis=1,keepdims=True).cpu()         # Guardar predicciones y targets         all_preds = np.append(all_preds,preds)         all_targets = np.append(all_targets,y.cpu())  In\u00a0[13]: Copied! <pre>print(classification_report(all_targets,all_preds))\n</pre> print(classification_report(all_targets,all_preds)) <pre>            precision   recall f1-score  support\n0.0              0.96     0.99     0.98      798\n1.0              0.99     0.97     0.98      920\n2.0              0.98     0.96     0.97      797\n3.0              0.96     0.97     0.96      858\n4.0              0.98     0.94     0.96      813\n5.0              0.97     0.95     0.96      772\n6.0              0.97     0.99     0.98      849\n7.0              0.98     0.96     0.97      929\n8.0              0.92     0.96     0.94      810\n9.0              0.94     0.96     0.95      854\n\naccuracy                           0.96     8400\nmacro avg        0.96     0.96     0.96     8400\nweighted avg     0.97     0.96     0.96     8400\n</pre> In\u00a0[14]: Copied! <pre>sns.heatmap(confusion_matrix(all_targets,all_preds), annot=True, cmap=\"Blues\")\n</pre> sns.heatmap(confusion_matrix(all_targets,all_preds), annot=True, cmap=\"Blues\") Out[14]: <pre>&lt;Axes: &gt;</pre>"},{"location":"examples/nn/4-digit-recognizer/#mnist-dataset","title":"MNIST Dataset\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/","title":"Preprocessing","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sorix.preprocessing import OneHotEncoder\nfrom sorix.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\nfrom sorix.preprocessing import ColumnTransformer\n</pre> import numpy as np import pandas as pd from sorix.preprocessing import OneHotEncoder from sorix.preprocessing import StandardScaler,MinMaxScaler,RobustScaler from sorix.preprocessing import ColumnTransformer In\u00a0[3]: Copied! <pre>data = {'Edad': [25, 30, 45, 50, 35, 60, 20, 40],\n        'Ingresos': [30000, 50000, 100000, 120000, 70000, 150000, 20000, 80000],\n        'Pais': ['EE. UU.', 'Canad\u00e1', 'M\u00e9xico', 'EE. UU.', 'Canad\u00e1', 'M\u00e9xico', 'EE. UU.', 'Canad\u00e1'],\n        'Ciudades': ['New York', 'Toronto', 'Mexico City', 'New York', 'Toronto', 'Mexico City', 'New York', 'Toronto'],\n        'Compra': [0, 1, 1, 1, 0, 1, 0, 1]}\ndf = pd.DataFrame(data)\n\nX = df.drop('Compra', axis=1)\ny = df['Compra']\nX\n</pre> data = {'Edad': [25, 30, 45, 50, 35, 60, 20, 40],         'Ingresos': [30000, 50000, 100000, 120000, 70000, 150000, 20000, 80000],         'Pais': ['EE. UU.', 'Canad\u00e1', 'M\u00e9xico', 'EE. UU.', 'Canad\u00e1', 'M\u00e9xico', 'EE. UU.', 'Canad\u00e1'],         'Ciudades': ['New York', 'Toronto', 'Mexico City', 'New York', 'Toronto', 'Mexico City', 'New York', 'Toronto'],         'Compra': [0, 1, 1, 1, 0, 1, 0, 1]} df = pd.DataFrame(data)  X = df.drop('Compra', axis=1) y = df['Compra'] X Out[3]: Edad Ingresos Pais Ciudades 0 25 30000 EE. UU. New York 1 30 50000 Canad\u00e1 Toronto 2 45 100000 M\u00e9xico Mexico City 3 50 120000 EE. UU. New York 4 35 70000 Canad\u00e1 Toronto 5 60 150000 M\u00e9xico Mexico City 6 20 20000 EE. UU. New York 7 40 80000 Canad\u00e1 Toronto In\u00a0[4]: Copied! <pre>X.shape\n</pre> X.shape Out[4]: <pre>(8, 4)</pre> In\u00a0[5]: Copied! <pre>categorical_features = ['Pais', 'Ciudades']\nnumeric_features = ['Edad', 'Ingresos'] \n</pre> categorical_features = ['Pais', 'Ciudades'] numeric_features = ['Edad', 'Ingresos']  In\u00a0[6]: Copied! <pre>encoder = OneHotEncoder()\nX_encoded = encoder.fit_transform(X[categorical_features])\npd.DataFrame(X_encoded, columns=encoder.get_features_names())\n</pre> encoder = OneHotEncoder() X_encoded = encoder.fit_transform(X[categorical_features]) pd.DataFrame(X_encoded, columns=encoder.get_features_names()) Out[6]: Pais_Canad\u00e1 Pais_EE. UU. Pais_M\u00e9xico Ciudades_Mexico City Ciudades_New York Ciudades_Toronto 0 0.0 1.0 0.0 0.0 1.0 0.0 1 1.0 0.0 0.0 0.0 0.0 1.0 2 0.0 0.0 1.0 1.0 0.0 0.0 3 0.0 1.0 0.0 0.0 1.0 0.0 4 1.0 0.0 0.0 0.0 0.0 1.0 5 0.0 0.0 1.0 1.0 0.0 0.0 6 0.0 1.0 0.0 0.0 1.0 0.0 7 1.0 0.0 0.0 0.0 0.0 1.0 In\u00a0[7]: Copied! <pre>scaler = StandardScaler() \nX_scaled = scaler.fit_transform(X[numeric_features]) \npd.DataFrame(X_scaled, columns=scaler.get_features_names())\n</pre> scaler = StandardScaler()  X_scaled = scaler.fit_transform(X[numeric_features])  pd.DataFrame(X_scaled, columns=scaler.get_features_names()) Out[7]: Edad Ingresos 0 -1.051315 -1.137500 1 -0.650814 -0.658553 2 0.550689 0.538816 3 0.951190 1.017763 4 -0.250313 -0.179605 5 1.752192 1.736185 6 -1.451816 -1.376974 7 0.150188 0.059868 In\u00a0[8]: Copied! <pre>encoder = OneHotEncoder()\nscaler = StandardScaler() \nX_encoded = encoder.fit_transform(X[categorical_features])\nX_scaled = scaler.fit_transform(X[numeric_features]) \nX_train = np.hstack((X_encoded, X_scaled))\npd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names())\n</pre> encoder = OneHotEncoder() scaler = StandardScaler()  X_encoded = encoder.fit_transform(X[categorical_features]) X_scaled = scaler.fit_transform(X[numeric_features])  X_train = np.hstack((X_encoded, X_scaled)) pd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names()) Out[8]: Pais_Canad\u00e1 Pais_EE. UU. Pais_M\u00e9xico Ciudades_Mexico City Ciudades_New York Ciudades_Toronto Edad Ingresos 0 0.0 1.0 0.0 0.0 1.0 0.0 -1.051315 -1.137500 1 1.0 0.0 0.0 0.0 0.0 1.0 -0.650814 -0.658553 2 0.0 0.0 1.0 1.0 0.0 0.0 0.550689 0.538816 3 0.0 1.0 0.0 0.0 1.0 0.0 0.951190 1.017763 4 1.0 0.0 0.0 0.0 0.0 1.0 -0.250313 -0.179605 5 0.0 0.0 1.0 1.0 0.0 0.0 1.752192 1.736185 6 0.0 1.0 0.0 0.0 1.0 0.0 -1.451816 -1.376974 7 1.0 0.0 0.0 0.0 0.0 1.0 0.150188 0.059868 In\u00a0[9]: Copied! <pre>encoder = OneHotEncoder()\nscaler = MinMaxScaler()\nX_encoded = encoder.fit_transform(X[categorical_features])\nX_scaled = scaler.fit_transform(X[numeric_features])\nX_train = np.hstack((X_encoded, X_scaled))\npd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names())\n</pre> encoder = OneHotEncoder() scaler = MinMaxScaler() X_encoded = encoder.fit_transform(X[categorical_features]) X_scaled = scaler.fit_transform(X[numeric_features]) X_train = np.hstack((X_encoded, X_scaled)) pd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names()) Out[9]: Pais_Canad\u00e1 Pais_EE. UU. Pais_M\u00e9xico Ciudades_Mexico City Ciudades_New York Ciudades_Toronto Edad Ingresos 0 0.0 1.0 0.0 0.0 1.0 0.0 0.125 0.076923 1 1.0 0.0 0.0 0.0 0.0 1.0 0.250 0.230769 2 0.0 0.0 1.0 1.0 0.0 0.0 0.625 0.615385 3 0.0 1.0 0.0 0.0 1.0 0.0 0.750 0.769231 4 1.0 0.0 0.0 0.0 0.0 1.0 0.375 0.384615 5 0.0 0.0 1.0 1.0 0.0 0.0 1.000 1.000000 6 0.0 1.0 0.0 0.0 1.0 0.0 0.000 0.000000 7 1.0 0.0 0.0 0.0 0.0 1.0 0.500 0.461538 In\u00a0[10]: Copied! <pre>encoder = OneHotEncoder()\nscaler = RobustScaler()\nX_encoded = encoder.fit_transform(X[categorical_features])\nX_scaled = scaler.fit_transform(X[numeric_features])\nX_train = np.hstack((X_encoded, X_scaled))\npd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names())\n</pre> encoder = OneHotEncoder() scaler = RobustScaler() X_encoded = encoder.fit_transform(X[categorical_features]) X_scaled = scaler.fit_transform(X[numeric_features]) X_train = np.hstack((X_encoded, X_scaled)) pd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names()) Out[10]: Pais_Canad\u00e1 Pais_EE. UU. Pais_M\u00e9xico Ciudades_Mexico City Ciudades_New York Ciudades_Toronto Edad Ingresos 0 0.0 1.0 0.0 0.0 1.0 0.0 -0.714286 -0.750000 1 1.0 0.0 0.0 0.0 0.0 1.0 -0.428571 -0.416667 2 0.0 0.0 1.0 1.0 0.0 0.0 0.428571 0.416667 3 0.0 1.0 0.0 0.0 1.0 0.0 0.714286 0.750000 4 1.0 0.0 0.0 0.0 0.0 1.0 -0.142857 -0.083333 5 0.0 0.0 1.0 1.0 0.0 0.0 1.285714 1.250000 6 0.0 1.0 0.0 0.0 1.0 0.0 -1.000000 -0.916667 7 1.0 0.0 0.0 0.0 0.0 1.0 0.142857 0.083333 In\u00a0[11]: Copied! <pre>column_transformer = ColumnTransformer(\n    transformers = [\n        ('cat', OneHotEncoder(), categorical_features),\n        ('num', StandardScaler(), numeric_features)\n    ]\n)\n\nX_train = column_transformer.fit_transform(X)\n\npd.DataFrame(X_train, columns=column_transformer.get_features_names())\n</pre> column_transformer = ColumnTransformer(     transformers = [         ('cat', OneHotEncoder(), categorical_features),         ('num', StandardScaler(), numeric_features)     ] )  X_train = column_transformer.fit_transform(X)  pd.DataFrame(X_train, columns=column_transformer.get_features_names()) Out[11]: cat_Pais_Canad\u00e1 cat_Pais_EE. UU. cat_Pais_M\u00e9xico cat_Ciudades_Mexico City cat_Ciudades_New York cat_Ciudades_Toronto num_Edad num_Ingresos 0 0.0 1.0 0.0 0.0 1.0 0.0 -1.051315 -1.137500 1 1.0 0.0 0.0 0.0 0.0 1.0 -0.650814 -0.658553 2 0.0 0.0 1.0 1.0 0.0 0.0 0.550689 0.538816 3 0.0 1.0 0.0 0.0 1.0 0.0 0.951190 1.017763 4 1.0 0.0 0.0 0.0 0.0 1.0 -0.250313 -0.179605 5 0.0 0.0 1.0 1.0 0.0 0.0 1.752192 1.736185 6 0.0 1.0 0.0 0.0 1.0 0.0 -1.451816 -1.376974 7 1.0 0.0 0.0 0.0 0.0 1.0 0.150188 0.059868"},{"location":"examples/preprocessing/encoders-scalers-transformers/#preprocessing","title":"Preprocessing\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#one-hot-encoder","title":"One Hot Encoder\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#standard-scaler","title":"Standard Scaler\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#one-hot-ecoder-standard-scaler","title":"One Hot Ecoder + Standard Scaler\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#one-hot-ecoder-minmax-scaler","title":"One Hot Ecoder + MinMax Scaler\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#one-hot-ecoder-robust-scaler","title":"One Hot Ecoder + Robust Scaler\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#column-transformer","title":"Column Transformer\u00b6","text":""},{"location":"examples/regression/1-linear-regression/","title":"Linear Regression","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[2]: Copied! <pre>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport sorix\nfrom sorix.nn import Linear, MSELoss\nfrom sorix.optim import RMSprop, Adam\nfrom sorix.metrics import regression_report,r2_score\nfrom sorix.model_selection import train_test_split\n</pre> import os import numpy as np import matplotlib.pyplot as plt import pandas as pd  import sorix from sorix.nn import Linear, MSELoss from sorix.optim import RMSprop, Adam from sorix.metrics import regression_report,r2_score from sorix.model_selection import train_test_split In\u00a0[3]: Copied! <pre>device = 'gpu' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'gpu' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 13.6.0\n</pre> Out[3]: <pre>'gpu'</pre> In\u00a0[4]: Copied! <pre>n = 10000\nx=np.linspace(2,20,n)\ny=2*x + 1 + 2*np.sin(x) + np.random.randn(n)\n\ndata = pd.DataFrame({'x':x, 'y':y})\ndata.head()\n</pre> n = 10000 x=np.linspace(2,20,n) y=2*x + 1 + 2*np.sin(x) + np.random.randn(n)  data = pd.DataFrame({'x':x, 'y':y}) data.head() Out[4]: x y 0 2.000000 7.859412 1 2.001800 6.708418 2 2.003600 7.114425 3 2.005401 6.765781 4 2.007201 8.512988 In\u00a0[5]: Copied! <pre>plt.figure(figsize=(12,8))\nplt.scatter(data['x'],data['y'],s=50)\n</pre> plt.figure(figsize=(12,8)) plt.scatter(data['x'],data['y'],s=50) Out[5]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f979df65160&gt;</pre> In\u00a0[6]: Copied! <pre>data_train, data_test = train_test_split(data, test_size=0.2)\n\nX_train = data_train['x'].values.reshape(-1,1)\ny_train = data_train['y'].values.reshape(-1,1)\n\nX_test = data_test['x'].values.reshape(-1,1)\ny_test = data_test['y'].values.reshape(-1,1)\n</pre> data_train, data_test = train_test_split(data, test_size=0.2)  X_train = data_train['x'].values.reshape(-1,1) y_train = data_train['y'].values.reshape(-1,1)  X_test = data_test['x'].values.reshape(-1,1) y_test = data_test['y'].values.reshape(-1,1) In\u00a0[7]: Copied! <pre>X_train_tensor = sorix.tensor(X_train).to(device)\ny_train_tensor = sorix.tensor(y_train).to(device)\n\nX_test_tensor = sorix.tensor(X_test).to(device)\ny_test_tensor = sorix.tensor(y_test).to(device)\n\nprint(X_train_tensor.shape, y_train_tensor.shape, X_test_tensor.shape, y_test_tensor.shape)\n</pre> X_train_tensor = sorix.tensor(X_train).to(device) y_train_tensor = sorix.tensor(y_train).to(device)  X_test_tensor = sorix.tensor(X_test).to(device) y_test_tensor = sorix.tensor(y_test).to(device)  print(X_train_tensor.shape, y_train_tensor.shape, X_test_tensor.shape, y_test_tensor.shape) <pre>(8000, 1) (8000, 1) (2000, 1) (2000, 1)\n</pre> In\u00a0[8]: Copied! <pre>model = Linear(1,1).to(device)\n\nloss_fn = MSELoss()\noptimizer = Adam(model.parameters(), lr=0.01)\n</pre> model = Linear(1,1).to(device)  loss_fn = MSELoss() optimizer = Adam(model.parameters(), lr=0.01) In\u00a0[9]: Copied! <pre>for itr in range(1000+1):\n    y_pred = model(X_train_tensor)\n    loss = loss_fn(y_pred, y_train_tensor)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if itr % 100 == 0:\n        print(f\"Epoch: {itr:5d} | Loss: {loss.data:.6f}\")\n</pre> for itr in range(1000+1):     y_pred = model(X_train_tensor)     loss = loss_fn(y_pred, y_train_tensor)     optimizer.zero_grad()     loss.backward()     optimizer.step()          if itr % 100 == 0:         print(f\"Epoch: {itr:5d} | Loss: {loss.data:.6f}\") <pre>Epoch:     0 | Loss: 10.105829\n</pre> <pre>Epoch:   100 | Loss: 3.091929\nEpoch:   200 | Loss: 3.013971\n</pre> <pre>Epoch:   300 | Loss: 2.967149\nEpoch:   400 | Loss: 2.946037\n</pre> <pre>Epoch:   500 | Loss: 2.938480\nEpoch:   600 | Loss: 2.936291\n</pre> <pre>Epoch:   700 | Loss: 2.935774\nEpoch:   800 | Loss: 2.935674\n</pre> <pre>Epoch:   900 | Loss: 2.935658\nEpoch:  1000 | Loss: 2.935656\n</pre> In\u00a0[10]: Copied! <pre>with sorix.no_grad():\n    y_pred_train = model(X_train_tensor)\n    y_pred_test = model(X_test_tensor)\n\nprint(\"Train\")\nprint(regression_report(y_train_tensor, y_pred_train))\nprint(\"\\nTest\")\nprint(regression_report(y_test_tensor, y_pred_test))\n</pre> with sorix.no_grad():     y_pred_train = model(X_train_tensor)     y_pred_test = model(X_test_tensor)  print(\"Train\") print(regression_report(y_train_tensor, y_pred_train)) print(\"\\nTest\") print(regression_report(y_test_tensor, y_pred_test))  <pre>Train\n</pre> <pre>Metric |     Score |    Range\n-----------------------------\nR2     |    0.9736 | [0,   1]\nMAE    |    1.4261 | [0,  \u221e) \nMSE    |    2.9357 | [0,  \u221e) \nRMSE   |    1.7134 | [0,  \u221e) \nMAPE   |    8.5210 | [0, 100]\n\nTest\nMetric |     Score |    Range\n-----------------------------\nR2     |    0.9739 | [0,   1]\nMAE    |    1.4223 | [0,  \u221e) \nMSE    |    2.8948 | [0,  \u221e) \nRMSE   |    1.7014 | [0,  \u221e) \nMAPE   |    8.5094 | [0, 100]\n</pre> In\u00a0[11]: Copied! <pre>model.coef_, model.intercept_\n</pre> model.coef_, model.intercept_ Out[11]: <pre>(array([2.00083039]), 0.8921528091707408)</pre> In\u00a0[12]: Copied! <pre>r2_test = r2_score(y_test_tensor, y_pred_test)\n\nplt.scatter(X_test_tensor,y_test_tensor,s=50)\nplt.scatter(X_test_tensor,y_pred_test,s=50)\nplt.title(f'Linear Regression on Test Data(Accuracy:{r2_test*100:.3f}%)')\nplt.text(5, 5, f'y = {model.coef_[0]:.2f}x + {model.intercept_:.3f}')\n</pre> r2_test = r2_score(y_test_tensor, y_pred_test)  plt.scatter(X_test_tensor,y_test_tensor,s=50) plt.scatter(X_test_tensor,y_pred_test,s=50) plt.title(f'Linear Regression on Test Data(Accuracy:{r2_test*100:.3f}%)') plt.text(5, 5, f'y = {model.coef_[0]:.2f}x + {model.intercept_:.3f}') Out[12]: <pre>Text(5, 5, 'y = 2.00x + 0.892')</pre> In\u00a0[13]: Copied! <pre>sorix.save(model.state_dict(),\"regression_model.sor\")\n</pre> sorix.save(model.state_dict(),\"regression_model.sor\") In\u00a0[14]: Copied! <pre>model2 = Linear(1,1)\nmodel2.load_state_dict(sorix.load(\"regression_model.sor\"))\nmodel2.to(device)\n</pre> model2 = Linear(1,1) model2.load_state_dict(sorix.load(\"regression_model.sor\")) model2.to(device) Out[14]: <pre>&lt;sorix.nn.layers.Linear at 0x7f979dba8cd0&gt;</pre> In\u00a0[15]: Copied! <pre>with sorix.no_grad():\n    r2_train = r2_score(y_train_tensor, model2(X_train_tensor))\n    r2_test =  r2_score(y_test_tensor, model2(X_test_tensor))\n    print(f\"R2 Train: {100*r2_train:5.2f} % | R2 Test: {100*r2_test:5.2f} %\")\n</pre> with sorix.no_grad():     r2_train = r2_score(y_train_tensor, model2(X_train_tensor))     r2_test =  r2_score(y_test_tensor, model2(X_test_tensor))     print(f\"R2 Train: {100*r2_train:5.2f} % | R2 Test: {100*r2_test:5.2f} %\") <pre>R2 Train: 97.36 % | R2 Test: 97.39 %\n</pre>"},{"location":"examples/regression/1-linear-regression/#linear-regression","title":"Linear Regression\u00b6","text":""},{"location":"examples/regression/1-linear-regression/#save-model","title":"Save Model\u00b6","text":""},{"location":"examples/regression/2-logistic-regression/","title":"Logistic Regression","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[2]: Copied! <pre>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n</pre> import os import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns In\u00a0[3]: Copied! <pre>import sorix\nfrom sorix.model_selection import train_test_split\nfrom sorix.metrics import classification_report, confusion_matrix\nfrom sorix.nn import Linear, BCEWithLogitsLoss\nfrom sorix.optim import RMSprop, Adam\n</pre> import sorix from sorix.model_selection import train_test_split from sorix.metrics import classification_report, confusion_matrix from sorix.nn import Linear, BCEWithLogitsLoss from sorix.optim import RMSprop, Adam In\u00a0[4]: Copied! <pre>device = 'gpu' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'gpu' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 13.6.0\n</pre> Out[4]: <pre>'gpu'</pre> In\u00a0[5]: Copied! <pre>data=pd.read_csv(\"../data/Iris.csv\")\ndata.head()\n</pre> data=pd.read_csv(\"../data/Iris.csv\") data.head() Out[5]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[6]: Copied! <pre>data['Species'].unique()\ndata = data[data['Species'].isin(['Iris-setosa', 'Iris-versicolor'])]\ndata\n</pre> data['Species'].unique() data = data[data['Species'].isin(['Iris-setosa', 'Iris-versicolor'])] data Out[6]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa ... ... ... ... ... ... ... 95 96 5.7 3.0 4.2 1.2 Iris-versicolor 96 97 5.7 2.9 4.2 1.3 Iris-versicolor 97 98 6.2 2.9 4.3 1.3 Iris-versicolor 98 99 5.1 2.5 3.0 1.1 Iris-versicolor 99 100 5.7 2.8 4.1 1.3 Iris-versicolor <p>100 rows \u00d7 6 columns</p> In\u00a0[7]: Copied! <pre>labels2id = {label: i for i, label in enumerate(data['Species'].unique())}\nid2labels = {i: label for i, label in enumerate(data['Species'].unique())}\n</pre> labels2id = {label: i for i, label in enumerate(data['Species'].unique())} id2labels = {i: label for i, label in enumerate(data['Species'].unique())} In\u00a0[8]: Copied! <pre>data['labels']=data['Species'].map(labels2id)\ndata = data[data['labels'].isin([0,1])]\ndata.head()\n</pre> data['labels']=data['Species'].map(labels2id) data = data[data['labels'].isin([0,1])] data.head() Out[8]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species labels 0 1 5.1 3.5 1.4 0.2 Iris-setosa 0 1 2 4.9 3.0 1.4 0.2 Iris-setosa 0 2 3 4.7 3.2 1.3 0.2 Iris-setosa 0 3 4 4.6 3.1 1.5 0.2 Iris-setosa 0 4 5 5.0 3.6 1.4 0.2 Iris-setosa 0 In\u00a0[9]: Copied! <pre>plt.figure(figsize=(10,6))\nfor specie in data['Species'].unique():\n    specie_data = data[data['Species'] == specie]\n    plt.scatter(specie_data['PetalLengthCm'],specie_data['SepalWidthCm'],s=80 ,label=specie )\n    plt.legend()\n    plt.xlabel(\"Petal Length\")\n    plt.ylabel(\"Sepal Width\")\n</pre> plt.figure(figsize=(10,6)) for specie in data['Species'].unique():     specie_data = data[data['Species'] == specie]     plt.scatter(specie_data['PetalLengthCm'],specie_data['SepalWidthCm'],s=80 ,label=specie )     plt.legend()     plt.xlabel(\"Petal Length\")     plt.ylabel(\"Sepal Width\") In\u00a0[10]: Copied! <pre>independent_features=['PetalLengthCm','SepalWidthCm','PetalWidthCm','SepalLengthCm']\ndependent_feature=['labels']\n</pre> independent_features=['PetalLengthCm','SepalWidthCm','PetalWidthCm','SepalLengthCm'] dependent_feature=['labels'] In\u00a0[11]: Copied! <pre>data_train,data_test=train_test_split(data,test_size=0.2)\n\nX_train=sorix.tensor(data_train[independent_features],device=device)\nY_train=sorix.tensor(data_train[dependent_feature],device=device)\n\nX_test=sorix.tensor(data_test[independent_features],device=device)\nY_test=sorix.tensor(data_test[dependent_feature],device=device)\n\nX_train.shape,Y_train.shape,X_test.shape,Y_test.shape\n</pre>  data_train,data_test=train_test_split(data,test_size=0.2)  X_train=sorix.tensor(data_train[independent_features],device=device) Y_train=sorix.tensor(data_train[dependent_feature],device=device)  X_test=sorix.tensor(data_test[independent_features],device=device) Y_test=sorix.tensor(data_test[dependent_feature],device=device)  X_train.shape,Y_train.shape,X_test.shape,Y_test.shape Out[11]: <pre>((80, 4), (80, 1), (20, 4), (20, 1))</pre> In\u00a0[12]: Copied! <pre>model = Linear(4, 1).to(device)\n\nloss_fn = BCEWithLogitsLoss()\noptimizer = Adam(model.parameters(), lr=0.01)\n</pre> model = Linear(4, 1).to(device)  loss_fn = BCEWithLogitsLoss() optimizer = Adam(model.parameters(), lr=0.01)  In\u00a0[13]: Copied! <pre>for epoch in range(1000+1):\n    y_pred = model(X_train)\n    loss = loss_fn(y_pred, Y_train)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss.item():.4f}\")\n</pre> for epoch in range(1000+1):     y_pred = model(X_train)     loss = loss_fn(y_pred, Y_train)     optimizer.zero_grad()     loss.backward()     optimizer.step()     if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Loss: {loss.item():.4f}\") <pre>Epoch: 0 | Loss: 1.6742\n</pre> <pre>Epoch: 100 | Loss: 0.2143\nEpoch: 200 | Loss: 0.1048\nEpoch: 300 | Loss: 0.0634\n</pre> <pre>Epoch: 400 | Loss: 0.0433\n</pre> <pre>Epoch: 500 | Loss: 0.0319\nEpoch: 600 | Loss: 0.0247\nEpoch: 700 | Loss: 0.0198\n</pre> <pre>Epoch: 800 | Loss: 0.0163\n</pre> <pre>Epoch: 900 | Loss: 0.0137\nEpoch: 1000 | Loss: 0.0117\n</pre> In\u00a0[14]: Copied! <pre>with sorix.no_grad():\n    logits = model(X_test)\nprobs = sorix.sigmoid(logits)\npreds = (probs &gt; 0.5).astype('uint8')\nacc_test = (preds == Y_test).mean()\nplt.scatter(X_test[:,0],X_test[:,1],c=preds)\nplt.title(f\"Logistic Regression on Test Data(Accuracy:{100*acc_test.item():.2f}%)\")\nplt.show()\n</pre> with sorix.no_grad():     logits = model(X_test) probs = sorix.sigmoid(logits) preds = (probs &gt; 0.5).astype('uint8') acc_test = (preds == Y_test).mean() plt.scatter(X_test[:,0],X_test[:,1],c=preds) plt.title(f\"Logistic Regression on Test Data(Accuracy:{100*acc_test.item():.2f}%)\") plt.show() In\u00a0[15]: Copied! <pre>sorix.save(model.state_dict(),\"logistic_model.sor\")\n</pre> sorix.save(model.state_dict(),\"logistic_model.sor\") In\u00a0[16]: Copied! <pre>model2 = Linear(4, 1)\nmodel2.load_state_dict(sorix.load(\"logistic_model.sor\"))\nmodel2.to(device)\n</pre> model2 = Linear(4, 1) model2.load_state_dict(sorix.load(\"logistic_model.sor\")) model2.to(device) Out[16]: <pre>&lt;sorix.nn.layers.Linear at 0x7f36c1746c10&gt;</pre> In\u00a0[17]: Copied! <pre>with sorix.no_grad():\n    logits = model2(X_test)\nprobs = sorix.sigmoid(logits)\npreds = (probs.data &gt; 0.5).astype('uint8')\nacc_test = (preds == Y_test.data).mean()\npreds = preds.get() if device == 'gpu' else preds\n\nplt.scatter(X_test[:,0],X_test[:,1],c=preds)\nplt.title(f\"Logistic Regression on Test Data(Accuracy:{100*acc_test:.2f}%)\")\nplt.show()\n</pre> with sorix.no_grad():     logits = model2(X_test) probs = sorix.sigmoid(logits) preds = (probs.data &gt; 0.5).astype('uint8') acc_test = (preds == Y_test.data).mean() preds = preds.get() if device == 'gpu' else preds  plt.scatter(X_test[:,0],X_test[:,1],c=preds) plt.title(f\"Logistic Regression on Test Data(Accuracy:{100*acc_test:.2f}%)\") plt.show()"},{"location":"examples/regression/2-logistic-regression/#logistic-regression","title":"Logistic Regression\u00b6","text":""},{"location":"examples/regression/2-logistic-regression/#save-model","title":"Save Model\u00b6","text":""},{"location":"learn/01-tensor/","title":"Tensor","text":"In\u00a0[6]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[6]: Copied! <pre>import sorix\nimport numpy as np\nimport pandas as pd\n</pre> import sorix import numpy as np import pandas as pd In\u00a0[7]: Copied! <pre># from list\na = sorix.tensor([1,2,3])\na\n</pre> # from list a = sorix.tensor([1,2,3]) a Out[7]: <pre>Tensor(\n[1 2 3], shape=(3,), device=cpu, requires_grad=False)</pre> In\u00a0[8]: Copied! <pre>#from numpy\na = sorix.tensor(np.random.rand(5,5))\na\n</pre> #from numpy a = sorix.tensor(np.random.rand(5,5)) a Out[8]: <pre>Tensor(\n[[0.26103914 0.82704748 0.31740097 0.18041764 0.89844797]\n [0.60343146 0.33064018 0.55333563 0.01919862 0.17008929]\n [0.21479971 0.48801356 0.06972932 0.43100927 0.9317665 ]\n [0.90065646 0.88185773 0.69416948 0.73985093 0.67909367]\n [0.31243475 0.868807   0.72736226 0.49375667 0.51436013]], shape=(5, 5), device=cpu, requires_grad=False)</pre> In\u00a0[9]: Copied! <pre># from pandas\n\ndata = pd.DataFrame({\n    'a': [0.464307, 0.182403, 0.664873, 0.906638, 0.725385],\n    'b': [0.278199, 0.187902, 0.887387, 0.473387, 0.904510],\n    'c': [0.793136, 0.957675, 0.035765, 0.639977, 0.622032],\n    'd': [0.618634, 0.784397, 0.841349, 0.352944, 0.783273],\n    'e': [0.729128, 0.467162, 0.687347, 0.432614, 0.980809]\n})\n\nt = sorix.tensor(data)\nt\n</pre> # from pandas  data = pd.DataFrame({     'a': [0.464307, 0.182403, 0.664873, 0.906638, 0.725385],     'b': [0.278199, 0.187902, 0.887387, 0.473387, 0.904510],     'c': [0.793136, 0.957675, 0.035765, 0.639977, 0.622032],     'd': [0.618634, 0.784397, 0.841349, 0.352944, 0.783273],     'e': [0.729128, 0.467162, 0.687347, 0.432614, 0.980809] })  t = sorix.tensor(data) t Out[9]: <pre>Tensor(\n[[0.464307 0.278199 0.793136 0.618634 0.729128]\n [0.182403 0.187902 0.957675 0.784397 0.467162]\n [0.664873 0.887387 0.035765 0.841349 0.687347]\n [0.906638 0.473387 0.639977 0.352944 0.432614]\n [0.725385 0.90451  0.622032 0.783273 0.980809]], shape=(5, 5), device=cpu, requires_grad=False)</pre> <p>To access the underlying NumPy array within a Sorix tensor, you can use the <code>data</code> attribute and apply any NumPy operation directly to it.</p> In\u00a0[10]: Copied! <pre>t.data\n</pre> t.data Out[10]: <pre>array([[0.464307, 0.278199, 0.793136, 0.618634, 0.729128],\n       [0.182403, 0.187902, 0.957675, 0.784397, 0.467162],\n       [0.664873, 0.887387, 0.035765, 0.841349, 0.687347],\n       [0.906638, 0.473387, 0.639977, 0.352944, 0.432614],\n       [0.725385, 0.90451 , 0.622032, 0.783273, 0.980809]])</pre> In\u00a0[34]: Copied! <pre>type(t.data)\n</pre> type(t.data) Out[34]: <pre>numpy.ndarray</pre> In\u00a0[11]: Copied! <pre>t = sorix.as_tensor([1,2,3])\nt\n</pre> t = sorix.as_tensor([1,2,3]) t Out[11]: <pre>Tensor(\n[1 2 3], shape=(3,), device=cpu, requires_grad=False)</pre> In\u00a0[12]: Copied! <pre>t = sorix.randn(3,4)\nt\n</pre> t = sorix.randn(3,4) t Out[12]: <pre>Tensor(\n[[ 1.9094233   1.87395467 -0.74233853  1.2301894 ]\n [ 1.00624556 -0.39046544 -1.2465176   1.17108726]\n [-0.58068142 -0.19076851  1.62583008  0.05123753]], shape=(3, 4), device=cpu, requires_grad=False)</pre> In\u00a0[13]: Copied! <pre>t = sorix.zeros((3,4))\nt\n</pre> t = sorix.zeros((3,4)) t Out[13]: <pre>Tensor(\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]], shape=(3, 4), device=cpu, requires_grad=False)</pre> In\u00a0[14]: Copied! <pre>t = sorix.ones((3,4))\nt\n</pre> t = sorix.ones((3,4)) t Out[14]: <pre>Tensor(\n[[1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]], shape=(3, 4), device=cpu, requires_grad=False)</pre> In\u00a0[15]: Copied! <pre>t = sorix.eye(3)\nt\n</pre> t = sorix.eye(3) t Out[15]: <pre>Tensor(\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]], shape=(3, 3), device=cpu, requires_grad=False)</pre> In\u00a0[16]: Copied! <pre>t = sorix.diag([1,2,3])\nt\n</pre> t = sorix.diag([1,2,3]) t Out[16]: <pre>Tensor(\n[[1 0 0]\n [0 2 0]\n [0 0 3]], shape=(3, 3), device=cpu, requires_grad=False)</pre> In\u00a0[17]: Copied! <pre>t = sorix.randint(0,10,(3,4))\nt\n</pre> t = sorix.randint(0,10,(3,4)) t Out[17]: <pre>Tensor(\n[[4 4 1 4]\n [0 6 5 8]\n [6 2 7 3]], shape=(3, 4), device=cpu, requires_grad=False)</pre> In\u00a0[18]: Copied! <pre>t = sorix.arange(0,10)\nt\n</pre> t = sorix.arange(0,10) t Out[18]: <pre>Tensor(\n[0 1 2 3 4 5 6 7 8 9], shape=(10,), device=cpu, requires_grad=False)</pre> In\u00a0[19]: Copied! <pre>t = sorix.linspace(0,10,5)\nt\n</pre> t = sorix.linspace(0,10,5) t Out[19]: <pre>Tensor(\n[ 0.   2.5  5.   7.5 10. ], shape=(5,), device=cpu, requires_grad=False)</pre> In\u00a0[20]: Copied! <pre>t = sorix.logspace(0,10,5)\nt\n</pre> t = sorix.logspace(0,10,5) t Out[20]: <pre>Tensor(\n[1.00000000e+00 3.16227766e+02 1.00000000e+05 3.16227766e+07\n 1.00000000e+10], shape=(5,), device=cpu, requires_grad=False)</pre> In\u00a0[21]: Copied! <pre>t = sorix.randperm(5)\nt\n</pre> t = sorix.randperm(5) t Out[21]: <pre>Tensor(\n[2 4 0 1 3], shape=(5,), device=cpu, requires_grad=False)</pre> In\u00a0[22]: Copied! <pre>a = sorix.tensor([1,2,3])\nb = sorix.tensor([3,4,5])\n\nprint(a)\nprint(b)\n</pre> a = sorix.tensor([1,2,3]) b = sorix.tensor([3,4,5])  print(a) print(b) <pre>Tensor(\n[1 2 3], shape=(3,), device=cpu, requires_grad=False)\nTensor(\n[3 4 5], shape=(3,), device=cpu, requires_grad=False)\n</pre> In\u00a0[23]: Copied! <pre>c = a + b\nc\n</pre> c = a + b c Out[23]: <pre>Tensor(\n[4 6 8], shape=(3,), device=cpu, requires_grad=False)</pre> In\u00a0[24]: Copied! <pre>c = a - b\nc\n</pre> c = a - b c Out[24]: <pre>Tensor(\n[-2 -2 -2], shape=(3,), device=cpu, requires_grad=False)</pre> In\u00a0[25]: Copied! <pre>c = a * b\nc\n</pre> c = a * b c Out[25]: <pre>Tensor(\n[ 3  8 15], shape=(3,), device=cpu, requires_grad=False)</pre> In\u00a0[26]: Copied! <pre>c = a@b\nc\n</pre> c = a@b c Out[26]: <pre>Tensor(\n26, shape=(), device=cpu, requires_grad=False)</pre> In\u00a0[27]: Copied! <pre>c = a**2\nc\n</pre> c = a**2 c Out[27]: <pre>Tensor(\n[1 4 9], shape=(3,), device=cpu, requires_grad=False)</pre> In\u00a0[28]: Copied! <pre>a = sorix.tensor(np.random.rand(5,5))\na\n</pre> a = sorix.tensor(np.random.rand(5,5)) a Out[28]: <pre>Tensor(\n[[0.533261   0.54580051 0.81966961 0.16125006 0.19406496]\n [0.92803021 0.14010897 0.90195723 0.62335444 0.27689092]\n [0.00376482 0.27292441 0.234567   0.46286437 0.37391756]\n [0.22265504 0.93628789 0.11980395 0.47618568 0.81324681]\n [0.73690523 0.3150144  0.53104157 0.53582248 0.53764228]], shape=(5, 5), device=cpu, requires_grad=False)</pre> In\u00a0[29]: Copied! <pre>a[3,:]\n</pre> a[3,:] Out[29]: <pre>Tensor(\n[0.22265504 0.93628789 0.11980395 0.47618568 0.81324681], shape=(5,), device=cpu, requires_grad=False)</pre> In\u00a0[30]: Copied! <pre>a[3,3]\n</pre> a[3,3] Out[30]: <pre>Tensor(\n0.47618568372171655, shape=(), device=cpu, requires_grad=False)</pre> In\u00a0[35]: Copied! <pre>a[:,3]\n</pre> a[:,3] Out[35]: <pre>Tensor(\n[0.00970981 0.45828279 0.16871779 0.93106908 0.71455981], shape=(5,), device=gpu, requires_grad=False)</pre> In\u00a0[32]: Copied! <pre>device = 'gpu' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'gpu' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 13.6.0\n</pre> Out[32]: <pre>'gpu'</pre> In\u00a0[33]: Copied! <pre>a = sorix.tensor(np.random.rand(5,5), device=device)\nb = sorix.tensor(np.random.rand(5,5), device=device)\nc = a + b\nc\n</pre> a = sorix.tensor(np.random.rand(5,5), device=device) b = sorix.tensor(np.random.rand(5,5), device=device) c = a + b c Out[33]: <pre>Tensor(\n[[1.28204074 1.52585648 1.68812989 0.2422148  1.06811271]\n [0.93670912 1.47988872 0.76452445 1.30172208 0.60486268]\n [0.90259124 1.91283999 0.38445446 1.0423669  0.58394127]\n [0.82604729 0.91417964 0.66966319 1.49362473 1.4598561 ]\n [1.39870376 0.70303495 0.84490695 1.36847297 1.04001228]], shape=(5, 5), device=gpu, requires_grad=False)</pre>"},{"location":"learn/01-tensor/#tensor","title":"Tensor\u00b6","text":"<p>The Tensor is Sorix's core data structure, analogous to NumPy arrays but with the added ability to record every operation within a computational graph. This operation tracking is what enables the automatic computation of gradients(Autograd).</p>"},{"location":"learn/01-tensor/#create-a-tensor","title":"Create a Tensor\u00b6","text":"<p>A tensor can be initialized from a NumPy array, a pandas DataFrame, or a Python list. Internally, Sorix converts any supported input into a NumPy array.</p>"},{"location":"learn/01-tensor/#sorix-utils-to-create-tensors","title":"Sorix utils to create tensors\u00b6","text":""},{"location":"learn/01-tensor/#basic-operations","title":"Basic Operations\u00b6","text":""},{"location":"learn/01-tensor/#slicing","title":"Slicing\u00b6","text":""},{"location":"learn/01-tensor/#using-gpu","title":"Using GPU\u00b6","text":"<p>When running on a GPU, Sorix uses CuPy arrays instead of NumPy. You can enable GPU execution by setting the <code>device</code> parameter to <code>'gpu'</code> (the default is <code>'cpu'</code>). When <code>'gpu'</code> is specified, Sorix creates CuPy-based tensors and executes all operations on the GPU.</p> <p>To check whether a GPU is available, you can call <code>sorix.cuda.is_available()</code>. Refer to the examples below.</p>"},{"location":"learn/02-graph/","title":"Computational Graph","text":"In\u00a0[47]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[48]: Copied! <pre>from sorix import tensor\nimport numpy as np\n</pre> from sorix import tensor import numpy as np <p>Defining functions to plot</p> In\u00a0[38]: Copied! <pre>from graphviz import Digraph\n\ndef trace_graph(root):\n    ops = {}\n    edges = []\n\n    def build(t):\n        tid = id(t)\n\n        if t._op and tid not in ops:\n            ops[tid] = t\n\n        for child in t._prev:\n            edges.append((child, t))\n            build(child)\n\n    build(root)\n    return ops, edges\n\n\ndef tensor_label(t):\n    arr = t.to_numpy() if hasattr(t, \"to_numpy\") else t.data\n    if arr.ndim == 0:\n        return str(arr.item())\n    if arr.size &lt;= 6:\n        return str(arr.round(2))\n    return f\"{arr.shape}\"\n\n\ndef draw_graph_ops_forward(root, filename=\"graph_ops\", view=True):\n    dot = Digraph(format=\"png\", graph_attr={\"rankdir\": \"LR\"})\n\n    ops, edges = trace_graph(root)\n\n    for op_id, tensor_obj in ops.items():\n        label = tensor_obj._op\n        dot.node(str(op_id), label, shape=\"circle\",\n                 style=\"filled\", color=\"#F7DC6F\")\n\n    for parent_tensor, child_tensor in edges:\n        label = tensor_label(parent_tensor)\n\n        edge_color = \"green\" if parent_tensor.requires_grad else \"#888888\"\n\n        if parent_tensor._op:\n            parent_node = str(id(parent_tensor))\n        else:\n            parent_node = f\"INPUT_{id(parent_tensor)}\"\n            dot.node(parent_node, \"\", shape=\"point\", width=\"0.1\")\n\n\n        child_node = str(id(child_tensor))\n\n        dot.edge(parent_node, child_node,\n                 label=f'data={label}', color=edge_color)\n\n    final_label = tensor_label(root)\n    final_color = \"green\" if root.requires_grad else \"#888888\"\n\n    final_node = f\"FINAL_{id(root)}\"\n    dot.node(final_node, \"\", shape=\"point\", width=\"0.1\")\n\n    if root._op:\n        dot.edge(str(id(root)), final_node,\n                 label=final_label, color=final_color)\n\n    dot.render(filename, view=view)\n    return dot\n\n\ndef gradient_label(t):\n    \"\"\"Muestra el gradiente del tensor en lugar de su valor\"\"\"\n    if t.grad is None:\n        return \"grad=None\"\n    \n\n    if hasattr(t.grad, \"to_numpy\"):\n        grad_arr = t.grad.to_numpy()\n    elif hasattr(t.grad, \"data\"):\n        grad_arr = np.array(t.grad.data)\n    else:\n        grad_arr = np.array(t.grad)\n    \n    if grad_arr.ndim == 0:\n        return f\"grad={grad_arr.item():.2f}\"\n    if grad_arr.size &lt;= 6:\n\n        grad_list = grad_arr.round(2)\n        return f\"grad={grad_list}\"\n    return f\"grad.shape={grad_arr.shape}\"\n\ndef draw_graph_ops_backward(root, filename=\"graph_ops\", view=True):\n    dot = Digraph(format=\"png\", graph_attr={\"rankdir\": \"RL\"})\n    ops, edges = trace_graph(root)\n    \n\n    for op_id, tensor_obj in ops.items():\n        label = tensor_obj._op\n        dot.node(str(op_id), label, shape=\"circle\",\n                 style=\"filled\", color=\"#F7DC6F\")\n    \n\n    for parent_tensor, child_tensor in edges:\n        label = gradient_label(parent_tensor)\n        \n\n        edge_color = \"green\" if parent_tensor.grad is not None else \"#888888\"\n        \n\n        child_node = str(id(child_tensor))\n        \n\n        if parent_tensor._op:\n            parent_node = str(id(parent_tensor))\n        else:\n            parent_node = f\"INPUT_{id(parent_tensor)}\"\n            dot.node(parent_node, \"\", shape=\"point\", width=\"0.1\")\n        \n\n        dot.edge(child_node, parent_node,\n                 label=label, color=edge_color)\n    \n\n    final_label = gradient_label(root)\n    final_color = \"green\" if root.grad is not None else \"#888888\"\n    final_node = f\"FINAL_{id(root)}\"\n    dot.node(final_node, \"\", shape=\"point\", width=\"0.1\")\n    \n    if root._op:\n\n        dot.edge(final_node, str(id(root)),\n                 label=final_label, color=final_color)\n    \n    dot.render(filename, view=view)\n    return dot\n</pre> from graphviz import Digraph  def trace_graph(root):     ops = {}     edges = []      def build(t):         tid = id(t)          if t._op and tid not in ops:             ops[tid] = t          for child in t._prev:             edges.append((child, t))             build(child)      build(root)     return ops, edges   def tensor_label(t):     arr = t.to_numpy() if hasattr(t, \"to_numpy\") else t.data     if arr.ndim == 0:         return str(arr.item())     if arr.size &lt;= 6:         return str(arr.round(2))     return f\"{arr.shape}\"   def draw_graph_ops_forward(root, filename=\"graph_ops\", view=True):     dot = Digraph(format=\"png\", graph_attr={\"rankdir\": \"LR\"})      ops, edges = trace_graph(root)      for op_id, tensor_obj in ops.items():         label = tensor_obj._op         dot.node(str(op_id), label, shape=\"circle\",                  style=\"filled\", color=\"#F7DC6F\")      for parent_tensor, child_tensor in edges:         label = tensor_label(parent_tensor)          edge_color = \"green\" if parent_tensor.requires_grad else \"#888888\"          if parent_tensor._op:             parent_node = str(id(parent_tensor))         else:             parent_node = f\"INPUT_{id(parent_tensor)}\"             dot.node(parent_node, \"\", shape=\"point\", width=\"0.1\")           child_node = str(id(child_tensor))          dot.edge(parent_node, child_node,                  label=f'data={label}', color=edge_color)      final_label = tensor_label(root)     final_color = \"green\" if root.requires_grad else \"#888888\"      final_node = f\"FINAL_{id(root)}\"     dot.node(final_node, \"\", shape=\"point\", width=\"0.1\")      if root._op:         dot.edge(str(id(root)), final_node,                  label=final_label, color=final_color)      dot.render(filename, view=view)     return dot   def gradient_label(t):     \"\"\"Muestra el gradiente del tensor en lugar de su valor\"\"\"     if t.grad is None:         return \"grad=None\"           if hasattr(t.grad, \"to_numpy\"):         grad_arr = t.grad.to_numpy()     elif hasattr(t.grad, \"data\"):         grad_arr = np.array(t.grad.data)     else:         grad_arr = np.array(t.grad)          if grad_arr.ndim == 0:         return f\"grad={grad_arr.item():.2f}\"     if grad_arr.size &lt;= 6:          grad_list = grad_arr.round(2)         return f\"grad={grad_list}\"     return f\"grad.shape={grad_arr.shape}\"  def draw_graph_ops_backward(root, filename=\"graph_ops\", view=True):     dot = Digraph(format=\"png\", graph_attr={\"rankdir\": \"RL\"})     ops, edges = trace_graph(root)           for op_id, tensor_obj in ops.items():         label = tensor_obj._op         dot.node(str(op_id), label, shape=\"circle\",                  style=\"filled\", color=\"#F7DC6F\")           for parent_tensor, child_tensor in edges:         label = gradient_label(parent_tensor)                   edge_color = \"green\" if parent_tensor.grad is not None else \"#888888\"                   child_node = str(id(child_tensor))                   if parent_tensor._op:             parent_node = str(id(parent_tensor))         else:             parent_node = f\"INPUT_{id(parent_tensor)}\"             dot.node(parent_node, \"\", shape=\"point\", width=\"0.1\")                   dot.edge(child_node, parent_node,                  label=label, color=edge_color)           final_label = gradient_label(root)     final_color = \"green\" if root.grad is not None else \"#888888\"     final_node = f\"FINAL_{id(root)}\"     dot.node(final_node, \"\", shape=\"point\", width=\"0.1\")          if root._op:          dot.edge(final_node, str(id(root)),                  label=final_label, color=final_color)          dot.render(filename, view=view)     return dot In\u00a0[39]: Copied! <pre>x = tensor([3.0], requires_grad=True)\ny = tensor([4.0], requires_grad=True)\n\nf = x**2 + y**2\n\ndraw_graph_ops_forward(f, \"graph_ops\", view=False)\n</pre> x = tensor([3.0], requires_grad=True) y = tensor([4.0], requires_grad=True)  f = x**2 + y**2  draw_graph_ops_forward(f, \"graph_ops\", view=False)   Out[39]: In\u00a0[40]: Copied! <pre>f.backward()\nprint(f\"df/dx: {x.grad}\")\nprint(f\"df/dy: {y.grad}\")\n</pre>  f.backward() print(f\"df/dx: {x.grad}\") print(f\"df/dy: {y.grad}\") <pre>df/dx: [6.]\ndf/dy: [8.]\n</pre> In\u00a0[41]: Copied! <pre>draw_graph_ops_backward(f, \"graph_ops\", view=False)\n</pre> draw_graph_ops_backward(f, \"graph_ops\", view=False)  Out[41]: In\u00a0[42]: Copied! <pre>x = tensor(np.random.rand(2, 2).round(2), requires_grad=True)\ny = tensor(np.random.rand(2, 2).round(2), requires_grad=True)\n\nf = x**2 @ y**2\n\ndraw_graph_ops_forward(f, \"graph_ops\", view=False)\n</pre> x = tensor(np.random.rand(2, 2).round(2), requires_grad=True) y = tensor(np.random.rand(2, 2).round(2), requires_grad=True)  f = x**2 @ y**2  draw_graph_ops_forward(f, \"graph_ops\", view=False) Out[42]: In\u00a0[43]: Copied! <pre>f.backward()\n\nprint(f\"df/dx: {x.grad}\")\nprint(f\"df/dy: {y.grad}\")\n</pre> f.backward()  print(f\"df/dx: {x.grad}\") print(f\"df/dy: {y.grad}\") <pre>df/dx: [[0.448128 2.31895 ]\n [0.168048 1.85516 ]]\ndf/dy: [[0.189216 0.049056]\n [1.509702 2.900996]]\n</pre> <p>Backward</p> In\u00a0[44]: Copied! <pre>draw_graph_ops_backward(f, \"graph_ops\", view=False)\n</pre> draw_graph_ops_backward(f, \"graph_ops\", view=False) Out[44]: <p>In summary, the computational graph is the structural foundation that enables Sorix to perform precise, efficient, and fully automatic differentiation without requiring any manual derivation of gradients.</p>"},{"location":"learn/02-graph/#computational-graph","title":"Computational Graph\u00b6","text":"<p>Sorix constructs a directed acyclic computational graph (DAG) that records every operation applied to tensors created with the parameter <code>requires_grad=True</code>. In this graph, each node represents an elementary operation\u2014such as addition, multiplication, exponentiation, or matrix multiplication\u2014while the edges represent the tensors flowing between those operations.</p> <p>This mechanism allows Sorix to capture the complete functional dependency between the input variables and all intermediate expressions. Every time an operation is executed (e.g., <code>x + y</code>, <code>x * y</code>, <code>x**2</code>), Sorix creates a new node in the graph and connects it to the nodes corresponding to the operands involved in the computation.</p> <p>As a result:</p> <ul> <li>The graph encodes the exact sequence of algebraic transformations performed.</li> <li>It preserves the structural dependencies among all tensors participating in the computation.</li> <li>It grows dynamically only when <code>requires_grad=True</code>, avoiding unnecessary overhead when automatic differentiation is not required.</li> </ul>"},{"location":"learn/02-graph/#example","title":"Example\u00b6","text":""},{"location":"learn/02-graph/#the-computational-graph-of-fxy-x2-y2","title":"The computational graph of $f(x,y) = x^2 + y^2$\u00b6","text":"<p>Where:</p> <ul> <li>x = 3</li> <li>y = 4</li> </ul>"},{"location":"learn/02-graph/#foward","title":"Foward\u00b6","text":"<p>Sorix creates a computational graph</p>"},{"location":"learn/02-graph/#backward","title":"Backward\u00b6","text":"<p>When <code>f_output.backward()</code> is executed, Sorix traverses the previously constructed computational graph and applies the chain rule to compute all required derivatives. The resulting partial derivatives are written into the <code>.grad</code> attribute of the input Tensors $x$ and $y$.</p> <ul> <li>$ \\frac{\\partial f}{\\partial x} = 2x = 6.0 $</li> <li>$ \\frac{\\partial f}{\\partial y} = 2y = 8.0 $</li> </ul>"},{"location":"learn/02-graph/#matrix-example","title":"Matrix example\u00b6","text":"<p>Foward</p>"},{"location":"learn/03-autograd/","title":"Autograd","text":"In\u00a0[4]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-2, 2, 100)\n\nX, Y = np.meshgrid(x, y)\nZ = X**2 + Y**2\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis')\n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title(r'$Z=f(X,Y) = X^2 + Y^2$')\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  x = np.linspace(-2, 2, 100) y = np.linspace(-2, 2, 100)  X, Y = np.meshgrid(x, y) Z = X**2 + Y**2  fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot(111, projection='3d') ax.plot_surface(X, Y, Z, cmap='viridis')  ax.set_xlabel('X') ax.set_ylabel('Y') ax.set_title(r'$Z=f(X,Y) = X^2 + Y^2$') plt.show() In\u00a0[2]: Copied! <pre>from sorix import tensor\n\n# Gradient descent hyperparameters\nlr = 0.1\niters = 50\n\n# Initial variables\nx = tensor([3.0], requires_grad=True)\ny = tensor([4.0], requires_grad=True)\n\n# Initial computation\nf = x**2 + y**2\nf.backward()\n\ndx0 = x.grad.item()\ndy0 = y.grad.item()\n\nprint(\"Iter |     x    |     y    |  df/dx   |  df/dy   |   f(x,y)\")\nprint(\"--------------------------------------------------------------------\")\nprint(f\"{0:4d} | {x.item():8.4f} | {y.item():8.4f} | {dx0:8.4f} | {dy0:8.4f} | {f.item():9.4f}\")\n\n# Cleanup before loop\nx.grad = 0\ny.grad = 0\n\nfor i in range(1, iters + 1):\n\n    # Forward compute f(x,y)\n    f = x**2 + y**2\n\n    # Backward compute df/dx and df/dy\n    f.backward()\n    \n    dx = x.grad\n    dy = y.grad\n\n    # Update x and y values\n    x -= lr * x.grad\n    y -= lr * y.grad\n\n    # Periodic print\n    if i % 5 == 0:\n        print(f\"{i:4d} | {x.item():8.4f} | {y.item():8.4f} | {dx.item():8.4f} | {dy.item():8.4f} | {f.item():9.4f}\")\n\n    # Reset\n    x.grad = 0\n    y.grad = 0\n\n# Final result\nf_final = x**2 + y**2\nprint(\"\\nFinal result:\")\nprint(f\"x = {x.item():.6f}\")\nprint(f\"y = {y.item():.6f}\")\nprint(f\"f(x,y) = {f_final.item():.6f}\")\n</pre> from sorix import tensor  # Gradient descent hyperparameters lr = 0.1 iters = 50  # Initial variables x = tensor([3.0], requires_grad=True) y = tensor([4.0], requires_grad=True)  # Initial computation f = x**2 + y**2 f.backward()  dx0 = x.grad.item() dy0 = y.grad.item()  print(\"Iter |     x    |     y    |  df/dx   |  df/dy   |   f(x,y)\") print(\"--------------------------------------------------------------------\") print(f\"{0:4d} | {x.item():8.4f} | {y.item():8.4f} | {dx0:8.4f} | {dy0:8.4f} | {f.item():9.4f}\")  # Cleanup before loop x.grad = 0 y.grad = 0  for i in range(1, iters + 1):      # Forward compute f(x,y)     f = x**2 + y**2      # Backward compute df/dx and df/dy     f.backward()          dx = x.grad     dy = y.grad      # Update x and y values     x -= lr * x.grad     y -= lr * y.grad      # Periodic print     if i % 5 == 0:         print(f\"{i:4d} | {x.item():8.4f} | {y.item():8.4f} | {dx.item():8.4f} | {dy.item():8.4f} | {f.item():9.4f}\")      # Reset     x.grad = 0     y.grad = 0  # Final result f_final = x**2 + y**2 print(\"\\nFinal result:\") print(f\"x = {x.item():.6f}\") print(f\"y = {y.item():.6f}\") print(f\"f(x,y) = {f_final.item():.6f}\")  <pre>Iter |     x    |     y    |  df/dx   |  df/dy   |   f(x,y)\n--------------------------------------------------------------------\n   0 |   3.0000 |   4.0000 |   6.0000 |   8.0000 |   25.0000\n   5 |   0.9830 |   1.3107 |   2.4576 |   3.2768 |    4.1943\n  10 |   0.3221 |   0.4295 |   0.8053 |   1.0737 |    0.4504\n  15 |   0.1056 |   0.1407 |   0.2639 |   0.3518 |    0.0484\n  20 |   0.0346 |   0.0461 |   0.0865 |   0.1153 |    0.0052\n  25 |   0.0113 |   0.0151 |   0.0283 |   0.0378 |    0.0006\n  30 |   0.0037 |   0.0050 |   0.0093 |   0.0124 |    0.0001\n  35 |   0.0012 |   0.0016 |   0.0030 |   0.0041 |    0.0000\n  40 |   0.0004 |   0.0005 |   0.0010 |   0.0013 |    0.0000\n  45 |   0.0001 |   0.0002 |   0.0003 |   0.0004 |    0.0000\n  50 |   0.0000 |   0.0001 |   0.0001 |   0.0001 |    0.0000\n\nFinal result:\nx = 0.000043\ny = 0.000057\nf(x,y) = 0.000000\n</pre> In\u00a0[3]: Copied! <pre>from sorix import tensor\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Gradient descent hyperparameters\nlr = 0.1\niters = 50\n\n# Initial variables\nx = tensor([3.0], requires_grad=True)\ny = tensor([4.0], requires_grad=True)\nf = x**2 + y**2\n\n# History for plotting\nxs = [x.item()]\nys = [y.item()]\nfs = [f.item()]\n\n# Initial evaluation\nf.backward()\n\n# Initial reset\nx.grad = 0\ny.grad = 0\n\nfor i in range(1, iters + 1):\n\n    # Forward\n    f = x**2 + y**2\n\n    # Backward\n    f.backward()\n\n    # Update\n    x -= lr * x.grad\n    y -= lr * y.grad\n\n    # Save trajectory\n    xs.append(x.item())\n    ys.append(y.item())\n    fs.append(f.item())\n\n    # Reset gradients\n    x.grad = 0\n    y.grad = 0\n\n# -----------------------------\n# 3D plot of the surface + trajectory\n# -----------------------------\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\n\n# Surface f(x,y) = x^2 + y^2\nX = np.linspace(-5, 5, 200)\nY = np.linspace(-5, 5, 200)\nXX, YY = np.meshgrid(X, Y)\nZZ = XX**2 + YY**2\n\nax.plot_surface(XX, YY, ZZ, alpha=0.5, cmap='viridis')\n\n# Gradient descent trajectory\nax.plot(xs, ys, fs, marker='o', color='red', markersize=4, label=\"Trajectory\")\n\n# Final point\nax.scatter(xs[-1], ys[-1], fs[-1], color='black', s=60)\n\nax.set_title(\"Gradient Descent on f(x,y) = x\u00b2 + y\u00b2 (3D Plot)\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"f(x,y)\")\nax.legend()\n\nplt.show()\n</pre> from sorix import tensor import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D  # Gradient descent hyperparameters lr = 0.1 iters = 50  # Initial variables x = tensor([3.0], requires_grad=True) y = tensor([4.0], requires_grad=True) f = x**2 + y**2  # History for plotting xs = [x.item()] ys = [y.item()] fs = [f.item()]  # Initial evaluation f.backward()  # Initial reset x.grad = 0 y.grad = 0  for i in range(1, iters + 1):      # Forward     f = x**2 + y**2      # Backward     f.backward()      # Update     x -= lr * x.grad     y -= lr * y.grad      # Save trajectory     xs.append(x.item())     ys.append(y.item())     fs.append(f.item())      # Reset gradients     x.grad = 0     y.grad = 0  # ----------------------------- # 3D plot of the surface + trajectory # ----------------------------- fig = plt.figure(figsize=(8,6)) ax = fig.add_subplot(111, projection='3d')  # Surface f(x,y) = x^2 + y^2 X = np.linspace(-5, 5, 200) Y = np.linspace(-5, 5, 200) XX, YY = np.meshgrid(X, Y) ZZ = XX**2 + YY**2  ax.plot_surface(XX, YY, ZZ, alpha=0.5, cmap='viridis')  # Gradient descent trajectory ax.plot(xs, ys, fs, marker='o', color='red', markersize=4, label=\"Trajectory\")  # Final point ax.scatter(xs[-1], ys[-1], fs[-1], color='black', s=60)  ax.set_title(\"Gradient Descent on f(x,y) = x\u00b2 + y\u00b2 (3D Plot)\") ax.set_xlabel(\"x\") ax.set_ylabel(\"y\") ax.set_zlabel(\"f(x,y)\") ax.legend()  plt.show()"},{"location":"learn/03-autograd/#autograd","title":"Autograd\u00b6","text":""},{"location":"learn/03-autograd/#the-loss-function","title":"The Loss Function\u00b6","text":"<p>In machine learning, the loss function $ J(\\mathbf{w}) $ quantifies how well a model with parameters $ \\mathbf{w} $ fits the data. Formally, it maps the model\u2019s predictions and the true target values to a single real number that measures the discrepancy between them.</p> <p>A smaller value of $ J(\\mathbf{w}) $ indicates better model performance, while large values signal poor predictions. Because learning consists of adjusting the parameters so that the model improves, training a model is equivalent to solving an optimization problem:</p> <p>$$ \\min_{\\mathbf{w}} J(\\mathbf{w}) $$</p>"},{"location":"learn/03-autograd/#why-the-loss-function-becomes-difficult-to-minimize","title":"Why the loss function becomes difficult to minimize\u00b6","text":"<p>In realistic models, especially neural networks, $ J(\\mathbf{w}) $ is not a simple curve but a high-dimensional surface defined over potentially millions of parameters. Each parameter contributes a dimension, which means:</p> <ul> <li>The surface is extremely complex and non-linear.</li> <li>The gradient $ \\nabla J(\\mathbf{w}) $ contains one partial derivative per parameter.</li> <li>The equations $\\frac{\\partial J}{\\partial \\mathbf{w}} = 0$ form a massive coupled system that cannot be solved analytically.</li> </ul> <p>Patterns such as curvature, ridges, valleys, flat regions, and local minima make the surface far too intricate for direct algebraic minimization.</p> <p>For instance, even writing the stationary condition explicitly: $$ \\nabla J(\\mathbf{w}) = 0 $$</p> <p>produces a system of tens of thousands (or millions) of equations with the same number of unknowns\u2014something mathematically intractable to solve in closed form.</p>"},{"location":"learn/03-autograd/#why-iterative-methods-are-required","title":"Why Iterative Methods Are Required\u00b6","text":"<p>Because analytic solutions are impossible for high-dimensional, non-linear loss surfaces, we use iterative numerical optimization methods. These methods do not attempt to solve the system of equations directly. Instead, they progressively update the parameter vector:</p> <p>$$ \\mathbf{w}_0 \\rightarrow \\mathbf{w}_1 \\rightarrow \\mathbf{w}_2 \\rightarrow \\cdots $$</p> <p>moving step-by-step toward regions where $ J(\\mathbf{w})$  decreases.</p> <p>Among these iterative methods, the most fundamental and widely used is Gradient Descent.</p>"},{"location":"learn/03-autograd/#the-gradient-descent-method","title":"The Gradient Descent Method\u00b6","text":"<p>Gradient Descent is a first-order optimization algorithm used to find the local minimum of a function. It is based on the principle that the direction of maximum increase of a function at a point is the direction of the gradient vector ($\\nabla f$).</p> <p>Therefore, the direction of maximum decrease is the opposite of the gradient ($-\\nabla f$).</p>"},{"location":"learn/03-autograd/#algorithm-steps","title":"Algorithm Steps\u00b6","text":"<ol> <li><p>Initialization: Choose a random initial point $\\mathbf{w}_0 $ on the cost function surface $J(\\mathbf{w}) $.</p> </li> <li><p>Gradient Calculation: At the current point $\\mathbf{w}_t $, compute the gradient $\\nabla J(\\mathbf{w}_t)$.</p> </li> <li><p>Parameter Update: Move the parameters in the direction opposite to the gradient, using the update rule:</p> <p>$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\cdot \\nabla J(\\mathbf{w}_t) $$</p> <p>Where:</p> <ul> <li><p>$ \\mathbf{w}_{t+1} $ is the new set of parameters (weights).</p> </li> <li><p>$ \\mathbf{w}_t $ is the current set of parameters.</p> </li> <li><p>$ \\nabla J(\\mathbf{w}_t) $ is the gradient (vector of partial derivatives) of the cost function at $ \\mathbf{w}_t $.</p> </li> <li><p>$ \\eta $ is the learning rate. This is a crucial hyperparameter controlling the size of the \u201cstep\u201d toward the minimum.</p> </li> <li><p>If $ \\eta $ is too large, the algorithm may overshoot the minimum (divergence).</p> </li> <li><p>If $ \\eta $ is too small, convergence becomes very slow.</p> </li> </ul> </li> <li><p>Convergence: This process is repeated iteratively (epochs) until the update steps become negligible (i.e., the gradient approaches zero) or a maximum number of iterations is reached, indicating that a local minimum has been found.</p> </li> </ol>"},{"location":"learn/03-autograd/#sorixs-automatic-differentiation-engine","title":"Sorix\u2019s Automatic Differentiation Engine\u00b6","text":"<p>Sorix provides a fully automated mechanism for computing derivatives through its autograd engine, removing the need to compute gradients manually\u2014an essential capability for optimization methods such as Gradient Descent.</p> <p>At the core of this system is the Tensor, Sorix\u2019s fundamental data structure. While it behaves similarly to a NumPy array, a Tensor can also build a Computational Graph when initialized with <code>requires_grad=True</code>. From that moment, every arithmetic or functional operation is recorded as nodes and edges in the graph, capturing both the intermediate values and the dependencies between them.</p> <p>When the <code>.backward()</code> method is invoked, Sorix traverses this graph in reverse, applying the chain rule to compute the gradients of the output with respect to all Tensors that participated in the computation. This automatic differentiation process makes any function expressed through Sorix Tensors differentiable, enabling efficient and accurate gradient-based optimization without explicitly defining derivative formulas.</p>"},{"location":"learn/03-autograd/#example-finding-the-minimum-of-fx-y-x2-y2","title":"Example: Finding the Minimum of $f(x, y) = x^2 + y^2$\u00b6","text":""},{"location":"learn/03-autograd/#algebraic-search-for-the-minimum-critical-points","title":"Algebraic Search for the Minimum (Critical Points)\u00b6","text":"<p>A critical point is any point where the gradient of the function is zero:</p> <p>$$ \\nabla f = \\mathbf{0}. $$</p> <p>Computing the partial derivatives:</p> <p>$$ \\frac{\\partial f}{\\partial x} = 2x = 0 \\quad \\Rightarrow \\quad x = 0, $$ $$ \\frac{\\partial f}{\\partial y} = 2y = 0 \\quad \\Rightarrow \\quad y = 0. $$</p> <p>Thus, the only critical point is $(0,0)$. Evaluating the function there confirms it is the global minimum:</p> <p>$$ f(0,0)=0. $$</p> <p>This direct solution is possible because $f(x,y)$ is simple, convex, and differentiable everywhere.</p>"},{"location":"learn/03-autograd/#finding-the-minimum-of-fxyx2y2-with-gradient-descent","title":"Finding the Minimum of $ f(x,y)=x^2+y^2 $ with Gradient Descent\u00b6","text":"<p>The gradient descent update rule is:</p> <p>$$ (x,y)_{t+1} = (x,y)_t - \\eta \\nabla f(x,y), \\qquad t=0,1,2,\\dots,n. $$</p> <p>$$ (x,y)_{t+1} = (x,y)_t - 2\\eta(x,y)_t, \\qquad t=0,1,2,\\dots,n. $$</p> <p>Given:</p> <ul> <li>Initial point: $ (x,y)_0 = (3,4) $</li> <li>Learning rate($lr$): $ \\eta = 0.1 $</li> </ul> <p>the algorithm progressively moves the parameters toward the origin, decreasing the value of the function at each step.</p>"},{"location":"learn/03-autograd/#graphically","title":"Graphically\u00b6","text":""},{"location":"learn/03-autograd/#explanation-of-the-gradient-descent-plot","title":"Explanation of the Gradient Descent Plot\u00b6","text":"<p>The plot depicts the iterative behavior of gradient descent applied to $f(x,y)=x^{2}+y^{2}$. The surface represents the values of $f(x,y)$ over the plane, forming a convex paraboloid in which elevated regions correspond to larger function values and the lowest point at the center corresponds to the global minimum at $(0,0)$.</p> <p>The red trajectory shows the sequence of iterates $(x_t,y_t)$. It begins at $(3.0,4.0)$, located on a high region of the surface where $f(x,y)$ is large. At each step, the algorithm updates the variables in the direction opposite to the gradient, producing a monotonic descent toward smaller values of $f(x,y)$. The initial segments are long because the gradient is large in steep regions, while the steps progressively shorten as the iterates approach the origin, where the surface becomes flatter and the gradient diminishes. The trajectory follows an almost radial path, reflecting the symmetry of the function.</p> <p>The final point lies essentially at $(0,0)$, indicating convergence to the global minimum where $f(x,y)=0$.</p>"},{"location":"learn/04-layers/","title":"Layers","text":"In\u00a0[2]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main' In\u00a0[3]: Copied! <pre>import numpy as np\nfrom sorix import tensor\nfrom sorix.nn import ReLU,Linear,BatchNorm1d\nimport sorix\n</pre> import numpy as np from sorix import tensor from sorix.nn import ReLU,Linear,BatchNorm1d import sorix In\u00a0[32]: Copied! <pre># create random input data\nsamples = 10\nfeatures = 3\nneurons = 2\n\n# X \u2208 \u211d^(samples \u00d7 features)\nX = tensor(np.random.randn(samples, features))\nX\n</pre> # create random input data samples = 10 features = 3 neurons = 2  # X \u2208 \u211d^(samples \u00d7 features) X = tensor(np.random.randn(samples, features)) X Out[32]: <pre>Tensor(\n[[-2.11857762  1.68792676  1.31994672]\n [ 1.5472385   0.1157047  -0.47949683]\n [-0.60946832 -0.26404178  1.0974149 ]\n [-0.02944926  1.27697084  0.55260467]\n [-1.14989104  1.20767261  0.87711009]\n [-0.20930424 -1.50447589  1.70364656]\n [-0.52887809 -2.09572447  1.52516325]\n [ 0.33512794  1.30232238  2.43443063]\n [ 1.12924792 -0.34338007  0.09819372]\n [-3.17633068 -0.30102419  1.39405847]], shape=(10, 3), device=cpu, requires_grad=False)</pre> In\u00a0[33]: Copied! <pre># instantiate a Linear layer: \u211d^(samples \u00d7 features) \u2192 \u211d^(samples \u00d7 neurons)\nlinear = Linear(features, neurons)\n\n# weight matrix W \u2208 \u211d^(features \u00d7 neurons)\nprint(linear.W)\n\n# bias vector b \u2208 \u211d^(1 \u00d7 neurons)\nprint(linear.b)\n</pre> # instantiate a Linear layer: \u211d^(samples \u00d7 features) \u2192 \u211d^(samples \u00d7 neurons) linear = Linear(features, neurons)  # weight matrix W \u2208 \u211d^(features \u00d7 neurons) print(linear.W)  # bias vector b \u2208 \u211d^(1 \u00d7 neurons) print(linear.b) <pre>Tensor(\n[[-0.07543034  0.36379951]\n [-0.64783009 -1.21172978]\n [ 0.53614753  1.63158125]], shape=(3, 2), device=cpu, requires_grad=True)\nTensor(\n[[0. 0.]], shape=(1, 2), device=cpu, requires_grad=True)\n</pre> In\u00a0[34]: Copied! <pre># forward pass:\n# Y \u2208 \u211d^(samples \u00d7 neurons) = X @ W + b\nY = linear(X)\nprint(Y)\n</pre> # forward pass: # Y \u2208 \u211d^(samples \u00d7 neurons) = X @ W + b Y = linear(X) print(Y)  <pre>Tensor(\n[[-0.22599853 -0.66244831]\n [-0.44874675 -0.35965625]\n [ 0.8054029   1.88874458]\n [-0.52876113 -0.6564378 ]\n [-0.22536957 -0.45062629]\n [ 1.90383853  4.52651126]\n [ 2.21527932  4.83547393]\n [ 0.43625153  2.51582794]\n [ 0.18991871  0.98711474]\n [ 1.18202524  1.48373209]], shape=(10, 2), device=cpu, requires_grad=True)\n</pre> In\u00a0[38]: Copied! <pre># number of samples and features\nsamples = 8\nfeatures = 3\n\n# input tensor X \u2208 \u211d^(samples \u00d7 features)\nX = tensor(np.random.randn(samples, features))\nX\n</pre> # number of samples and features samples = 8 features = 3  # input tensor X \u2208 \u211d^(samples \u00d7 features) X = tensor(np.random.randn(samples, features)) X  Out[38]: <pre>Tensor(\n[[ 1.22129142 -0.0213213  -0.29432569]\n [-0.91636576 -0.06141393 -1.13231409]\n [-2.61212584 -0.89636751 -0.9959895 ]\n [-1.6714919  -1.37353929  0.9229026 ]\n [ 0.89873948 -0.36702833 -0.64483109]\n [ 0.52349229 -0.67205028 -0.62714142]\n [-0.14046412  0.33438818 -0.96513357]\n [ 0.80115808  1.47316211  0.29733451]], shape=(8, 3), device=cpu, requires_grad=False)</pre> In\u00a0[39]: Copied! <pre>bn = BatchNorm1d(features)\n\n# \u03b3 \u2208 \u211d^(1 \u00d7 features), \u03b2 \u2208 \u211d^(1 \u00d7 features)\nprint(bn.gamma)\nprint(bn.beta)\n</pre> bn = BatchNorm1d(features)  # \u03b3 \u2208 \u211d^(1 \u00d7 features), \u03b2 \u2208 \u211d^(1 \u00d7 features) print(bn.gamma) print(bn.beta)  <pre>Tensor(\n[[1. 1. 1.]], shape=(1, 3), device=cpu, requires_grad=True)\nTensor(\n[[0. 0. 0.]], shape=(1, 3), device=cpu, requires_grad=True)\n</pre> In\u00a0[40]: Copied! <pre># forward pass (training mode)\nY = bn(X)\nY\n</pre> # forward pass (training mode) Y = bn(X) Y  Out[40]: <pre>Tensor(\n[[ 1.1334296   0.21814242  0.20320982]\n [-0.52805754  0.16864665 -1.05249018]\n [-1.8460816  -0.86213323 -0.84821195]\n [-1.1149769  -1.45121875  2.02718921]\n [ 0.88272715 -0.20864519 -0.32201181]\n [ 0.59106748 -0.58520564 -0.29550437]\n [ 0.0750095   0.65727842 -0.80197528]\n [ 0.80688232  2.06313531  1.08979455]], shape=(8, 3), device=cpu, requires_grad=True)</pre> In\u00a0[41]: Copied! <pre># running statistics after the forward pass\nprint(bn.running_mean)\nprint(bn.running_var)\n</pre> # running statistics after the forward pass print(bn.running_mean) print(bn.running_var)  <pre>[[-0.02369708 -0.01980213 -0.04299373]]\n[[1.06553105 0.96561242 0.94453426]]\n</pre> In\u00a0[42]: Copied! <pre># inference mode\nbn.training = False\nY_eval = bn(X)\nY_eval\n</pre> # inference mode bn.training = False Y_eval = bn(X) Y_eval  Out[42]: <pre>Tensor(\n[[ 1.20609147e+00 -1.54597652e-03 -2.58604792e-01]\n [-8.64779137e-01 -4.23460407e-02 -1.12084217e+00]\n [-2.50755880e+00 -8.92032437e-01 -9.80572746e-01]\n [-1.59631297e+00 -1.37762394e+00  9.93846602e-01]\n [ 8.93616984e-01 -3.53353027e-01 -6.19252818e-01]\n [ 5.30093599e-01 -6.63757116e-01 -6.01051251e-01]\n [-1.13118899e-01  3.60440022e-01 -9.48823932e-01]\n [ 7.99084306e-01  1.51930769e+00  3.50176361e-01]], shape=(8, 3), device=cpu, requires_grad=True)</pre> In\u00a0[44]: Copied! <pre># number of samples and features\nsamples = 8\nfeatures = 3\n\n# input tensor X \u2208 \u211d^(samples \u00d7 features)\nX = tensor(np.random.randn(samples, features))\nX\n</pre> # number of samples and features samples = 8 features = 3  # input tensor X \u2208 \u211d^(samples \u00d7 features) X = tensor(np.random.randn(samples, features)) X Out[44]: <pre>Tensor(\n[[-0.89057293 -0.26609062 -0.07709271]\n [-0.32024358 -1.6020258   0.18186056]\n [ 0.06751868  0.94575422 -0.27170854]\n [-0.93181959  0.22443158 -1.08736002]\n [-1.66797155 -0.89579603  2.077023  ]\n [ 0.68590973 -2.62432205  1.08619278]\n [ 0.38510981 -0.49339594 -0.00567828]\n [-0.34719925  0.55854604 -0.53541034]], shape=(8, 3), device=cpu, requires_grad=False)</pre> In\u00a0[45]: Copied! <pre>relu = ReLU()\nReLU(X)\n</pre> relu = ReLU() ReLU(X) Out[45]: <pre>Tensor(\n[[0.         0.         0.        ]\n [0.         0.         0.18186056]\n [0.06751868 0.94575422 0.        ]\n [0.         0.22443158 0.        ]\n [0.         0.         2.077023  ]\n [0.68590973 0.         1.08619278]\n [0.38510981 0.         0.        ]\n [0.         0.55854604 0.        ]], shape=(8, 3), device=cpu, requires_grad=False)</pre> In\u00a0[56]: Copied! <pre># number of samples and features\nsamples = 8\nfeatures = 3\nneurons = 2\n\n# input tensor X \u2208 \u211d^(samples \u00d7 features)\nX = tensor(np.random.randn(samples, features))\n\n#Linear: X \u2208 \u211d^(samples \u00d7 features) -&gt; X \u2208 \u211d^(samples \u00d7 neurons)\nlinear = Linear(features,neurons)\n\n#BatchNorm1d: X \u2208 \u211d^(samples \u00d7 neurons) -&gt; X \u2208 \u211d^(samples \u00d7 neurons)\nbn = BatchNorm1d(neurons)\n\n#Relu: X \u2208 \u211d^(samples \u00d7 neurons) -&gt; X \u2208 \u211d^(samples \u00d7 neurons)\nrelu = ReLU()\n\nY = ReLU(bn(linear(X)))\nY\n</pre> # number of samples and features samples = 8 features = 3 neurons = 2  # input tensor X \u2208 \u211d^(samples \u00d7 features) X = tensor(np.random.randn(samples, features))  #Linear: X \u2208 \u211d^(samples \u00d7 features) -&gt; X \u2208 \u211d^(samples \u00d7 neurons) linear = Linear(features,neurons)  #BatchNorm1d: X \u2208 \u211d^(samples \u00d7 neurons) -&gt; X \u2208 \u211d^(samples \u00d7 neurons) bn = BatchNorm1d(neurons)  #Relu: X \u2208 \u211d^(samples \u00d7 neurons) -&gt; X \u2208 \u211d^(samples \u00d7 neurons) relu = ReLU()  Y = ReLU(bn(linear(X))) Y Out[56]: <pre>Tensor(\n[[0.7899231  0.        ]\n [0.76963545 0.        ]\n [0.         1.27633776]\n [0.59711683 0.        ]\n [0.69790922 0.6917095 ]\n [0.62584084 0.        ]\n [0.         0.88076543]\n [0.         0.81695402]], shape=(8, 2), device=cpu, requires_grad=True)</pre> In\u00a0[57]: Copied! <pre>device = 'gpu' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'gpu' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 13.6.0\n</pre> Out[57]: <pre>'gpu'</pre> In\u00a0[58]: Copied! <pre>features = 5\nclasses = 2\nexamples = 10\n\nX = tensor(np.random.randn(examples, features), device=device)\nlinear = Linear(features,classes).to(device)\nlinear(X)\n</pre> features = 5 classes = 2 examples = 10  X = tensor(np.random.randn(examples, features), device=device) linear = Linear(features,classes).to(device) linear(X) Out[58]: <pre>Tensor(\n[[-5.16703327 -3.59290969]\n [ 2.79656698  2.81765267]\n [ 1.39700557  1.11127165]\n [-1.22504713 -0.81609523]\n [ 0.15312325 -0.12838952]\n [-0.22987993 -0.93993913]\n [ 1.69610384 -1.71439391]\n [-0.35339305  0.65142666]\n [ 0.67518722 -0.39164053]\n [ 0.22420753 -0.41889924]], shape=(10, 2), device=gpu, requires_grad=True)</pre> In\u00a0[59]: Copied! <pre>bn = BatchNorm1d(classes).to(device)\nbn(linear(X))\n</pre> bn = BatchNorm1d(classes).to(device) bn(linear(X)) Out[59]: <pre>Tensor(\n[[-2.5330769  -2.00942872]\n [ 1.37349087  1.95325517]\n [ 0.68693186  0.89845714]\n [-0.59932389 -0.29294312]\n [ 0.07674169  0.1321616 ]\n [-0.11114166 -0.36949713]\n [ 0.8336554  -0.84822572]\n [-0.17173139  0.61420433]\n [ 0.33284172 -0.0305668 ]\n [ 0.11161229 -0.04741675]], shape=(10, 2), device=gpu, requires_grad=True)</pre> In\u00a0[60]: Copied! <pre>relu = ReLU()\nReLU(X)\n</pre> relu = ReLU() ReLU(X) Out[60]: <pre>Tensor(\n[[0.         0.         0.         1.42350407 0.        ]\n [1.44277295 1.88202868 0.         0.         0.6079959 ]\n [0.         1.41685226 0.         0.         0.50730439]\n [0.55991084 0.         0.         0.52641969 0.        ]\n [0.16115509 0.53765079 0.         0.         0.        ]\n [0.         0.0046861  0.23670877 0.         0.18401204]\n [1.49590944 0.         0.         0.         0.85012796]\n [0.         0.75653219 0.48502291 0.         0.        ]\n [1.22528896 0.         0.50027597 0.         0.87791816]\n [0.         0.13026655 0.         0.         0.        ]], shape=(10, 5), device=gpu, requires_grad=False)</pre> In\u00a0[61]: Copied! <pre>ReLU(bn(linear(X)))\n</pre> ReLU(bn(linear(X))) Out[61]: <pre>Tensor(\n[[0.         0.        ]\n [1.37349087 1.95325517]\n [0.68693186 0.89845714]\n [0.         0.        ]\n [0.07674169 0.1321616 ]\n [0.         0.        ]\n [0.8336554  0.        ]\n [0.         0.61420433]\n [0.33284172 0.        ]\n [0.11161229 0.        ]], shape=(10, 2), device=gpu, requires_grad=True)</pre>"},{"location":"learn/04-layers/#layers","title":"Layers\u00b6","text":"<p>The layers are implemented as Python classes that encapsulate one or more fundamental tensor operations. Each layer defines its trainable parameters as class attributes represented by <code>tensor</code> objects, which are initialized with <code>requires_grad=True</code> by default to enable automatic differentiation.</p> <p>These layers are designed to be composable and device-aware, supporting execution on both CPU and GPU backends. Parameter initialization follows standard schemes (e.g., He or Xavier initialization), and all learnable parameters are exposed through a unified interface for optimization. Non-trainable quantities, such as running statistics in normalization layers, are handled as buffers and are therefore excluded from gradient computation.</p> <p>At a high level, the available layers include linear transformations, nonlinear activation functions, and normalization modules. Each class defines the forward computation via the <code>__call__</code> interface, while gradient propagation is handled internally through the underlying tensor autograd mechanism. Detailed mathematical formulations and implementation specifics of each layer are discussed in the following sections.</p>"},{"location":"learn/04-layers/#import-sorix","title":"Import Sorix\u00b6","text":""},{"location":"learn/04-layers/#linear","title":"Linear\u00b6","text":"<p>The Linear layer implements an affine transformation between finite-dimensional real vector spaces and constitutes a fundamental operator in deep learning architectures. Formally, it defines a linear mapping from an input feature space to an output representation space, optionally augmented by a bias term. This transformation is applied independently to each element of a batch.</p>"},{"location":"learn/04-layers/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>Let  $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ be an input tensor representing a batch of $N$ samples, where each sample is a vector in a $d$-dimensional feature space. The Linear layer defines the affine transformation</p> <p>$$\\mathbf{Y} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}$$</p> <p>where the involved quantities have the following dimensions:</p> <ul> <li>$ \\mathbf{X} \\in \\mathbb{R}^{N \\times d} $ : input batch matrix</li> <li>$ \\mathbf{W} \\in \\mathbb{R}^{d \\times m} $ : weight matrix (trainable parameters)</li> <li>$ \\mathbf{b} \\in \\mathbb{R}^{1 \\times m} $ : bias vector associated with the output neurons</li> <li>$ \\mathbf{Y} \\in \\mathbb{R}^{N \\times m} $ : output tensor</li> <li>$ m $ : number of neurons, i.e., the dimensionality of the output space</li> </ul> <p>From a dimensional analysis standpoint, the matrix product</p> <p>$$ \\mathbf{X}\\mathbf{W} : \\mathbb{R}^{N \\times d} \\times \\mathbb{R}^{d \\times m} \\;\\longrightarrow\\; \\mathbb{R}^{N \\times m} $$</p> <p>is well-defined. The bias term $\\mathbf{b}$ is then added column-wise to the resulting matrix, meaning that each component $b_j$ is added to all entries of the $j$-th output column. Explicitly,</p> <p>$$ Y_{ij} = (\\mathbf{X}\\mathbf{W})_{ij} + b_j, \\quad i = 1,\\dots,N,\\; j = 1,\\dots,m. $$</p>"},{"location":"learn/04-layers/#interpretation-as-a-linear-mapping","title":"Interpretation as a linear mapping\u00b6","text":"<p>At the level of individual samples, for each $ i \\in \\{1, \\dots, N\\}, $ the transformation can be written as</p> <p>$$ \\mathbf{y}_i = \\mathbf{x}_i \\mathbf{W} + \\mathbf{b}, \\quad \\mathbf{x}_i \\in \\mathbb{R}^{1 \\times d},\\; \\mathbf{y}_i \\in \\mathbb{R}^{1 \\times m}. $$</p> <p>Thus, each output vector $\\mathbf{y}_i$ is obtained as a linear combination of the input features, defined by the columns of $\\mathbf{W}$, followed by a translation in the output space determined by the bias vector $\\mathbf{b}$.</p>"},{"location":"learn/04-layers/#functional-view","title":"Functional view\u00b6","text":"<p>The Linear layer realizes the mapping</p> <p>$$ \\text{Linear}:\\; \\mathbb{R}^{N \\times d} \\;\\longrightarrow\\; \\mathbb{R}^{N \\times m}, $$</p> <p>where the same affine transformation is applied independently to each sample in the batch. This operator forms the mathematical foundation upon which more complex nonlinear models are constructed when composed with activation and normalization layers.</p>"},{"location":"learn/04-layers/#parameterization-and-gradients","title":"Parameterization and gradients\u00b6","text":"<p>The parameters $\\mathbf{W}$ and $\\mathbf{b}$ are represented as <code>tensor</code> objects with <code>requires_grad=True</code>, enabling automatic gradient computation via automatic differentiation. During backpropagation, the following gradients are computed:</p> <p>$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}}, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}}, $$</p> <p>where $\\mathcal{L}$ denotes the global loss function of the model.</p>"},{"location":"learn/04-layers/#parameter-initialization","title":"Parameter initialization\u00b6","text":"<p>The weight matrix $\\mathbf{W}$ is initialized from a zero-mean normal distribution with a standard deviation determined by the chosen initialization scheme:</p> <ul> <li><p>He initialization (recommended for ReLU-like activations): $$ \\sigma = \\sqrt{\\frac{2}{d}}. $$</p> </li> <li><p>Xavier initialization (suitable for symmetric activations such as $\\tanh$): $$ \\sigma = \\sqrt{\\frac{2}{d + m}}. $$</p> </li> </ul> <p>Formally, $$ W_{ij} \\sim \\mathcal{N}(0, \\sigma^2). $$</p> <p>When present, the bias vector $\\mathbf{b}$ is initialized to zero.</p>"},{"location":"learn/04-layers/#forward-computation","title":"Forward computation\u00b6","text":"<p>Given an input tensor $\\mathbf{X}$, the forward evaluation of the layer is performed through the matrix operation</p> <p>$$ \\text{Linear}(\\mathbf{X}) = \\mathbf{X}\\mathbf{W} + \\mathbf{b}. $$</p> <p>In the implementation, this computation is exposed via the <code>__call__</code> method, enabling a concise and functional syntax consistent with the rest of the framework.</p>"},{"location":"learn/04-layers/#multi-device-support","title":"Multi-device support\u00b6","text":"<p>The Linear layer is device-aware. Parameters and computations may reside on either CPU or GPU, using NumPy or CuPy as the numerical backend, respectively. The <code>to(device)</code> method ensures consistent parameter transfer across devices while preserving the mathematical semantics of the transformation.</p>"},{"location":"learn/04-layers/#parameter-interface","title":"Parameter interface\u00b6","text":"<p>The trainable parameters of the layer are exposed through the <code>parameters()</code> method, which returns the set</p> <p>$$ \\{\\mathbf{W}, \\mathbf{b}\\}, $$</p> <p>or only $\\mathbf{W}$ when the bias term is disabled. This abstraction allows direct integration with gradient-based optimization algorithms.</p>"},{"location":"learn/04-layers/#statistical-interpretation","title":"Statistical interpretation\u00b6","text":"<p>From a statistical perspective, the Linear layer can be interpreted as a multivariate linear regression model, where each output neuron represents a linear combination of the input features. In this context, the coefficients of the weight matrix $\\mathbf{W}$ and the bias vector $\\mathbf{b}$ define hyperplanes in the output space that approximate the relationship between input and output variables.</p>"},{"location":"learn/04-layers/#batch-normalization-batchnorm1d","title":"Batch Normalization (BatchNorm1d)\u00b6","text":"<p>The BatchNorm1d layer implements batch normalization, a technique designed to stabilize and accelerate the training of deep neural networks by reducing internal covariate shift. This is achieved by normalizing intermediate activations across the batch dimension and subsequently applying a learnable affine transformation.</p>"},{"location":"learn/04-layers/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>Let $ \\mathbf{X} \\in \\mathbb{R}^{N \\times d} $ be an input tensor representing a batch of $N$ samples, where each sample has $d$ features. Batch normalization operates feature-wise, normalizing each feature independently across the batch.</p> <p>During training, the batch-wise mean and variance are computed as</p> <p>$$ \\boldsymbol{\\mu}_B = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{x}_i \\;\\in\\; \\mathbb{R}^{1 \\times d}, $$</p> <p>$$ \\boldsymbol{\\sigma}_B^2 = \\frac{1}{N} \\sum_{i=1}^{N} (\\mathbf{x}_i - \\boldsymbol{\\mu}_B)^2 \\;\\in\\; \\mathbb{R}^{1 \\times d}, $$</p> <p>where $\\mathbf{x}_i \\in \\mathbb{R}^{1 \\times d}$ denotes the $i$-th sample in the batch.</p>"},{"location":"learn/04-layers/#normalization-step","title":"Normalization step\u00b6","text":"<p>Each input sample is normalized using the batch statistics:</p> <p>$$ \\widehat{\\mathbf{X}} = \\frac{\\mathbf{X} - \\boldsymbol{\\mu}_B} {\\sqrt{\\boldsymbol{\\sigma}_B^2 + \\varepsilon}}, \\quad \\widehat{\\mathbf{X}} \\in \\mathbb{R}^{N \\times d}, $$</p> <p>where $\\varepsilon &gt; 0$ is a small constant introduced for numerical stability.</p>"},{"location":"learn/04-layers/#learnable-affine-transformation","title":"Learnable affine transformation\u00b6","text":"<p>To preserve the representational capacity of the network, batch normalization introduces two learnable parameters:</p> <ul> <li>Scale parameter: $ \\boldsymbol{\\gamma} \\in \\mathbb{R}^{1 \\times d} $</li> <li>Shift parameter: $ \\boldsymbol{\\beta} \\in \\mathbb{R}^{1 \\times d} $</li> </ul> <p>The final output of the layer is given by</p> <p>$$ \\mathbf{Y} = \\boldsymbol{\\gamma} \\odot \\widehat{\\mathbf{X}} + \\boldsymbol{\\beta}, \\quad \\mathbf{Y} \\in \\mathbb{R}^{N \\times d}, $$</p> <p>where $\\odot$ denotes element-wise multiplication applied column-wise, i.e., independently to each feature.</p>"},{"location":"learn/04-layers/#running-statistics-and-inference-mode","title":"Running statistics and inference mode\u00b6","text":"<p>In addition to batch statistics, BatchNorm1d maintains running estimates of the mean and variance:</p> <p>$$ \\boldsymbol{\\mu}_{\\text{run}} \\in \\mathbb{R}^{1 \\times d}, \\quad \\boldsymbol{\\sigma}^2_{\\text{run}} \\in \\mathbb{R}^{1 \\times d}. $$</p> <p>These statistics are updated during training using an exponential moving average:</p> <p>$$ \\boldsymbol{\\mu}_{\\text{run}} \\leftarrow \\alpha \\boldsymbol{\\mu}_{\\text{run}} + (1 - \\alpha)\\boldsymbol{\\mu}_B, $$</p> <p>$$ \\boldsymbol{\\sigma}^2_{\\text{run}} \\leftarrow \\alpha \\boldsymbol{\\sigma}^2_{\\text{run}} + (1 - \\alpha)\\boldsymbol{\\sigma}^2_B, $$</p> <p>where $\\alpha \\in (0,1)$ is the momentum parameter controlling the update rate.</p> <p>During inference (evaluation mode), normalization is performed using these accumulated running statistics instead of the batch statistics, ensuring deterministic behavior:</p> <p>$$ \\widehat{\\mathbf{X}} = \\frac{\\mathbf{X} - \\boldsymbol{\\mu}_{\\text{run}}} {\\sqrt{\\boldsymbol{\\sigma}^2_{\\text{run}} + \\varepsilon}}. $$</p>"},{"location":"learn/04-layers/#functional-view","title":"Functional view\u00b6","text":"<p>The BatchNorm1d layer realizes the mapping</p> <p>$$ \\text{BatchNorm1d}:\\; \\mathbb{R}^{N \\times d} \\;\\longrightarrow\\; \\mathbb{R}^{N \\times d}, $$</p> <p>where normalization and affine reparameterization are applied independently to each feature across the batch.</p>"},{"location":"learn/04-layers/#parameterization-and-gradients","title":"Parameterization and gradients\u00b6","text":"<p>The learnable parameters $\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\beta}$ are represented as <code>tensor</code> objects with <code>requires_grad=True</code>. Gradients are computed with respect to these parameters, as well as with respect to the input tensor $\\mathbf{X}$, during backpropagation:</p> <p>$$ \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\gamma}}, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\beta}}, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}}. $$</p> <p>The running statistics are treated as buffers and do not participate in gradient computation.</p>"},{"location":"learn/04-layers/#multi-device-support","title":"Multi-device support\u00b6","text":"<p>BatchNorm1d is device-aware and supports execution on CPU and GPU backends. Learnable parameters are stored as tensors on the selected device, while running statistics are maintained as NumPy or CuPy arrays and transferred consistently when changing devices via the <code>to(device)</code> method.</p>"},{"location":"learn/04-layers/#parameter-interface","title":"Parameter interface\u00b6","text":"<p>The trainable parameters of the layer are exposed through the <code>parameters()</code> method, which returns</p> <p>$$ \\{\\boldsymbol{\\gamma}, \\boldsymbol{\\beta}\\}. $$</p>"},{"location":"learn/04-layers/#statistical-interpretation","title":"Statistical interpretation\u00b6","text":"<p>From a statistical perspective, BatchNorm1d performs a feature-wise standardization of the input distribution, followed by a learned affine transformation. This can be interpreted as dynamically re-centering and re-scaling the feature space, which improves numerical conditioning and facilitates optimization in deep networks.</p>"},{"location":"learn/04-layers/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)\u00b6","text":"<p>The ReLU layer implements the Rectified Linear Unit activation function, one of the most widely used nonlinearities in deep learning due to its simplicity, computational efficiency, and favorable gradient properties. ReLU introduces nonlinearity by applying an element-wise thresholding operation to its input.</p>"},{"location":"learn/04-layers/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>Let $ \\mathbf{X} \\in \\mathbb{R}^{N \\times d} $ be an input tensor representing a batch of $N$ samples with $d$ features. The ReLU activation is defined element-wise as</p> <p>$$ \\operatorname{ReLU}(x) = \\max(0, x). $$</p> <p>Applied to a tensor, this yields</p> <p>$$ \\mathbf{Y} = \\operatorname{ReLU}(\\mathbf{X}), \\quad Y_{ij} = \\max(0, X_{ij}), $$</p> <p>where</p> <ul> <li>$ \\mathbf{Y} \\in \\mathbb{R}^{N \\times d} $ is the output tensor,</li> <li>$ i = 1,\\dots,N $ indexes the samples,</li> <li>$ j = 1,\\dots,d $ indexes the features.</li> </ul> <p>The transformation preserves the dimensionality of the input:</p> <p>$$ \\text{ReLU}:\\; \\mathbb{R}^{N \\times d} \\;\\longrightarrow\\; \\mathbb{R}^{N \\times d}. $$</p>"},{"location":"learn/04-layers/#forward-computation","title":"Forward computation\u00b6","text":"<p>In the forward pass, ReLU performs a simple element-wise comparison with zero, retaining positive values and setting negative values to zero. This operation is computationally inexpensive and trivially parallelizable.</p> <p>Formally,</p> <p>$$ \\mathbf{Y} = \\max(\\mathbf{0}, \\mathbf{X}), $$</p> <p>where the maximum is taken element-wise.</p>"},{"location":"learn/04-layers/#backward-computation-gradient","title":"Backward computation (gradient)\u00b6","text":"<p>The derivative of the ReLU function is given by</p> <p>$$ \\frac{d}{dx} \\operatorname{ReLU}(x) = \\begin{cases} 1, &amp; x &gt; 0, \\\\ 0, &amp; x \\le 0. \\end{cases} $$</p> <p>Accordingly, during backpropagation, the gradient with respect to the input tensor is computed as</p> <p>$$ \\frac{\\partial \\mathcal{L}}{\\partial X_{ij}} = \\frac{\\partial \\mathcal{L}}{\\partial Y_{ij}} \\cdot \\mathbb{I}(X_{ij} &gt; 0), $$</p> <p>where $\\mathbb{I}(\\cdot)$ denotes the indicator function.</p> <p>This matches the implementation, where the upstream gradient is masked by the condition $X &gt; 0$.</p>"},{"location":"learn/04-layers/#autograd-implementation-details","title":"Autograd implementation details\u00b6","text":"<p>In the provided implementation, the output tensor stores a reference to the input tensor and defines a custom backward function:</p> <ul> <li>The forward pass computes $\\mathbf{Y} = \\max(0, \\mathbf{X})$.</li> <li>The backward pass accumulates gradients in the input tensor via $ X.\\text{grad} \\mathrel{+}= \\text{out.grad} \\cdot (X &gt; 0). $</li> </ul> <p>This ensures correct gradient flow through the activation during optimization.</p>"},{"location":"learn/04-layers/#device-awareness","title":"Device awareness\u00b6","text":"<p>The ReLU layer is device-aware and supports execution on both CPU and GPU. The numerical backend (NumPy or CuPy) is selected dynamically based on the device associated with the input tensor, ensuring consistency and performance portability.</p>"},{"location":"learn/04-layers/#functional-role-in-neural-networks","title":"Functional role in neural networks\u00b6","text":"<p>From a functional standpoint, ReLU introduces nonlinearity while preserving sparsity in activations, as negative inputs are mapped exactly to zero. This property mitigates the vanishing gradient problem commonly observed with saturating nonlinearities and enables efficient training of deep architectures.</p>"},{"location":"learn/04-layers/#relu-batchnorm1d-linear","title":"ReLu + BatchNorm1d + Linear\u00b6","text":""},{"location":"learn/04-layers/#using-gpu","title":"Using GPU\u00b6","text":""},{"location":"learn/04-layers/#linear-gpu","title":"Linear + GPU\u00b6","text":""},{"location":"learn/04-layers/#batch-norm-gpu","title":"Batch Norm + GPU\u00b6","text":""},{"location":"learn/04-layers/#relu-gpu","title":"ReLU + GPU\u00b6","text":""},{"location":"learn/04-layers/#linear-relu-batch-norm-gpu","title":"Linear + ReLU + Batch Norm + GPU\u00b6","text":""}]}