{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Sorix","text":"<p>Sorix is a minimalist and high-performance library for Machine Learning and Deep Learning, designed to run neural networks directly on NumPy with minimal resource usage.</p> <p>Inspired by the PyTorch API, Sorix maintains a clear and intuitive interface that allows for rapid adoption without compromising efficiency. Its architecture facilitates a smooth transition from research prototype to production.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p> High Performance</p> <p>Executes optimized neural networks on NumPy with optional GPU acceleration via CuPy.</p> </li> <li> <p> PyTorch-like API</p> <p>Expressive and familiar syntax based on PyTorch design principles, ensuring a short learning curve.</p> </li> <li> <p> Lightweight</p> <p>Ideal for environments with limited computational resources or where low overhead is required.</p> </li> <li> <p> Production Ready</p> <p>Develop models that are ready for real-world deployment without the need to rewrite in other frameworks.</p> </li> </ul>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"#standard-cpu","title":"\ud83d\udcbb Standard (CPU)","text":"<p>For general use on CPU environments.</p> pipPoetryuv <pre><code>pip install sorix\n</code></pre> <pre><code>poetry add sorix\n</code></pre> <pre><code>uv add sorix\n</code></pre>"},{"location":"#gpu-accelerated","title":"\ud83d\ude80 GPU Accelerated","text":"<p>Requires CuPy v13+ and CUDA.</p> pipPoetryuv <pre><code>pip install \"sorix[cp13]\"\n</code></pre> <pre><code>poetry add \"sorix[cp13]\"\n</code></pre> <pre><code>uv add \"sorix[cp13]\"\n</code></pre>"},{"location":"#quick-start","title":"\u26a1 Quick Start","text":""},{"location":"#1-autograd-engine","title":"1. Autograd Engine","text":"<p>Sorix features a simple but powerful autograd engine for automatic differentiation.</p> <pre><code>from sorix import tensor\n\n# Create tensors with gradient tracking\nx = tensor([2.0], requires_grad=True)\nw = tensor([3.0], requires_grad=True)\nb = tensor([1.0], requires_grad=True)\n\n# Define a simple function: y = w*x + b\ny = w * x + b\n\n# Compute gradients via backpropagation\ny.backward()\n\nprint(f\"dy/dx: {x.grad}\") # \u2192 3.0\nprint(f\"dy/dw: {w.grad}\") # \u2192 2.0\n</code></pre>"},{"location":"#2-full-training-pipeline","title":"2. Full Training Pipeline","text":"<p>Building a neural network, training it, and persisting it for later use is as intuitive as in PyTorch.</p> <pre><code>import numpy as np\nfrom sorix import tensor, save, load\nfrom sorix.nn import Sequential, Linear, ReLU, MSELoss\nfrom sorix.optim import SGD\n\n# 1. Prepare data (y = 3x^2 + 2)\nX = np.linspace(-1, 1, 100).reshape(-1, 1)\ny = 3 * X**2 + 2 + 0.1 * np.random.randn(*X.shape)\nX_t, y_t = tensor(X), tensor(y)\n\n# 2. Define a multi-layer model\nmodel = Sequential(\n    Linear(1, 10),\n    ReLU(),\n    Linear(10, 1)\n)\n\n# 3. Define loss and optimizer\ncriterion = MSELoss()\noptimizer = SGD(model.parameters(), lr=0.01)\n\n# 4. Training loop\nfor epoch in range(1000):\n    y_pred = model(X_t)\n    loss = criterion(y_pred, y_t)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 200 == 0:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n\n# 5. Save the model\nsave(model, \"model.sor\")\n\n# 6. Load and verify\nmodel_loaded = load(\"model.sor\")\ntest_val = tensor([[0.5]])\nprint(f\"Prediction for 0.5: {model_loaded(test_val).item():.4f}\")\n</code></pre>"},{"location":"#explore-the-documentation","title":"\ud83d\udcc2 Explore the Documentation","text":"<ul> <li> <p> Learn Basics</p> <p>Understand Tensors, Graphs, Autograd and Modules.</p> <p> Start Learning</p> </li> <li> <p> Examples</p> <p>Real-world models: Linear/Logistic Regression, MNIST, and more.</p> <p> View Examples</p> </li> <li> <p> API Reference</p> <p>Detailed documentation for every class and method.</p> <p> Browse API</p> </li> </ul>"},{"location":"#project-status","title":"\ud83d\udea7 Project Status","text":"<p>Sorix is under active development. We are constantly working on extending key functionalities:</p> <ul> <li>Integration of more essential neural network layers.</li> <li>Optimization of GPU support via CuPy.</li> <li>Extension of the <code>autograd</code> engine.</li> </ul>"},{"location":"#important-links","title":"\ud83d\udd17 Important Links","text":"Resource Link PyPI Package View on PyPI Source Code GitHub Repository"},{"location":"api/","title":"sorix","text":""},{"location":"api/#sorix","title":"sorix","text":"<p>Sorix: A lightweight deep learning library with automatic differentiation.</p> <p>Sorix provides a flexible Tensor class with autograd support, a variety of  neural network layers, optimizers, and metrics, designed to feel familiar  to users of other modern deep learning frameworks while remaining simple  and easy to understand.</p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>sorix</li> <li>_version</li> <li>clustering<ul> <li>k_means</li> </ul> </li> <li>cuda<ul> <li>cuda</li> </ul> </li> <li>cupy<ul> <li>cupy</li> </ul> </li> <li>metrics<ul> <li>metrics</li> </ul> </li> <li>model_selection<ul> <li>train_test</li> </ul> </li> <li>nn<ul> <li>init</li> <li>layers</li> <li>loss</li> <li>net</li> </ul> </li> <li>optim<ul> <li>optim</li> </ul> </li> <li>preprocessing<ul> <li>encoders</li> <li>scalers</li> <li>transformers</li> </ul> </li> <li>tensor</li> <li>utils<ul> <li>data<ul> <li>dataloader</li> <li>dataset</li> </ul> </li> <li>math</li> <li>utils</li> </ul> </li> </ul>"},{"location":"api/_version/","title":"_version","text":""},{"location":"api/_version/#sorix._version","title":"sorix._version","text":""},{"location":"api/tensor/","title":"tensor","text":""},{"location":"api/tensor/#sorix.tensor","title":"sorix.tensor","text":""},{"location":"api/tensor/#sorix.tensor.Device","title":"Device","text":"<pre><code>Device(device)\n</code></pre> <p>Represents a computing device in Sorix, matching PyTorch's torch.device.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def __init__(self, device: Union[str, Device]):\n    if isinstance(device, Device):\n        self.type = device.type\n        self.index = device.index\n    elif isinstance(device, str):\n        if ':' in device:\n            self.type, index_str = device.split(':')\n            self.index = int(index_str)\n        else:\n            self.type = device\n            self.index = 0 if device != 'cpu' else None\n    else:\n        raise ValueError(f\"Invalid device: {device}\")\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Size","title":"Size","text":"<p>               Bases: <code>tuple</code></p> <p>A tuple subclass that represents the shape of a Tensor, matching PyTorch's torch.Size.</p>"},{"location":"api/tensor/#sorix.tensor.DType","title":"DType","text":"<pre><code>DType(name)\n</code></pre> <p>Represents a data type in Sorix, matching PyTorch's torch.dtype.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def __init__(self, name: str):\n    self.name = name\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.no_grad","title":"no_grad","text":"<pre><code>no_grad()\n</code></pre> <p>Context manager that disables autograd engine.</p> <p>Examples:</p> <pre><code>with sorix.no_grad():\n    x = sorix.tensor([1.0], requires_grad=True)\n    y = x + 2\nprint(y.requires_grad)  # False\n</code></pre> Source code in <code>sorix/tensor.py</code> <pre><code>def __init__(self):\n    self.prev = True\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor","title":"Tensor","text":"<pre><code>Tensor(\n    data,\n    _children=[],\n    _op=\"\",\n    device=\"cpu\",\n    requires_grad=False,\n    dtype=None,\n)\n</code></pre> <p>Primitive unit in Sorix. A multi-dimensional array with automatic differentiation.</p> <p>Attributes:</p> <ul> <li> <code>data</code>               (<code>ndarray | ndarray</code>)           \u2013            <p>The actual numerical data.</p> </li> <li> <code>device</code>               (<code>str</code>)           \u2013            <p>'cpu' or 'cuda'.</p> </li> <li> <code>requires_grad</code>               (<code>bool</code>)           \u2013            <p>If True, gradients will be computed for this tensor.</p> </li> <li> <code>grad</code>               (<code>ndarray | ndarray | None</code>)           \u2013            <p>Accumulated gradient for this tensor.</p> </li> </ul> <p>Examples:</p> <pre><code>x = Tensor([1, 2, 3], requires_grad=True)\nprint(x)\n# Tensor(\n# [1 2 3], shape=(3,), device=cpu, requires_grad=True)\n</code></pre> <p>Initializes a new Tensor.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>TensorData</code>)           \u2013            <p>Numerical data (numpy array, list, scalar, etc.).</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Computing device ('cpu' or 'cuda').</p> </li> <li> <code>requires_grad</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to track gradients for this tensor.</p> </li> <li> <code>dtype</code>               (<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>Data type for the tensor elements.</p> </li> </ul> Source code in <code>sorix/tensor.py</code> <pre><code>def __init__(\n    self, \n    data: TensorData, \n    _children: Union[List[Tensor], Tuple[Tensor, ...]] = [], \n    _op: str = '',\n    device: str = 'cpu',\n    requires_grad: bool = False,\n    dtype: Any = None\n) -&gt; None:\n    \"\"\"\n    Initializes a new Tensor.\n\n    Args:\n        data: Numerical data (numpy array, list, scalar, etc.).\n        device: Computing device ('cpu' or 'cuda').\n        requires_grad: Whether to track gradients for this tensor.\n        dtype: Data type for the tensor elements.\n    \"\"\"\n    self.device = Device(device)\n\n    if self.device.type == 'cuda' and not _cupy_available:\n        raise Exception('Cupy is not available')\n\n    xp = cp if (self.device.type == 'cuda' and _cupy_available) else np\n\n    if self.device.type == 'cuda' and _cupy_available:\n        with cp.cuda.Device(self.device.index):\n            if isinstance(data, (list, tuple, int, float)):\n                data = xp.array(data, dtype=dtype.name if isinstance(dtype, DType) else dtype)\n                if dtype is None and data.dtype == xp.float64:\n                    data = data.astype(xp.float32)\n            elif isinstance(data, (np.ndarray, xp.ndarray if _cupy_available else np.ndarray, pd.DataFrame, pd.Series)):\n                data = xp.array(data, dtype=dtype.name if isinstance(dtype, DType) else dtype)\n            else:\n                data = xp.array(data, dtype=dtype.name if isinstance(dtype, DType) else dtype)\n                if dtype is None and data.dtype == xp.float64:\n                    data = data.astype(xp.float32)\n    else:\n        if isinstance(data, (list, tuple, int, float)):\n            data = xp.array(data, dtype=dtype.name if isinstance(dtype, DType) else dtype)\n            if dtype is None and data.dtype == xp.float64:\n                data = data.astype(xp.float32)\n        elif isinstance(data, (np.ndarray, xp.ndarray if _cupy_available else np.ndarray, pd.DataFrame, pd.Series)):\n            data = xp.array(data, dtype=dtype.name if isinstance(dtype, DType) else dtype)\n        else:\n            data = xp.array(data, dtype=dtype.name if isinstance(dtype, DType) else dtype)\n            if dtype is None and data.dtype == xp.float64:\n                data = data.astype(xp.float32)\n\n    self.data: Any = data\n    self.requires_grad: bool = requires_grad\n\n    if requires_grad:\n        d_name = self.dtype.name if isinstance(self.dtype, DType) else str(self.dtype)\n        if self.device.type == 'cuda' and _cupy_available:\n            with cp.cuda.Device(self.device.index):\n                self.grad = xp.zeros_like(self.data, dtype=d_name)\n        else:\n            self.grad = xp.zeros_like(self.data, dtype=d_name)\n    else:\n        self.grad = None\n\n    self._backward = _noop\n    global _autograd_enabled\n    self._prev: Set[Tensor] = set(_children) if (_autograd_enabled and requires_grad) else set()\n    self._op: str = _op if _autograd_enabled else ''\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.T","title":"T  <code>property</code>","text":"<pre><code>T\n</code></pre> <p>Transpose of the tensor.</p>"},{"location":"api/tensor/#sorix.tensor.Tensor.to","title":"to","text":"<pre><code>to(device)\n</code></pre> <p>Moves the tensor to the specified device.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>Union[str, Device]</code>)           \u2013            <p>'cpu', 'cuda', 'cuda:0', etc.</p> </li> </ul> Source code in <code>sorix/tensor.py</code> <pre><code>def to(self, device: Union[str, Device]) -&gt; Tensor:\n    \"\"\"\n    Moves the tensor to the specified device.\n\n    Args:\n        device: 'cpu', 'cuda', 'cuda:0', etc.\n    \"\"\"\n    new_device = Device(device)\n    if new_device == self.device:\n        return self\n\n    if new_device.type == 'cuda':\n        if not _cupy_available:\n            raise RuntimeError(\"CuPy is not installed, you cannot use CUDA\")\n        with cp.cuda.Device(new_device.index):\n            self.data = cp.asarray(self.data)\n            self.grad = cp.array(self.grad) if (self.requires_grad and self.grad is not None) else None\n    elif new_device.type == \"cpu\":\n        self.data = cp.asnumpy(self.data) if self.device == 'cuda' else self.data\n        self.grad = cp.asnumpy(self.grad) if (self.requires_grad and self.grad is not None) else None\n    else:\n        raise ValueError(f\"Invalid device type: {new_device.type}\")\n\n    self.device = new_device\n    return self\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.cpu","title":"cpu","text":"<pre><code>cpu()\n</code></pre> <p>Moves tensor to CPU.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def cpu(self) -&gt; Tensor:\n    \"\"\"Moves tensor to CPU.\"\"\"\n    return self.to(\"cpu\")\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.gpu","title":"gpu","text":"<pre><code>gpu()\n</code></pre> <p>Moves tensor to GPU.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def gpu(self) -&gt; Tensor:\n    \"\"\"Moves tensor to GPU.\"\"\"\n    return self.to('cuda')\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.add_","title":"add_","text":"<pre><code>add_(other)\n</code></pre> <p>In-place addition.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def add_(self, other: Union[Tensor, float, int]) -&gt; Tensor:\n    \"\"\"In-place addition.\"\"\"\n    other_data = other.data if isinstance(other, Tensor) else other\n    self.data += other_data\n    return self\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.sub_","title":"sub_","text":"<pre><code>sub_(other)\n</code></pre> <p>In-place subtraction.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def sub_(self, other: Union[Tensor, float, int]) -&gt; Tensor:\n    \"\"\"In-place subtraction.\"\"\"\n    other_data = other.data if isinstance(other, Tensor) else other\n    self.data -= other_data\n    return self\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.mul_","title":"mul_","text":"<pre><code>mul_(other)\n</code></pre> <p>In-place multiplication.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def mul_(self, other: Union[Tensor, float, int]) -&gt; Tensor:\n    \"\"\"In-place multiplication.\"\"\"\n    other_data = other.data if isinstance(other, Tensor) else other\n    self.data *= other_data\n    return self\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.div_","title":"div_","text":"<pre><code>div_(other)\n</code></pre> <p>In-place division.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def div_(self, other: Union[Tensor, float, int]) -&gt; Tensor:\n    \"\"\"In-place division.\"\"\"\n    other_data = other.data if isinstance(other, Tensor) else other\n    self.data /= other_data\n    return self\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.add","title":"add","text":"<pre><code>add(other)\n</code></pre> <p>Element-wise addition.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[Tensor, float, int]</code>)           \u2013            <p>The tensor or scalar to add.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>A new tensor with the sum.</p> </li> </ul> <p>Examples:</p> <pre><code>x = Tensor([1, 2])\ny = Tensor([3, 4])\nz = x.add(y)  # Tensor([4, 6])\n</code></pre> Source code in <code>sorix/tensor.py</code> <pre><code>def add(self, other: Union[Tensor, float, int]) -&gt; Tensor:\n    \"\"\"\n    Element-wise addition.\n\n    Args:\n        other: The tensor or scalar to add.\n\n    Returns:\n        A new tensor with the sum.\n\n    Examples:\n        ```python\n        x = Tensor([1, 2])\n        y = Tensor([3, 4])\n        z = x.add(y)  # Tensor([4, 6])\n        ```\n    \"\"\"\n    other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)\n    global _autograd_enabled\n\n    if not _autograd_enabled:\n        return Tensor(self.data + other.data, device=self.device)\n\n    requires_grad = self.requires_grad or other.requires_grad   \n    out = Tensor(self.data + other.data, [self, other], '+', device=self.device, requires_grad=requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n        if self.requires_grad:\n            grad_self = Tensor._match_shape(out.grad, self.data.shape)\n            self._accumulate_grad(grad_self)\n        if other.requires_grad:\n            grad_other = Tensor._match_shape(out.grad, other.data.shape)\n            other._accumulate_grad(grad_other)\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.sub","title":"sub","text":"<pre><code>sub(other)\n</code></pre> <p>Element-wise subtraction.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[Tensor, float, int]</code>)           \u2013            <p>The tensor or scalar to subtract.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>A new tensor with the result.</p> </li> </ul> <p>Examples:</p> <pre><code>x = Tensor([5, 5])\ny = Tensor([1, 2])\nz = x.sub(y)  # Tensor([4, 3])\n</code></pre> Source code in <code>sorix/tensor.py</code> <pre><code>def sub(self, other: Union[Tensor, float, int]) -&gt; Tensor:\n    \"\"\"\n    Element-wise subtraction.\n\n    Args:\n        other: The tensor or scalar to subtract.\n\n    Returns:\n        A new tensor with the result.\n\n    Examples:\n        ```python\n        x = Tensor([5, 5])\n        y = Tensor([1, 2])\n        z = x.sub(y)  # Tensor([4, 3])\n        ```\n    \"\"\"\n    other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)\n    global _autograd_enabled\n\n    if not _autograd_enabled:\n        return Tensor(self.data - other.data, device=self.device)\n\n    requires_grad = self.requires_grad or other.requires_grad\n    out = Tensor(self.data - other.data, [self, other], '-', device=self.device, requires_grad=requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n\n        if self.requires_grad:\n            grad_self = Tensor._match_shape(out.grad, self.data.shape)\n            self._accumulate_grad(grad_self)\n\n        if other.requires_grad:\n            grad_other = Tensor._match_shape(out.grad, other.data.shape)\n            other._accumulate_grad(-grad_other)\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.mul","title":"mul","text":"<pre><code>mul(other)\n</code></pre> <p>Element-wise multiplication.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[Tensor, float, int]</code>)           \u2013            <p>The tensor or scalar to multiply by.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>A new tensor with the product.</p> </li> </ul> <p>Examples:</p> <pre><code>x = Tensor([2, 3])\ny = Tensor([4, 5])\nz = x.mul(y)  # Tensor([8, 15])\n</code></pre> Source code in <code>sorix/tensor.py</code> <pre><code>def mul(self, other: Union[Tensor, float, int]) -&gt; Tensor:\n    \"\"\"\n    Element-wise multiplication.\n\n    Args:\n        other: The tensor or scalar to multiply by.\n\n    Returns:\n        A new tensor with the product.\n\n    Examples:\n        ```python\n        x = Tensor([2, 3])\n        y = Tensor([4, 5])\n        z = x.mul(y)  # Tensor([8, 15])\n        ```\n    \"\"\"\n    other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)\n    global _autograd_enabled\n\n    if not _autograd_enabled:\n        return Tensor(self.data * other.data, device=self.device)\n\n    requires_grad = self.requires_grad or other.requires_grad\n    out = Tensor(self.data * other.data, [self, other], '*', device=self.device, requires_grad=requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n\n        if self.requires_grad:\n            grad_self = Tensor._match_shape(other.data * out.grad, self.data.shape)\n            self._accumulate_grad(grad_self)\n\n        if other.requires_grad:\n            grad_other = Tensor._match_shape(self.data * out.grad, other.data.shape)\n            other._accumulate_grad(grad_other)\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.matmul","title":"matmul","text":"<pre><code>matmul(other)\n</code></pre> <p>Matrix multiplication.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[Tensor, ndarray]</code>)           \u2013            <p>The tensor or array to multiply by.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>A new tensor with the matrix product.</p> </li> </ul> <p>Examples:</p> <pre><code>x = Tensor([[1, 2], [3, 4]])\ny = Tensor([[5], [6]])\nz = x.matmul(y) # [[17], [39]]\n</code></pre> Source code in <code>sorix/tensor.py</code> <pre><code>def matmul(self, other: Union[Tensor, np.ndarray]) -&gt; Tensor:\n    \"\"\"\n    Matrix multiplication.\n\n    Args:\n        other: The tensor or array to multiply by.\n\n    Returns:\n        A new tensor with the matrix product.\n\n    Examples:\n        ```python\n        x = Tensor([[1, 2], [3, 4]])\n        y = Tensor([[5], [6]])\n        z = x.matmul(y) # [[17], [39]]\n        ```\n    \"\"\"\n    other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)\n    global _autograd_enabled\n\n    if not _autograd_enabled :\n        return Tensor(self.data @ other.data, device=self.device)\n\n    requires_grad = self.requires_grad or other.requires_grad\n    out = Tensor(self.data @ other.data, [self, other], '@', device=self.device, requires_grad=requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n\n        if self.requires_grad:\n            grad_self = out.grad @ other.data.T\n            self._accumulate_grad(Tensor._match_shape(grad_self, self.data.shape))\n\n        if other.requires_grad:\n            grad_other = self.data.T @ out.grad\n            other._accumulate_grad(Tensor._match_shape(grad_other, other.data.shape))\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.tanh","title":"tanh","text":"<pre><code>tanh()\n</code></pre> <p>Hyperbolic tangent activation.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def tanh(self) -&gt; Tensor:\n    \"\"\"Hyperbolic tangent activation.\"\"\"\n    xp = cp if self.device == 'cuda' else np\n    global _autograd_enabled\n\n    if not _autograd_enabled:\n        return Tensor(xp.tanh(self.data), device=self.device)\n\n    out = Tensor(xp.tanh(self.data), [self], 'tanh', device=self.device, requires_grad=self.requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n        if self.requires_grad:\n            self._accumulate_grad(out.grad * (1 - out.data**2))\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.pow","title":"pow","text":"<pre><code>pow(n)\n</code></pre> <p>Raises tensor to the power of n.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def pow(self, n: Union[int, float]) -&gt; Tensor:\n    \"\"\"Raises tensor to the power of n.\"\"\"\n    assert isinstance(n, (int, float)), \"only supporting int/float powers for now\"\n    global _autograd_enabled\n\n    if not _autograd_enabled:\n        return Tensor(self.data**n, device=self.device)\n\n    out = Tensor(self.data**n, [self], f'**{n}', device=self.device, requires_grad=self.requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n\n        if self.requires_grad:\n            grad = out.grad * (n * (self.data**(n-1)))\n            self._accumulate_grad(grad)\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.sigmoid","title":"sigmoid","text":"<pre><code>sigmoid()\n</code></pre> <p>Sigmoid activation.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def sigmoid(self) -&gt; Tensor:\n    \"\"\"Sigmoid activation.\"\"\"\n    xp = cp if self.device == 'cuda' else np\n    global _autograd_enabled\n\n    out_data = 1 / (1 + xp.exp(-self.data))\n    if not _autograd_enabled:\n        return Tensor(out_data, device=self.device, requires_grad=self.requires_grad)\n\n    out = Tensor(out_data, [self], 'sigmoid', device=self.device, requires_grad=self.requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n        if self.requires_grad:\n            self._accumulate_grad(out.grad * out.data * (1 - out.data))\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.softmax","title":"softmax","text":"<pre><code>softmax(axis=-1)\n</code></pre> <p>Softmax activation along an axis.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def softmax(self, axis: int = -1) -&gt; Tensor:\n    \"\"\"Softmax activation along an axis.\"\"\"\n    xp = cp if self.device == 'cuda' else np\n    global _autograd_enabled\n\n    # Stability trick\n    shifted_data = self.data - xp.max(self.data, axis=axis, keepdims=True)\n    exp_data = xp.exp(shifted_data)\n    out_data = exp_data / xp.sum(exp_data, axis=axis, keepdims=True)\n\n    if not _autograd_enabled:\n        return Tensor(out_data, device=self.device, requires_grad=self.requires_grad)\n\n    out = Tensor(out_data, [self], 'softmax', device=self.device, requires_grad=self.requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n        if self.requires_grad:\n            # Softmax gradient: s * (grad - sum(grad * s, axis, keepdims))\n            sum_grad_s = xp.sum(out.grad * out.data, axis=axis, keepdims=True)\n            self._accumulate_grad(out.data * (out.grad - sum_grad_s))\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.div","title":"div","text":"<pre><code>div(other)\n</code></pre> <p>Element-wise division.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def div(self, other: Union[Tensor, float, int]) -&gt; Tensor:\n    \"\"\"Element-wise division.\"\"\"\n    other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)\n    global _autograd_enabled\n\n    if not _autograd_enabled:\n        return Tensor(self.data / other.data, device=self.device)\n\n    requires_grad = self.requires_grad or other.requires_grad\n    out = Tensor(self.data / other.data, [self, other], '/', device=self.device, requires_grad=requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n\n        if self.requires_grad:\n            grad_self = Tensor._match_shape(out.grad / other.data, self.data.shape)\n            self._accumulate_grad(grad_self)\n\n        if other.requires_grad:\n            grad_other = Tensor._match_shape(-self.data * out.grad / (other.data**2), other.data.shape)\n            other._accumulate_grad(grad_other)\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.mean","title":"mean","text":"<pre><code>mean(axis=None, keepdims=False)\n</code></pre> <p>Computes mean along axis.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def mean(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -&gt; Tensor:\n    \"\"\"Computes mean along axis.\"\"\"\n    global _autograd_enabled\n    xp = cp if self.device == 'cuda' else np\n\n    if not _autograd_enabled:\n        return Tensor(xp.mean(self.data, axis=axis, keepdims=keepdims), device=self.device)\n\n    out = Tensor(xp.mean(self.data, axis=axis, keepdims=keepdims), [self], 'mean', device=self.device, requires_grad=self.requires_grad)\n\n    def _backward() -&gt; None:            \n        if out.grad is None:\n            return\n\n        if self.requires_grad:\n            grad = out.grad\n            if not keepdims and axis is not None:\n                grad = xp.expand_dims(grad, axis=axis)\n            self._accumulate_grad(grad * xp.ones_like(self.data) / self.data.size)\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.sum","title":"sum","text":"<pre><code>sum(axis=None, keepdims=False)\n</code></pre> <p>Computes sum along axis.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def sum(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -&gt; Tensor:\n    \"\"\"Computes sum along axis.\"\"\"\n    global _autograd_enabled\n    xp = cp if self.device == 'cuda' else np\n\n    if not _autograd_enabled:\n        return Tensor(self.data.sum(axis=axis, keepdims=keepdims), device=self.device)\n\n    out = Tensor(self.data.sum(axis=axis, keepdims=keepdims), [self], 'sum', device=self.device, requires_grad=self.requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n\n        if self.requires_grad:\n            grad = out.grad\n            if not keepdims and axis is not None:\n                grad = xp.expand_dims(grad, axis=axis)\n            self._accumulate_grad(xp.ones_like(self.data) * grad)\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.abs","title":"abs","text":"<pre><code>abs()\n</code></pre> <p>Absolute value.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def abs(self) -&gt; Tensor:\n    \"\"\"Absolute value.\"\"\"\n    xp = cp if self.device == 'cuda' else np\n    return Tensor(xp.abs(self.data), device=self.device)\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.reshape","title":"reshape","text":"<pre><code>reshape(*shape)\n</code></pre> <p>Reshapes the tensor to a new shape.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def reshape(self, *shape: Any) -&gt; Tensor:\n    \"\"\"Reshapes the tensor to a new shape.\"\"\"\n    if len(shape) == 1 and isinstance(shape[0], (list, tuple)):\n        shape = shape[0]\n\n    global _autograd_enabled\n    if not _autograd_enabled:\n        return Tensor(self.data.reshape(*shape), device=self.device, requires_grad=self.requires_grad)\n\n    out = Tensor(self.data.reshape(*shape), [self], 'reshape', device=self.device, requires_grad=self.requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n        if self.requires_grad:\n            self._accumulate_grad(out.grad.reshape(self.data.shape))\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.transpose","title":"transpose","text":"<pre><code>transpose(*axes)\n</code></pre> <p>Transposes the tensor axes.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def transpose(self, *axes: Any) -&gt; Tensor:\n    \"\"\"Transposes the tensor axes.\"\"\"\n    global _autograd_enabled\n    if not _autograd_enabled:\n        return Tensor(self.data.transpose(*axes), device=self.device, requires_grad=self.requires_grad)\n\n    out = Tensor(self.data.transpose(*axes), [self], 'transpose', device=self.device, requires_grad=self.requires_grad)\n\n    def _backward() -&gt; None:\n        if out.grad is None:\n            return\n        if self.requires_grad:\n            if not axes:\n                self._accumulate_grad(out.grad.transpose())\n            else:\n                inv_axes = np.argsort(axes)\n                self._accumulate_grad(out.grad.transpose(*inv_axes))\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.flatten","title":"flatten","text":"<pre><code>flatten()\n</code></pre> <p>Flattens the tensor into 1D.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def flatten(self) -&gt; Tensor:\n    \"\"\"Flattens the tensor into 1D.\"\"\"\n    return self.reshape(-1)\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.backward","title":"backward","text":"<pre><code>backward()\n</code></pre> <p>Computes the gradient of current tensor w.r.t. graph leaves.</p> <p>The graph is traversed in reverse topological order to propagate gradients.</p> <p>Examples:</p> <pre><code>x = Tensor([2.0], requires_grad=True)\ny = x * x\ny.backward()\nprint(x.grad)  # [4.]\n</code></pre> Source code in <code>sorix/tensor.py</code> <pre><code>def backward(self) -&gt; None:\n    \"\"\"\n    Computes the gradient of current tensor w.r.t. graph leaves.\n\n    The graph is traversed in reverse topological order to propagate gradients.\n\n    Examples:\n        ```python\n        x = Tensor([2.0], requires_grad=True)\n        y = x * x\n        y.backward()\n        print(x.grad)  # [4.]\n        ```\n    \"\"\"\n    topo: List[Tensor] = []\n    visited: Set[int] = set()\n\n    def build_topo(t: Tensor) -&gt; None:\n        if id(t) not in visited:\n            visited.add(id(t))\n            for child in t._prev:\n                build_topo(child)\n            topo.append(t)\n\n    build_topo(self)\n\n    xp = cp if self.device.type == 'cuda' else np\n    d_name = self.dtype.name if isinstance(self.dtype, DType) else str(self.dtype)\n    if self.grad is None:\n         self.grad = xp.ones_like(self.data, dtype=d_name)\n    else:\n         self.grad += xp.ones_like(self.data, dtype=d_name)\n\n    for node in reversed(topo):\n        node._backward()\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.astype","title":"astype","text":"<pre><code>astype(dtype)\n</code></pre> <p>Casts tensor to a new data type.</p> Source code in <code>sorix/tensor.py</code> <pre><code>def astype(self, dtype: Any) -&gt; Tensor:\n    \"\"\"Casts tensor to a new data type.\"\"\"\n    return Tensor(self.data.astype(dtype), device=self.device)\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.numpy","title":"numpy","text":"<pre><code>numpy()\n</code></pre> <p>Returns the data as a NumPy array.</p> <p>If the tensor is on the GPU, it will be copied to the host.</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The numerical data as a NumPy ndarray.</p> </li> </ul> Source code in <code>sorix/tensor.py</code> <pre><code>def numpy(self) -&gt; np.ndarray:\n    \"\"\"\n    Returns the data as a NumPy array.\n\n    If the tensor is on the GPU, it will be copied to the host.\n\n    Returns:\n        The numerical data as a NumPy ndarray.\n    \"\"\"\n    return self.data if self.device == 'cpu' else self.data.get()   \n</code></pre>"},{"location":"api/tensor/#sorix.tensor.Tensor.item","title":"item","text":"<pre><code>item()\n</code></pre> <p>Returns the scalar value of a 1-element tensor.</p> <p>Examples:</p> <pre><code>x = Tensor([42])\nval = x.item()  # 42\n</code></pre> Source code in <code>sorix/tensor.py</code> <pre><code>def item(self) -&gt; Union[float, int]:\n    \"\"\"\n    Returns the scalar value of a 1-element tensor.\n\n    Examples:\n        ```python\n        x = Tensor([42])\n        val = x.item()  # 42\n        ```\n    \"\"\"\n    return self.data.item()\n</code></pre>"},{"location":"api/tensor/#sorix.tensor.tensor","title":"tensor","text":"<pre><code>tensor(data, device='cpu', requires_grad=False, dtype=None)\n</code></pre> <p>Factory function to create a Sorix Tensor.</p> <p>Examples:</p> <pre><code>x = sorix.tensor([1.0, 2.0], requires_grad=True, dtype=sorix.float32)\n</code></pre> Source code in <code>sorix/tensor.py</code> <pre><code>def tensor(\n    data: TensorData, \n    device: str = 'cpu', \n    requires_grad: bool = False,\n    dtype: Any = None\n) -&gt; Tensor:\n    \"\"\"\n    Factory function to create a Sorix Tensor.\n\n    Examples:\n        ```python\n        x = sorix.tensor([1.0, 2.0], requires_grad=True, dtype=sorix.float32)\n        ```\n    \"\"\"\n    return Tensor(data, device=device, requires_grad=requires_grad, dtype=dtype)\n</code></pre>"},{"location":"api/clustering/","title":"Index","text":""},{"location":"api/clustering/#sorix.clustering","title":"sorix.clustering","text":""},{"location":"api/clustering/k_means/","title":"k_means","text":""},{"location":"api/clustering/k_means/#sorix.clustering.k_means","title":"sorix.clustering.k_means","text":""},{"location":"api/clustering/k_means/#sorix.clustering.k_means.Kmeans","title":"Kmeans","text":"<pre><code>Kmeans(n_clusters)\n</code></pre> <p>K-means clustering</p> <p>Parameters: n_clusters (int): number of clusters</p> Source code in <code>sorix/clustering/k_means.py</code> <pre><code>def __init__(self, n_clusters:int):\n    \"\"\"\n    Parameters:\n    n_clusters (int): number of clusters\n    \"\"\"\n    self.n_clusters = n_clusters\n    self._centroids = None\n    self.features_names = None\n    self.labels = None\n</code></pre>"},{"location":"api/clustering/k_means/#sorix.clustering.k_means.Kmeans.fit","title":"fit","text":"<pre><code>fit(features, eps=0.001, max_iters=1000)\n</code></pre> <p>Fit the model.</p> <p>Parameters:</p> <ul> <li> <code>features</code>               (<code>tensor</code>)           \u2013            <p>Features to predict.</p> </li> <li> <code>eps</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Stop criterion, by default 0.001</p> </li> <li> <code>max_iters</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Maximum number of iterations, by default 1000</p> </li> </ul> Source code in <code>sorix/clustering/k_means.py</code> <pre><code>def fit(self, \n          features: Tensor, \n          eps:float=0.001,\n          max_iters:int=1000) -&gt; None:\n\n    \"\"\"\n    Fit the model.\n\n    Parameters:\n        features (tensor): Features to predict.\n        eps (float, optional): Stop criterion, by default 0.001\n        max_iters (int, optional): Maximum number of iterations, by default 1000\n\n    \"\"\"\n\n    features_train = self._data_preprocessing_train(features)\n    iters = 0\n    while True:\n        iters += 1\n        distances = self._distances(features_train, self._centroids)\n        self.labels = self._new_labels(distances)\n        centroids_before = self._centroids\n        self._centroids = self._new_centroids(features_train, self.labels)\n        moviment = self._moviment(centroids_before, self._centroids)\n        if (moviment &lt; eps) or (iters &gt; max_iters):\n            break\n</code></pre>"},{"location":"api/clustering/k_means/#sorix.clustering.k_means.Kmeans.predict","title":"predict","text":"<pre><code>predict(features)\n</code></pre> <p>Predict the labels of features</p> <p>Parameters:</p> <ul> <li> <code>features</code>               (<code>tensor</code>)           \u2013            <p>Features to predict.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>labels</code> (              <code>tensor</code> )          \u2013            <p>Labels of features</p> </li> </ul> Source code in <code>sorix/clustering/k_means.py</code> <pre><code>def predict(self, features: Tensor) -&gt; Tensor:\n\n    \"\"\"\n    Predict the labels of features\n\n    Parameters:\n        features (tensor): Features to predict.\n\n    Returns:\n        labels (tensor): Labels of features\n    \"\"\"\n\n    distances = self._distances(features, self._centroids)\n    labels = self._new_labels(distances)\n    return tensor(labels)\n</code></pre>"},{"location":"api/clustering/k_means/#sorix.clustering.k_means.Kmeans.get_distances","title":"get_distances","text":"<pre><code>get_distances(features)\n</code></pre> <p>Get distances between features and centroids</p> <p>Parameters:</p> <ul> <li> <code>features</code>               (<code>tensor</code>)           \u2013            <p>Features to predict.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>distances</code> (              <code>tensor</code> )          \u2013            <p>Distances betwen features and centroids</p> </li> </ul> Source code in <code>sorix/clustering/k_means.py</code> <pre><code>def get_distances(self, features: Tensor) -&gt; Tensor:\n\n    \"\"\"\n    Get distances between features and centroids\n\n    Parameters:\n        features (tensor): Features to predict.\n\n    Returns:\n        distances (tensor): Distances betwen features and centroids\n    \"\"\"\n\n    return tensor(self._distances(features, self._centroids))\n</code></pre>"},{"location":"api/clustering/k_means/#sorix.clustering.k_means.Kmeans.get_inertia","title":"get_inertia","text":"<pre><code>get_inertia(features)\n</code></pre> <p>Get inertia of features for k-centroids</p> <p>Parameters:</p> <ul> <li> <code>features</code>               (<code>tensor</code>)           \u2013            <p>Features to predict.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>inertia</code> (              <code>float</code> )          \u2013            <p>Inertia of features</p> </li> </ul> Source code in <code>sorix/clustering/k_means.py</code> <pre><code>def get_inertia(self, features: Tensor) -&gt; float:\n\n    \"\"\"\n    Get inertia of features for k-centroids\n\n    Parameters:\n        features (tensor): Features to predict.\n\n    Returns:\n        inertia (float): Inertia of features\n    \"\"\"\n\n    distances = self._distances(features, self._centroids)\n    labels = self._new_labels(distances)\n\n    return smat.sum((features - self.centroids[labels])**2).item()\n</code></pre>"},{"location":"api/cuda/","title":"Index","text":""},{"location":"api/cuda/#sorix.cuda","title":"sorix.cuda","text":""},{"location":"api/cuda/cuda/","title":"cuda","text":""},{"location":"api/cuda/cuda/#sorix.cuda.cuda","title":"sorix.cuda.cuda","text":""},{"location":"api/cupy/","title":"Index","text":""},{"location":"api/cupy/#sorix.cupy","title":"sorix.cupy","text":""},{"location":"api/cupy/cupy/","title":"cupy","text":""},{"location":"api/cupy/cupy/#sorix.cupy.cupy","title":"sorix.cupy.cupy","text":""},{"location":"api/metrics/","title":"Index","text":""},{"location":"api/metrics/#sorix.metrics","title":"sorix.metrics","text":""},{"location":"api/metrics/metrics/","title":"metrics","text":""},{"location":"api/metrics/metrics/#sorix.metrics.metrics","title":"sorix.metrics.metrics","text":""},{"location":"api/metrics/metrics/#sorix.metrics.metrics.mean_squared_error","title":"mean_squared_error","text":"<pre><code>mean_squared_error(Y_true, Y_pred)\n</code></pre> <p>Computes the mean squared error regression loss.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def mean_squared_error(Y_true: Any, Y_pred: Any) -&gt; float:\n    \"\"\"Computes the mean squared error regression loss.\"\"\"\n    Y_true, Y_pred = _get_metric_data(Y_true, Y_pred)\n    mse = ((Y_true-Y_pred)**2).mean()\n    return mse.item() if hasattr(mse, 'item') else float(mse)\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.root_mean_squared_error","title":"root_mean_squared_error","text":"<pre><code>root_mean_squared_error(Y_true, Y_pred)\n</code></pre> <p>Computes the root mean squared error regression loss.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def root_mean_squared_error(Y_true: Any, Y_pred: Any) -&gt; float:\n    \"\"\"Computes the root mean squared error regression loss.\"\"\"\n    return mean_squared_error(Y_true, Y_pred)**0.5\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.mean_absolute_error","title":"mean_absolute_error","text":"<pre><code>mean_absolute_error(Y_true, Y_pred)\n</code></pre> <p>Computes the mean absolute error regression loss.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def mean_absolute_error(Y_true: Any, Y_pred: Any) -&gt; float:\n    \"\"\"Computes the mean absolute error regression loss.\"\"\"\n    Y_true, Y_pred = _get_metric_data(Y_true, Y_pred)\n    if isinstance(Y_true, Tensor):\n        mae = (Y_true-Y_pred).abs().mean()\n    else:\n        mae = np.abs(Y_true-Y_pred).mean()\n    return mae.item() if hasattr(mae, 'item') else float(mae)\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.mean_absolute_percentage_error","title":"mean_absolute_percentage_error","text":"<pre><code>mean_absolute_percentage_error(Y_true, Y_pred)\n</code></pre> <p>Computes the mean absolute percentage error regression loss.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def mean_absolute_percentage_error(Y_true: Any, Y_pred: Any) -&gt; float:\n    \"\"\"Computes the mean absolute percentage error regression loss.\"\"\"\n    Y_true, Y_pred = _get_metric_data(Y_true, Y_pred)\n    if isinstance(Y_true, Tensor):\n        mape = ((Y_true-Y_pred)/Y_true).abs().mean()\n    else:\n        mape = np.abs((Y_true-Y_pred)/Y_true).mean()\n    return mape.item() if hasattr(mape, 'item') else float(mape)\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.r2_score","title":"r2_score","text":"<pre><code>r2_score(Y_true, Y_pred)\n</code></pre> <p>Computes the R^2 (coefficient of determination) regression score.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def r2_score(Y_true: Any, Y_pred: Any) -&gt; float:\n    \"\"\"Computes the R^2 (coefficient of determination) regression score.\"\"\"\n    Y_true, Y_pred = _get_metric_data(Y_true, Y_pred)\n    sr = ((Y_true-Y_pred)**2).mean()\n    sy = ((Y_true-Y_true.mean())**2).mean()\n    r2 = (1-(sr/sy))\n    return r2.item() if hasattr(r2, 'item') else float(r2)\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.regression_report","title":"regression_report","text":"<pre><code>regression_report(y_true, y_pred)\n</code></pre> <p>Returns a comprehensive regression report as a formatted string.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def regression_report(y_true: Any, y_pred: Any) -&gt; str:\n    \"\"\"\n    Returns a comprehensive regression report as a formatted string.\n    \"\"\"\n    metrics = {\n        \"R2\":   (r2_score(y_true, y_pred), \"[0,   1]\"),\n        \"MAE\":  (mean_absolute_error(y_true, y_pred), \"[0,  \u221e)\"),\n        \"MSE\":  (mean_squared_error(y_true, y_pred), \"[0,  \u221e)\"),\n        \"RMSE\": (root_mean_squared_error(y_true, y_pred), \"[0,  \u221e)\"),\n        \"MAPE\": (mean_absolute_percentage_error(y_true, y_pred) * 100, \"[0, 100]\"),\n    }\n\n    # Force all ranges to the same length (8 characters)\n    fixed_width = 8\n    metrics_with_ranges = {}\n    for k, (val, rng) in metrics.items():\n        metrics_with_ranges[k] = (val, rng.ljust(fixed_width))\n\n    col_metric = 6\n    col_score = 9\n    col_range = fixed_width\n\n    header = f\"{'Metric':&lt;{col_metric}} | {'Score':&gt;{col_score}} | {'Range':&gt;{col_range}}\"\n    lines = [header, \"-\" * len(header)]\n\n    for name, (value, rng) in metrics_with_ranges.items():\n        lines.append(f\"{name:&lt;{col_metric}} | {value:&gt;{col_score}.4f} | {rng:&gt;{col_range}}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.accuracy_score","title":"accuracy_score","text":"<pre><code>accuracy_score(Y_true, Y_pred)\n</code></pre> <p>Computes the accuracy classification score.</p> <p>Examples:</p> <pre><code>y_true = [0, 1, 2, 3]\ny_pred = [0, 2, 1, 3]\nacc = accuracy_score(y_true, y_pred) # 0.5\n</code></pre> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def accuracy_score(Y_true: Any, Y_pred: Any) -&gt; float:\n    \"\"\"\n    Computes the accuracy classification score.\n\n    Examples:\n        ```python\n        y_true = [0, 1, 2, 3]\n        y_pred = [0, 2, 1, 3]\n        acc = accuracy_score(y_true, y_pred) # 0.5\n        ```\n    \"\"\"\n    Y_true, Y_pred = _get_metric_data(Y_true, Y_pred)\n    if isinstance(Y_true, Tensor):\n        acc = (Y_true == Y_pred).mean()\n    else:\n        acc = (Y_true == Y_pred).mean()\n    return acc.item() if hasattr(acc, 'item') else float(acc)\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.confusion_matrix","title":"confusion_matrix","text":"<pre><code>confusion_matrix(y_true, y_pred)\n</code></pre> <p>Computes confusion matrix to evaluate the accuracy of a classification.</p> <p>Examples:</p> <pre><code>y_true = [2, 0, 2, 2, 0, 1]\ny_pred = [0, 0, 2, 2, 0, 2]\ncm = confusion_matrix(y_true, y_pred)\n# array([[2, 0, 0],\n#        [0, 0, 1],\n#        [1, 0, 2]])\n</code></pre> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def confusion_matrix(y_true: Any, y_pred: Any) -&gt; np.ndarray:\n    \"\"\"\n    Computes confusion matrix to evaluate the accuracy of a classification.\n\n    Examples:\n        ```python\n        y_true = [2, 0, 2, 2, 0, 1]\n        y_pred = [0, 0, 2, 2, 0, 2]\n        cm = confusion_matrix(y_true, y_pred)\n        # array([[2, 0, 0],\n        #        [0, 0, 1],\n        #        [1, 0, 2]])\n        ```\n    \"\"\"\n    y_true_data, y_pred_data = _get_classification_data(y_true, y_pred)\n\n    classes = np.unique(y_true_data)\n    cm = np.zeros((len(classes), len(classes)))\n\n    for i, c1 in enumerate(classes):\n        for j, c2 in enumerate(classes):\n            cm[i, j] = np.sum((y_true_data == c1) &amp; (y_pred_data == c2))\n    cm = cm.astype(int)\n\n    return cm\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.precision_score","title":"precision_score","text":"<pre><code>precision_score(\n    y_true, y_pred, average=\"binary\", pos_label=1\n)\n</code></pre> <p>Computes the precision - the ability of the classifier not to label as positive a sample that is negative.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def precision_score(y_true: Any, y_pred: Any, average: str = 'binary', pos_label: Union[int, str] = 1) -&gt; Union[float, np.ndarray]:\n    \"\"\"Computes the precision - the ability of the classifier not to label as positive a sample that is negative.\"\"\"\n    y_true_data, y_pred_data = _get_classification_data(y_true, y_pred)\n    classes = np.unique(y_true_data)\n\n    if average == 'binary':\n        true_pos = np.sum((y_true_data == pos_label) &amp; (y_pred_data == pos_label))\n        pred_pos = np.sum(y_pred_data == pos_label)\n        return true_pos / pred_pos if pred_pos &gt; 0 else 0.0\n\n    precisions = []\n    supports = []\n    for c in classes:\n        true_pos = np.sum((y_true_data == c) &amp; (y_pred_data == c))\n        pred_pos = np.sum(y_pred_data == c)\n        precisions.append(true_pos / pred_pos if pred_pos &gt; 0 else 0.0)\n        supports.append(np.sum(y_true_data == c))\n\n    if average == 'macro':\n        return np.mean(precisions)\n    elif average == 'weighted':\n        return np.average(precisions, weights=supports)\n    return np.array(precisions)\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.recall_score","title":"recall_score","text":"<pre><code>recall_score(y_true, y_pred, average='binary', pos_label=1)\n</code></pre> <p>Computes the recall - the ability of the classifier to find all the positive samples.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def recall_score(y_true: Any, y_pred: Any, average: str = 'binary', pos_label: Union[int, str] = 1) -&gt; Union[float, np.ndarray]:\n    \"\"\"Computes the recall - the ability of the classifier to find all the positive samples.\"\"\"\n    y_true_data, y_pred_data = _get_classification_data(y_true, y_pred)\n    classes = np.unique(y_true_data)\n\n    if average == 'binary':\n        true_pos = np.sum((y_true_data == pos_label) &amp; (y_pred_data == pos_label))\n        actual_pos = np.sum(y_true_data == pos_label)\n        return true_pos / actual_pos if actual_pos &gt; 0 else 0.0\n\n    recalls = []\n    supports = []\n    for c in classes:\n        true_pos = np.sum((y_true_data == c) &amp; (y_pred_data == c))\n        actual_pos = np.sum(y_true_data == c)\n        recalls.append(true_pos / actual_pos if actual_pos &gt; 0 else 0.0)\n        supports.append(actual_pos)\n\n    if average == 'macro':\n        return np.mean(recalls)\n    elif average == 'weighted':\n        return np.average(recalls, weights=supports)\n    return np.array(recalls)\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.f1_score","title":"f1_score","text":"<pre><code>f1_score(y_true, y_pred, average='binary', pos_label=1)\n</code></pre> <p>Computes the F1 score, also known as balanced F-score or F-measure.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def f1_score(y_true: Any, y_pred: Any, average: str = 'binary', pos_label: Union[int, str] = 1) -&gt; Union[float, np.ndarray]:\n    \"\"\"Computes the F1 score, also known as balanced F-score or F-measure.\"\"\"\n    p = precision_score(y_true, y_pred, average=average, pos_label=pos_label)\n    r = recall_score(y_true, y_pred, average=average, pos_label=pos_label)\n\n    if isinstance(p, (np.ndarray, list)):\n        p_arr = np.array(p)\n        r_arr = np.array(r)\n        denom = p_arr + r_arr\n        denom[denom == 0] = 1e-9\n        return 2 * p_arr * r_arr / denom\n\n    return 2 * p * r / (p + r) if (p + r) &gt; 0 else 0.0\n</code></pre>"},{"location":"api/metrics/metrics/#sorix.metrics.metrics.classification_report","title":"classification_report","text":"<pre><code>classification_report(y_true, y_pred)\n</code></pre> <p>Builds a text report showing the main classification metrics.</p> Source code in <code>sorix/metrics/metrics.py</code> <pre><code>def classification_report(y_true: Any, y_pred: Any) -&gt; str:\n    \"\"\"\n    Builds a text report showing the main classification metrics.\n    \"\"\"\n    y_true_data, y_pred_data = _get_classification_data(y_true, y_pred)\n    classes = sorted(np.unique(y_true_data))\n    report = {}\n\n    total_true = len(y_true_data)\n\n    # Metrics per class\n    for c in classes:\n        true_pos = np.sum((y_true_data == c) &amp; (y_pred_data == c))\n        pred_pos = np.sum(y_pred_data == c)\n        actual_pos = np.sum(y_true_data == c)\n\n        precision = true_pos / pred_pos if pred_pos &gt; 0 else 0.0\n        recall = true_pos / actual_pos if actual_pos &gt; 0 else 0.0\n        f1 = (2 * precision * recall / (precision + recall)\n              if (precision + recall) &gt; 0 else 0.0)\n\n        report[c] = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1,\n            \"support\": actual_pos\n        }\n\n    # Macro average\n    macro_precision = np.mean([report[c][\"precision\"] for c in classes])\n    macro_recall = np.mean([report[c][\"recall\"] for c in classes])\n    macro_f1 = np.mean([report[c][\"f1\"] for c in classes])\n\n    # Weighted average\n    weights = np.array([report[c][\"support\"] for c in classes])\n    weighted_precision = np.average([report[c][\"precision\"] for c in classes], weights=weights)\n    weighted_recall = np.average([report[c][\"recall\"] for c in classes], weights=weights)\n    weighted_f1 = np.average([report[c][\"f1\"] for c in classes], weights=weights)\n\n    header = f\"{'':&lt;12}{'precision':&gt;9}{'recall':&gt;9}{'f1-score':&gt;9}{'support':&gt;9}\"\n    lines = [header]\n\n    for c in classes:\n        lines.append(f\"{str(c):&lt;12}{report[c]['precision']:&gt;9.2f}{report[c]['recall']:&gt;9.2f}{report[c]['f1']:&gt;9.2f}{report[c]['support']:&gt;9}\")\n\n    lines.append(\"\")\n\n    accuracy = np.sum(y_true_data == y_pred_data) / total_true\n    lines.append(f\"{'accuracy':&lt;12}{'':&gt;9}{'':&gt;9}{accuracy:&gt;9.2f}{total_true:&gt;9}\")\n\n    lines.append(f\"{'macro avg':&lt;12}{macro_precision:&gt;9.2f}{macro_recall:&gt;9.2f}{macro_f1:&gt;9.2f}{total_true:&gt;9}\")\n    lines.append(f\"{'weighted avg':&lt;12}{weighted_precision:&gt;9.2f}{weighted_recall:&gt;9.2f}{weighted_f1:&gt;9.2f}{total_true:&gt;9}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/model_selection/","title":"Index","text":""},{"location":"api/model_selection/#sorix.model_selection","title":"sorix.model_selection","text":""},{"location":"api/model_selection/train_test/","title":"train_test","text":""},{"location":"api/model_selection/train_test/#sorix.model_selection.train_test","title":"sorix.model_selection.train_test","text":""},{"location":"api/model_selection/train_test/#sorix.model_selection.train_test.train_test_split","title":"train_test_split","text":"<pre><code>train_test_split(\n    X, Y=None, test_size=0.2, random_state=42, shuffle=True\n)\n</code></pre> <p>Method to split the data into train and test sets.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>DataFrame</code>)           \u2013            <p>Features.</p> </li> <li> <code>Y</code>               (<code>Series</code>, default:                   <code>None</code> )           \u2013            <p>Labels.</p> </li> <li> <code>test_size</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>Proportion of the dataset to include in the test split. Default is 0.2.</p> </li> <li> <code>random_state</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random state for reproducibility. Default is 42.</p> </li> <li> <code>shuffle</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to shuffle the data before splitting. Default is True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (              <code>tuple</code> )          \u2013            <p>(X_train, X_test, y_train, y_test)</p> </li> </ul> Source code in <code>sorix/model_selection/train_test.py</code> <pre><code>def train_test_split(X: pd.DataFrame,\n                     Y: pd.Series = None,\n                     test_size: float = 0.2,\n                     random_state: int = 42,\n                     shuffle: bool = True) -&gt; tuple:\n    \"\"\"\n    Method to split the data into train and test sets.\n\n    Args:\n        X (pd.DataFrame): Features.\n        Y (pd.Series): Labels.\n        test_size (float, optional): Proportion of the dataset to include in the test split. Default is 0.2.\n        random_state (int, optional): Random state for reproducibility. Default is 42.\n        shuffle (bool, optional): Whether to shuffle the data before splitting. Default is True.\n\n    Returns:\n        tuple: (X_train, X_test, y_train, y_test)\n    \"\"\"\n\n    # Validate test_size\n    if not (0 &lt; test_size &lt; 1):\n        raise ValueError(\"test_size must be between 0 and 1.\")\n\n    # Set random seed for reproducibility\n    np.random.seed(random_state)\n\n    # Shuffle the data if specified\n    if shuffle:\n        indices = np.random.permutation(len(X))\n        X = X.iloc[indices]\n        if Y is not None:\n            Y = Y.iloc[indices]\n\n    # Calculate the split index\n    split_index = int(len(X) * (1 - test_size))\n\n    # Split the data\n    X_train = X.iloc[:split_index]\n    X_test = X.iloc[split_index:]\n    if Y is not None:\n        Y_train = Y.iloc[:split_index]\n        Y_test = Y.iloc[split_index:]\n        return X_train, X_test, Y_train, Y_test\n\n    return X_train, X_test\n</code></pre>"},{"location":"api/nn/","title":"Index","text":""},{"location":"api/nn/#sorix.nn","title":"sorix.nn","text":"<p>Neural network components including layers, containers, and loss functions.</p>"},{"location":"api/nn/init/","title":"init","text":""},{"location":"api/nn/init/#sorix.nn.init","title":"sorix.nn.init","text":""},{"location":"api/nn/init/#sorix.nn.init.uniform_","title":"uniform_","text":"<pre><code>uniform_(tensor, a=0.0, b=1.0)\n</code></pre> <p>Fills the input tensor with values drawn from the uniform distribution U(a, b).</p> Source code in <code>sorix/nn/init.py</code> <pre><code>def uniform_(tensor: Any, a: float = 0.0, b: float = 1.0) -&gt; Any:\n    \"\"\"Fills the input tensor with values drawn from the uniform distribution U(a, b).\"\"\"\n    xp = _get_xp(tensor)\n    tensor.data = xp.random.uniform(a, b, size=tensor.shape)\n    return tensor\n</code></pre>"},{"location":"api/nn/init/#sorix.nn.init.normal_","title":"normal_","text":"<pre><code>normal_(tensor, mean=0.0, std=1.0)\n</code></pre> <p>Fills the input tensor with values drawn from the normal distribution N(mean, std^2).</p> Source code in <code>sorix/nn/init.py</code> <pre><code>def normal_(tensor: Any, mean: float = 0.0, std: float = 1.0) -&gt; Any:\n    \"\"\"Fills the input tensor with values drawn from the normal distribution N(mean, std^2).\"\"\"\n    xp = _get_xp(tensor)\n    tensor.data = xp.random.normal(mean, std, size=tensor.shape)\n    return tensor\n</code></pre>"},{"location":"api/nn/init/#sorix.nn.init.constant_","title":"constant_","text":"<pre><code>constant_(tensor, val)\n</code></pre> <p>Fills the input tensor with the value val.</p> Source code in <code>sorix/nn/init.py</code> <pre><code>def constant_(tensor: Any, val: float) -&gt; Any:\n    \"\"\"Fills the input tensor with the value val.\"\"\"\n    xp = _get_xp(tensor)\n    tensor.data = xp.full(tensor.shape, val)\n    return tensor\n</code></pre>"},{"location":"api/nn/init/#sorix.nn.init.zeros_","title":"zeros_","text":"<pre><code>zeros_(tensor)\n</code></pre> <p>Fills the input tensor with the scalar value 0.</p> Source code in <code>sorix/nn/init.py</code> <pre><code>def zeros_(tensor: Any) -&gt; Any:\n    \"\"\"Fills the input tensor with the scalar value 0.\"\"\"\n    return constant_(tensor, 0.0)\n</code></pre>"},{"location":"api/nn/init/#sorix.nn.init.ones_","title":"ones_","text":"<pre><code>ones_(tensor)\n</code></pre> <p>Fills the input tensor with the scalar value 1.</p> Source code in <code>sorix/nn/init.py</code> <pre><code>def ones_(tensor: Any) -&gt; Any:\n    \"\"\"Fills the input tensor with the scalar value 1.\"\"\"\n    return constant_(tensor, 1.0)\n</code></pre>"},{"location":"api/nn/init/#sorix.nn.init.xavier_uniform_","title":"xavier_uniform_","text":"<pre><code>xavier_uniform_(tensor, gain=1.0)\n</code></pre> <p>Fills the input tensor with values according to the Xavier uniform initialization.</p> Source code in <code>sorix/nn/init.py</code> <pre><code>def xavier_uniform_(tensor: Any, gain: float = 1.0) -&gt; Any:\n    \"\"\"Fills the input tensor with values according to the Xavier uniform initialization.\"\"\"\n    xp = _get_xp(tensor)\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    std = gain * xp.sqrt(6.0 / (fan_in + fan_out))\n    return uniform_(tensor, -std, std)\n</code></pre>"},{"location":"api/nn/init/#sorix.nn.init.xavier_normal_","title":"xavier_normal_","text":"<pre><code>xavier_normal_(tensor, gain=1.0)\n</code></pre> <p>Fills the input tensor with values according to the Xavier normal initialization.</p> Source code in <code>sorix/nn/init.py</code> <pre><code>def xavier_normal_(tensor: Any, gain: float = 1.0) -&gt; Any:\n    \"\"\"Fills the input tensor with values according to the Xavier normal initialization.\"\"\"\n    xp = _get_xp(tensor)\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    std = gain * xp.sqrt(2.0 / (fan_in + fan_out))\n    return normal_(tensor, 0.0, std)\n</code></pre>"},{"location":"api/nn/init/#sorix.nn.init.kaiming_uniform_","title":"kaiming_uniform_","text":"<pre><code>kaiming_uniform_(\n    tensor, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\"\n)\n</code></pre> <p>Fills the input tensor with values according to the Kaiming uniform initialization.</p> Source code in <code>sorix/nn/init.py</code> <pre><code>def kaiming_uniform_(tensor: Any, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu') -&gt; Any:\n    \"\"\"Fills the input tensor with values according to the Kaiming uniform initialization.\"\"\"\n    xp = _get_xp(tensor)\n    fan = _calculate_correct_fan(tensor, mode)\n    gain = _calculate_gain(nonlinearity, a)\n    std = gain / xp.sqrt(fan)\n    bound = xp.sqrt(3.0) * std  \n    return uniform_(tensor, -bound, bound)\n</code></pre>"},{"location":"api/nn/init/#sorix.nn.init.kaiming_normal_","title":"kaiming_normal_","text":"<pre><code>kaiming_normal_(\n    tensor, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\"\n)\n</code></pre> <p>Fills the input tensor with values according to the Kaiming normal initialization.</p> Source code in <code>sorix/nn/init.py</code> <pre><code>def kaiming_normal_(tensor: Any, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu') -&gt; Any:\n    \"\"\"Fills the input tensor with values according to the Kaiming normal initialization.\"\"\"\n    xp = _get_xp(tensor)\n    fan = _calculate_correct_fan(tensor, mode)\n    gain = _calculate_gain(nonlinearity, a)\n    std = gain / xp.sqrt(fan)\n    return normal_(tensor, 0.0, std)\n</code></pre>"},{"location":"api/nn/layers/","title":"layers","text":""},{"location":"api/nn/layers/#sorix.nn.layers","title":"sorix.nn.layers","text":""},{"location":"api/nn/layers/#sorix.nn.layers.Linear","title":"Linear","text":"<pre><code>Linear(\n    features, neurons, bias=True, init=\"he\", device=\"cpu\"\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Applies a linear transformation to the incoming data.</p> <p>Attributes:</p> <ul> <li> <code>W</code>               (<code>Tensor</code>)           \u2013            <p>Weights of the layer.</p> </li> <li> <code>b</code>               (<code>Tensor</code>)           \u2013            <p>Biases of the layer.</p> </li> </ul> <p>Examples:</p> <pre><code>layer = Linear(10, 5)\nx = tensor(np.random.randn(8, 10))\ny = layer(x)\nprint(y.shape)  # (8, 5)\n</code></pre> Source code in <code>sorix/nn/layers.py</code> <pre><code>def __init__(\n    self, \n    features: int, \n    neurons: int,\n    bias: bool = True, \n    init: str = 'he',\n    device: str = 'cpu'\n) -&gt; None:\n    super().__init__()\n    if device == 'cuda' and not _cupy_available:\n        raise Exception('Cupy is not available')\n\n    self.device = device\n    xp = cp if device == 'cuda' else np\n\n    if init not in ['he', 'xavier']:\n        raise ValueError(f'Invalid initialization method: {init}. Valid methods are \"he\" and \"xavier\"')\n\n    if init == 'he':\n        self.std_dev = xp.sqrt(2.0 / features)  # He init for ReLU\n    elif init == 'xavier':\n        self.std_dev = xp.sqrt(2.0 / (features + neurons))  # Xavier init for tanh\n\n    self.bias = bias\n    self.W = tensor(xp.random.normal(0, self.std_dev, size=(features, neurons)), \n                    device=self.device, requires_grad=True, dtype=float32)\n    self.b = tensor(xp.zeros((1, neurons)), \n                    device=self.device, requires_grad=True, dtype=float32) if self.bias else None\n</code></pre>"},{"location":"api/nn/layers/#sorix.nn.layers.Linear.coef_","title":"coef_  <code>property</code>","text":"<pre><code>coef_\n</code></pre> <p>Returns weights as a flattened numpy array (Scikit-Learn parity).</p>"},{"location":"api/nn/layers/#sorix.nn.layers.Linear.intercept_","title":"intercept_  <code>property</code>","text":"<pre><code>intercept_\n</code></pre> <p>Returns biases as a flattened numpy array or scalar (Scikit-Learn parity).</p>"},{"location":"api/nn/layers/#sorix.nn.layers.ReLU","title":"ReLU","text":"<pre><code>ReLU()\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Rectified Linear Unit activation function.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self.device: str = 'cpu'\n    self.training: bool = True\n</code></pre>"},{"location":"api/nn/layers/#sorix.nn.layers.Sigmoid","title":"Sigmoid","text":"<pre><code>Sigmoid()\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Numerically stable Sigmoid activation function.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self.device: str = 'cpu'\n    self.training: bool = True\n</code></pre>"},{"location":"api/nn/layers/#sorix.nn.layers.Tanh","title":"Tanh","text":"<pre><code>Tanh()\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic tangent activation function.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self.device: str = 'cpu'\n    self.training: bool = True\n</code></pre>"},{"location":"api/nn/layers/#sorix.nn.layers.BatchNorm1d","title":"BatchNorm1d","text":"<pre><code>BatchNorm1d(\n    features, alpha=0.9, epsilon=1e-05, device=\"cpu\"\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Applies Batch Normalization over a 2D input.</p> Source code in <code>sorix/nn/layers.py</code> <pre><code>def __init__(\n    self, \n    features: int, \n    alpha: float = 0.9, \n    epsilon: float = 1e-5, \n    device: str = 'cpu'\n) -&gt; None:\n    super().__init__()\n    self.device = device\n    xp = cp if device == 'cuda' else np\n\n    self.gamma = tensor(xp.ones((1, features)), requires_grad=True, dtype=float32)\n    self.beta = tensor(xp.zeros((1, features)), requires_grad=True, dtype=float32)\n\n    # buffers (captured by state_dict)\n    self.running_mean = tensor(xp.zeros((1, features)), requires_grad=False, dtype=float32)\n    self.running_var = tensor(xp.ones((1, features)), requires_grad=False, dtype=float32)\n\n    self.alpha = alpha\n    self.epsilon = epsilon\n    self.device = device\n\n    if self.device != 'cpu':\n        self.to(self.device)\n</code></pre>"},{"location":"api/nn/layers/#sorix.nn.layers.Dropout","title":"Dropout","text":"<pre><code>Dropout(p=0.5)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>During training, randomly zeroes some of the elements of the input tensor  with probability p using samples from a Bernoulli distribution.</p> Source code in <code>sorix/nn/layers.py</code> <pre><code>def __init__(self, p: float = 0.5) -&gt; None:\n    super().__init__()\n    self.p = p\n</code></pre>"},{"location":"api/nn/loss/","title":"loss","text":""},{"location":"api/nn/loss/#sorix.nn.loss","title":"sorix.nn.loss","text":""},{"location":"api/nn/loss/#sorix.nn.loss.MSELoss","title":"MSELoss","text":"<p>Computes the Mean Squared Error loss between the prediction and the target.</p>"},{"location":"api/nn/loss/#sorix.nn.loss.BCEWithLogitsLoss","title":"BCEWithLogitsLoss","text":"<p>This loss combines a Sigmoid layer and the BCELoss in one single class. More numerically stable than using a plain Sigmoid followed by a BCELoss.</p>"},{"location":"api/nn/loss/#sorix.nn.loss.CrossEntropyLoss","title":"CrossEntropyLoss","text":"<pre><code>CrossEntropyLoss(one_hot=False)\n</code></pre> <p>This criterion computes the cross entropy loss between input and target.</p> Source code in <code>sorix/nn/loss.py</code> <pre><code>def __init__(self, one_hot: bool = False) -&gt; None:\n    self.one_hot = one_hot\n    self.xp = np\n</code></pre>"},{"location":"api/nn/net/","title":"net","text":""},{"location":"api/nn/net/#sorix.nn.net","title":"sorix.nn.net","text":""},{"location":"api/nn/net/#sorix.nn.net.Module","title":"Module","text":"<pre><code>Module()\n</code></pre> <p>Base class for all neural network modules.</p> <p>Your models should also subclass this class.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self.device: str = 'cpu'\n    self.training: bool = True\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Defines the computation performed at every call. Should be overridden by all subclasses.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Defines the computation performed at every call.\n    Should be overridden by all subclasses.\n    \"\"\"\n    raise NotImplementedError(\"You must implement forward in the subclass.\")\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.parameters","title":"parameters","text":"<pre><code>parameters()\n</code></pre> <p>Returns an iterator over module parameters (tensors that require gradients).</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def parameters(self) -&gt; List[Tensor]:\n    \"\"\"\n    Returns an iterator over module parameters (tensors that require gradients).\n    \"\"\"\n    params: List[Tensor] = []\n    visited: Set[int] = set()\n\n    def _gather_params(obj: Any) -&gt; None:\n        if id(obj) in visited:\n            return\n        visited.add(id(obj))\n\n        if isinstance(obj, Tensor):\n            if obj.requires_grad:\n                params.append(obj)\n        elif hasattr(obj, \"parameters\") and callable(obj.parameters) and obj is not self:\n            # If the object has its own parameters() method, use it\n            params.extend(obj.parameters())\n        elif hasattr(obj, \"__dict__\"):\n            # Recurse into attributes\n            for k, v in obj.__dict__.items():\n                if not k.startswith('_'):\n                    _gather_params(v)\n        elif isinstance(obj, (list, tuple)):\n            for item in obj:\n                _gather_params(item)\n        elif isinstance(obj, dict):\n            for item in obj.values():\n                _gather_params(item)\n\n    _gather_params(self)\n    return params\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.to","title":"to","text":"<pre><code>to(device)\n</code></pre> <p>Moves all model parameters and buffers to the specified device.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>)           \u2013            <p>'cpu' or 'cuda'.</p> </li> </ul> Source code in <code>sorix/nn/net.py</code> <pre><code>def to(self, device: str) -&gt; Module:\n    \"\"\"\n    Moves all model parameters and buffers to the specified device.\n\n    Args:\n        device: 'cpu' or 'cuda'.\n    \"\"\"\n    self.device = device\n\n    def _apply(obj: Any) -&gt; Any:\n        if hasattr(obj, \"to\") and callable(obj.to) and obj is not self:\n            return obj.to(device)\n\n        if isinstance(obj, Tensor):\n            return obj.to(device)\n\n        if isinstance(obj, list):\n            return [_apply(v) for v in obj]\n        if isinstance(obj, tuple):\n            return tuple(_apply(v) for v in obj)\n        if isinstance(obj, dict):\n            return {k: _apply(v) for k, v in obj.items()}\n\n        if hasattr(obj, \"__dict__\") and obj is not self:\n             # Try to move attributes of non-Module objects\n             for k, v in obj.__dict__.items():\n                 if not k.startswith('_'):\n                     setattr(obj, k, _apply(v))\n\n        return obj\n\n    for k, v in self.__dict__.items():\n        if not k.startswith('_'):\n            setattr(self, k, _apply(v))\n\n    return self\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.train","title":"train","text":"<pre><code>train()\n</code></pre> <p>Sets the module in training mode.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def train(self) -&gt; None:\n    \"\"\"Sets the module in training mode.\"\"\"\n    self.training = True\n    def _apply(obj: Any) -&gt; None:\n        if hasattr(obj, \"training\"):\n            obj.training = True\n        if isinstance(obj, (list, tuple)):\n            for o in obj: _apply(o)\n        elif isinstance(obj, dict):\n            for v in obj.values(): _apply(v)\n    for k, v in self.__dict__.items():\n        if not k.startswith('_'):\n            _apply(v)\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.eval","title":"eval","text":"<pre><code>eval()\n</code></pre> <p>Sets the module in evaluation mode.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def eval(self) -&gt; None:\n    \"\"\"Sets the module in evaluation mode.\"\"\"\n    self.training = False\n    def _apply(obj: Any) -&gt; None:\n        if hasattr(obj, \"training\"):\n            obj.training = False\n        if isinstance(obj, (list, tuple)):\n            for o in obj: _apply(o)\n        elif isinstance(obj, dict):\n            for v in obj.values(): _apply(v)\n    for k, v in self.__dict__.items():\n        if not k.startswith('_'):\n            _apply(v)\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.state_dict","title":"state_dict","text":"<pre><code>state_dict()\n</code></pre> <p>Returns a dictionary containing the whole state of the module (parameters and buffers).</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, Tensor]:\n    \"\"\"\n    Returns a dictionary containing the whole state of the module (parameters and buffers).\n    \"\"\"\n    state: Dict[str, Tensor] = {}\n\n    def _get_state(obj: Any, prefix: str) -&gt; None:\n        for name, val in obj.__dict__.items():\n            if name.startswith('_') or name == 'device' or name == 'training':\n                continue\n\n            key = prefix + name\n            if isinstance(val, Tensor):\n                state[key] = val\n            elif hasattr(val, 'state_dict') and callable(val.state_dict) and val is not self:\n                sub_state = val.state_dict()\n                for sk, sv in sub_state.items():\n                    state[key + '.' + sk] = sv\n            elif hasattr(val, '__dict__'):\n                 for kn, kv in val.__dict__.items():\n                     if isinstance(kv, Tensor):\n                         state[key + '.' + kn] = kv\n\n    _get_state(self, \"\")\n    return state\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> <p>Set the extra representation of the module. Should be overridden by subclasses.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Set the extra representation of the module.\n    Should be overridden by subclasses.\n    \"\"\"\n    return \"\"\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Module.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict)\n</code></pre> <p>Copies parameters and buffers from state_dict into this module and its descendants.</p> Source code in <code>sorix/nn/net.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Tensor]) -&gt; None:\n    \"\"\"\n    Copies parameters and buffers from state_dict into this module and its descendants.\n    \"\"\"\n    own_state = self.state_dict()\n    for name, param in state_dict.items():\n        if name in own_state:\n            if isinstance(param, Tensor):\n                own_state[name].data = param.data\n                own_state[name].to(own_state[name].device) # Ensure device consistency\n            else:\n                pass\n        else:\n            pass\n</code></pre>"},{"location":"api/nn/net/#sorix.nn.net.Sequential","title":"Sequential","text":"<pre><code>Sequential(*args)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A sequential container. Modules will be added to it in the order they are passed in the constructor.</p> <p>Examples:</p> <pre><code>model = nn.Sequential(\n    nn.Linear(10, 5),\n    nn.ReLU(),\n    nn.Linear(5, 2)\n)\n</code></pre> Source code in <code>sorix/nn/net.py</code> <pre><code>def __init__(self, *args: Any) -&gt; None:\n    super().__init__()\n    self._modules: Dict[str, Module] = {}\n    if len(args) == 1 and isinstance(args[0], dict):\n        for name, module in args[0].items():\n            setattr(self, name, module)\n        self._modules = args[0]\n    else:\n        for idx, module in enumerate(args):\n            name = str(idx)\n            setattr(self, name, module)\n            self._modules[name] = module\n</code></pre>"},{"location":"api/optim/","title":"Index","text":""},{"location":"api/optim/#sorix.optim","title":"sorix.optim","text":""},{"location":"api/optim/optim/","title":"optim","text":""},{"location":"api/optim/optim/#sorix.optim.optim","title":"sorix.optim.optim","text":""},{"location":"api/optim/optim/#sorix.optim.optim.Optimizer","title":"Optimizer","text":"<pre><code>Optimizer(parameters, lr=0.001)\n</code></pre> <p>Base class for all optimizers.</p> Source code in <code>sorix/optim/optim.py</code> <pre><code>def __init__(self, parameters: List[Tensor], lr: float = 1e-3) -&gt; None:\n    self.parameters = parameters\n    self.lr = lr\n    self.device = parameters[0].device\n    self.xp = cp if self.device == 'cuda' else np\n</code></pre>"},{"location":"api/optim/optim/#sorix.optim.optim.Optimizer.zero_grad","title":"zero_grad","text":"<pre><code>zero_grad()\n</code></pre> <p>Sets gradients of all optimized tensors to zero.</p> Source code in <code>sorix/optim/optim.py</code> <pre><code>def zero_grad(self) -&gt; None:\n    \"\"\"Sets gradients of all optimized tensors to zero.\"\"\"\n    for param in self.parameters:\n        if param.grad is not None:\n            param.grad = self.xp.zeros_like(param.grad)\n</code></pre>"},{"location":"api/optim/optim/#sorix.optim.optim.Optimizer.step","title":"step","text":"<pre><code>step()\n</code></pre> <p>Performs a single optimization step.</p> Source code in <code>sorix/optim/optim.py</code> <pre><code>def step(self) -&gt; None:\n    \"\"\"Performs a single optimization step.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/optim/optim/#sorix.optim.optim.SGD","title":"SGD","text":"<pre><code>SGD(parameters, lr=0.001)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements stochastic gradient descent.</p> Source code in <code>sorix/optim/optim.py</code> <pre><code>def __init__(self, parameters: List[Tensor], lr: float = 1e-3) -&gt; None:\n    super().__init__(parameters, lr)\n</code></pre>"},{"location":"api/optim/optim/#sorix.optim.optim.SGDMomentum","title":"SGDMomentum","text":"<pre><code>SGDMomentum(parameters, lr=0.001, momentum=0.9)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements SGD with momentum.</p> Source code in <code>sorix/optim/optim.py</code> <pre><code>def __init__(self, parameters: List[Tensor], lr: float = 1e-3, momentum: float = 0.9) -&gt; None:\n    super().__init__(parameters, lr)\n    self.momentum = momentum\n    # Initialize velocity buffers for each parameter\n    self.vts = [self.xp.zeros_like(p.data) for p in self.parameters]\n</code></pre>"},{"location":"api/optim/optim/#sorix.optim.optim.RMSprop","title":"RMSprop","text":"<pre><code>RMSprop(parameters, lr=0.001, decay=0.9, epsilon=1e-08)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements RMSprop algorithm.</p> Source code in <code>sorix/optim/optim.py</code> <pre><code>def __init__(self, parameters: List[Tensor], lr: float = 1e-3, decay: float = 0.9, epsilon: float = 1e-8) -&gt; None:\n    super().__init__(parameters, lr)\n    self.decay = decay\n    self.epsilon = epsilon\n    # Initialize square gradient buffers\n    self.vts = [self.xp.zeros_like(p.data) for p in self.parameters]\n</code></pre>"},{"location":"api/optim/optim/#sorix.optim.optim.Adam","title":"Adam","text":"<pre><code>Adam(\n    parameters,\n    lr=0.001,\n    beta1=0.9,\n    beta2=0.999,\n    epsilon=1e-08,\n)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements Adam algorithm.</p> <p>Examples:</p> <pre><code>optimizer = Adam(model.parameters(), lr=1e-3)\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n</code></pre> Source code in <code>sorix/optim/optim.py</code> <pre><code>def __init__(self, parameters: List[Tensor], lr: float = 1e-3, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8) -&gt; None:\n    super().__init__(parameters, lr)\n    self.beta1 = beta1\n    self.beta2 = beta2\n    self.epsilon = epsilon\n    self.t = 0\n    # Initialize first and second moment buffers\n    self.vts = [self.xp.zeros_like(p.data) for p in self.parameters]\n    self.rts = [self.xp.zeros_like(p.data) for p in self.parameters]\n</code></pre>"},{"location":"api/preprocessing/","title":"Index","text":""},{"location":"api/preprocessing/#sorix.preprocessing","title":"sorix.preprocessing","text":""},{"location":"api/preprocessing/encoders/","title":"encoders","text":""},{"location":"api/preprocessing/encoders/#sorix.preprocessing.encoders","title":"sorix.preprocessing.encoders","text":""},{"location":"api/preprocessing/encoders/#sorix.preprocessing.encoders.OneHotEncoder","title":"OneHotEncoder","text":"<pre><code>OneHotEncoder()\n</code></pre> Source code in <code>sorix/preprocessing/encoders.py</code> <pre><code>def __init__(self):\n    self.n_features = 0\n    self.features = {\n\n    }\n</code></pre>"},{"location":"api/preprocessing/encoders/#sorix.preprocessing.encoders.OneHotEncoder.state_dict","title":"state_dict","text":"<pre><code>state_dict()\n</code></pre> <p>Returns a dictionary with the state of the encoder.</p> Source code in <code>sorix/preprocessing/encoders.py</code> <pre><code>def state_dict(self):\n    \"\"\"Returns a dictionary with the state of the encoder.\"\"\"\n    return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}\n</code></pre>"},{"location":"api/preprocessing/encoders/#sorix.preprocessing.encoders.OneHotEncoder.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict)\n</code></pre> <p>Loads the state of the encoder from a dictionary.</p> Source code in <code>sorix/preprocessing/encoders.py</code> <pre><code>def load_state_dict(self, state_dict):\n    \"\"\"Loads the state of the encoder from a dictionary.\"\"\"\n    for k, v in state_dict.items():\n        setattr(self, k, v)\n    return self\n</code></pre>"},{"location":"api/preprocessing/scalers/","title":"scalers","text":""},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers","title":"sorix.preprocessing.scalers","text":""},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.BaseScaler","title":"BaseScaler","text":"<pre><code>BaseScaler()\n</code></pre> <p>Base class for all scalers, implementing common methods.</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def __init__(self):\n    self.numerical_features: List[str] = []\n    self.n_features: int = 0\n</code></pre>"},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.BaseScaler.prepros","title":"prepros","text":"<pre><code>prepros(X)\n</code></pre> <p>Validates and registers column names.</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def prepros(self, X: Union[np.ndarray, pd.DataFrame]):\n    \"\"\"Validates and registers column names.\"\"\"\n    if isinstance(X, pd.DataFrame):\n        self.numerical_features = list(X.columns)\n        X = X.to_numpy()\n    elif isinstance(X, np.ndarray):\n        self.numerical_features = [f\"F{i}\" for i in range(X.shape[1])] if X.ndim &gt; 1 else [\"F0\"]\n    else:\n        raise TypeError(\"Input must be a NumPy ndarray or a Pandas DataFrame.\")\n\n    self.n_features = X.shape[1] if X.ndim &gt; 1 else 1\n    return X\n</code></pre>"},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.BaseScaler.state_dict","title":"state_dict","text":"<pre><code>state_dict()\n</code></pre> <p>Returns a dictionary with the scaler's state.</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def state_dict(self):\n    \"\"\"Returns a dictionary with the scaler's state.\"\"\"\n    return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}\n</code></pre>"},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.BaseScaler.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict)\n</code></pre> <p>Loads the scaler's state from a dictionary.</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def load_state_dict(self, state_dict):\n    \"\"\"Loads the scaler's state from a dictionary.\"\"\"\n    for k, v in state_dict.items():\n        setattr(self, k, v)\n    return self\n</code></pre>"},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.MinMaxScaler","title":"MinMaxScaler","text":"<pre><code>MinMaxScaler()\n</code></pre> <p>               Bases: <code>BaseScaler</code></p> <p>Scales features to a range [0, 1].</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.min: Optional[np.ndarray] = None\n    self.max: Optional[np.ndarray] = None\n</code></pre>"},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.StandardScaler","title":"StandardScaler","text":"<pre><code>StandardScaler()\n</code></pre> <p>               Bases: <code>BaseScaler</code></p> <p>Standardizes by removing the mean and scaling to unit variance.</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.mean: Optional[np.ndarray] = None\n    self.std: Optional[np.ndarray] = None\n</code></pre>"},{"location":"api/preprocessing/scalers/#sorix.preprocessing.scalers.RobustScaler","title":"RobustScaler","text":"<pre><code>RobustScaler()\n</code></pre> <p>               Bases: <code>BaseScaler</code></p> <p>Scales using median and IQR (robust to outliers).</p> Source code in <code>sorix/preprocessing/scalers.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.median: Optional[np.ndarray] = None\n    self.q1: Optional[np.ndarray] = None\n    self.q3: Optional[np.ndarray] = None\n</code></pre>"},{"location":"api/preprocessing/transformers/","title":"transformers","text":""},{"location":"api/preprocessing/transformers/#sorix.preprocessing.transformers","title":"sorix.preprocessing.transformers","text":""},{"location":"api/preprocessing/transformers/#sorix.preprocessing.transformers.ColumnTransformer","title":"ColumnTransformer","text":"<pre><code>ColumnTransformer(transformers)\n</code></pre> Source code in <code>sorix/preprocessing/transformers.py</code> <pre><code>def __init__(self, transformers: list[tuple]):\n    self.transformers = transformers\n    self.n_features = 0\n    self.features_names = []\n</code></pre>"},{"location":"api/preprocessing/transformers/#sorix.preprocessing.transformers.ColumnTransformer.state_dict","title":"state_dict","text":"<pre><code>state_dict()\n</code></pre> <p>Returns a dictionary with the state of the column transformer.</p> Source code in <code>sorix/preprocessing/transformers.py</code> <pre><code>def state_dict(self):\n    \"\"\"Returns a dictionary with the state of the column transformer.\"\"\"\n    state = {\n        'n_features': self.n_features,\n        'features_names': self.features_names,\n        'transformers_states': []\n    }\n    for name, tf, cols in self.transformers:\n        if hasattr(tf, 'state_dict') and callable(tf.state_dict):\n            state['transformers_states'].append((name, tf.state_dict(), cols))\n        else:\n            # Fallback if the transformer doesn't have state_dict (unlikely in sorix)\n            state['transformers_states'].append((name, tf, cols))\n    return state\n</code></pre>"},{"location":"api/preprocessing/transformers/#sorix.preprocessing.transformers.ColumnTransformer.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict)\n</code></pre> <p>Loads the state of the column transformer.</p> Source code in <code>sorix/preprocessing/transformers.py</code> <pre><code>def load_state_dict(self, state_dict):\n    \"\"\"Loads the state of the column transformer.\"\"\"\n    self.n_features = state_dict['n_features']\n    self.features_names = state_dict['features_names']\n\n    # Mapping for easier lookup\n    tf_states = {name: (state, cols) for name, state, cols in state_dict['transformers_states']}\n\n    for name, tf, cols in self.transformers:\n        if name in tf_states:\n            state, _ = tf_states[name]\n            if hasattr(tf, 'load_state_dict') and callable(tf.load_state_dict):\n                tf.load_state_dict(state)\n            else:\n                # If it was saved as the object itself\n                # (this might happen if tf didn't have state_dict when saved)\n                # But in our new system they will have it.\n                pass\n    return self\n</code></pre>"},{"location":"api/utils/","title":"Index","text":""},{"location":"api/utils/#sorix.utils","title":"sorix.utils","text":""},{"location":"api/utils/math/","title":"math","text":""},{"location":"api/utils/math/#sorix.utils.math","title":"sorix.utils.math","text":""},{"location":"api/utils/utils/","title":"utils","text":""},{"location":"api/utils/utils/#sorix.utils.utils","title":"sorix.utils.utils","text":""},{"location":"api/utils/utils/#sorix.utils.utils.save","title":"save","text":"<pre><code>save(obj, f)\n</code></pre> <p>Saves an object to a file using pickle.  Tensors will be automatically moved to CPU during serialization.</p> Source code in <code>sorix/utils/utils.py</code> <pre><code>def save(obj, f):\n    \"\"\"\n    Saves an object to a file using pickle. \n    Tensors will be automatically moved to CPU during serialization.\n    \"\"\"\n    if isinstance(f, str):\n        with open(f, 'wb') as file:\n            pickle.dump(obj, file)\n    else:\n        pickle.dump(obj, f)\n</code></pre>"},{"location":"api/utils/utils/#sorix.utils.utils.load","title":"load","text":"<pre><code>load(f)\n</code></pre> <p>Loads an object from a file using pickle.</p> Source code in <code>sorix/utils/utils.py</code> <pre><code>def load(f):\n    \"\"\"\n    Loads an object from a file using pickle.\n    \"\"\"\n    if isinstance(f, str):\n        with open(f, 'rb') as file:\n            return pickle.load(file)\n    else:\n        return pickle.load(f)\n</code></pre>"},{"location":"api/utils/data/","title":"Index","text":""},{"location":"api/utils/data/#sorix.utils.data","title":"sorix.utils.data","text":""},{"location":"api/utils/data/dataloader/","title":"dataloader","text":""},{"location":"api/utils/data/dataloader/#sorix.utils.data.dataloader","title":"sorix.utils.data.dataloader","text":""},{"location":"api/utils/data/dataset/","title":"dataset","text":""},{"location":"api/utils/data/dataset/#sorix.utils.data.dataset","title":"sorix.utils.data.dataset","text":""},{"location":"examples/clustering/Iris-dataset/","title":"Iris Dataset","text":"In\u00a0[1]: Copied! <pre>#Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> #Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sorix.clustering import Kmeans\nfrom sorix import tensor\n</pre> import pandas as pd import matplotlib.pyplot as plt from sorix.clustering import Kmeans from sorix import tensor In\u00a0[3]: Copied! <pre>data=pd.read_csv(\"../data/Iris.csv\")\ndata.head()\n</pre> data=pd.read_csv(\"../data/Iris.csv\") data.head() Out[3]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[4]: Copied! <pre>data['labels'] = data.Species.map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\ndata.head()\n</pre> data['labels'] = data.Species.map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}) data.head() Out[4]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species labels 0 1 5.1 3.5 1.4 0.2 Iris-setosa 0 1 2 4.9 3.0 1.4 0.2 Iris-setosa 0 2 3 4.7 3.2 1.3 0.2 Iris-setosa 0 3 4 4.6 3.1 1.5 0.2 Iris-setosa 0 4 5 5.0 3.6 1.4 0.2 Iris-setosa 0 In\u00a0[5]: Copied! <pre>plt.figure(figsize=(12,8))\nplt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=data['labels'])\n</pre> plt.figure(figsize=(12,8)) plt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=data['labels']) Out[5]: <pre>&lt;matplotlib.collections.PathCollection at 0x7fec905a8c20&gt;</pre> In\u00a0[6]: Copied! <pre>X=data[['PetalLengthCm','PetalWidthCm','PetalLengthCm','PetalWidthCm']]\nX = tensor(X)\nX\n</pre> X=data[['PetalLengthCm','PetalWidthCm','PetalLengthCm','PetalWidthCm']] X = tensor(X) X Out[6]: <pre>tensor([[1.4, 0.2, 1.4, 0.2],\n        [1.4, 0.2, 1.4, 0.2],\n        [1.3, 0.2, 1.3, 0.2],\n        [1.5, 0.2, 1.5, 0.2],\n        [1.4, 0.2, 1.4, 0.2],\n        [1.7, 0.4, 1.7, 0.4],\n        [1.4, 0.3, 1.4, 0.3],\n        [1.5, 0.2, 1.5, 0.2],\n        [1.4, 0.2, 1.4, 0.2],\n        [1.5, 0.1, 1.5, 0.1],\n        [1.5, 0.2, 1.5, 0.2],\n        [1.6, 0.2, 1.6, 0.2],\n        [1.4, 0.1, 1.4, 0.1],\n        [1.1, 0.1, 1.1, 0.1],\n        [1.2, 0.2, 1.2, 0.2],\n        [1.5, 0.4, 1.5, 0.4],\n        [1.3, 0.4, 1.3, 0.4],\n        [1.4, 0.3, 1.4, 0.3],\n        [1.7, 0.3, 1.7, 0.3],\n        [1.5, 0.3, 1.5, 0.3],\n        [1.7, 0.2, 1.7, 0.2],\n        [1.5, 0.4, 1.5, 0.4],\n        [1. , 0.2, 1. , 0.2],\n        [1.7, 0.5, 1.7, 0.5],\n        [1.9, 0.2, 1.9, 0.2],\n        [1.6, 0.2, 1.6, 0.2],\n        [1.6, 0.4, 1.6, 0.4],\n        [1.5, 0.2, 1.5, 0.2],\n        [1.4, 0.2, 1.4, 0.2],\n        [1.6, 0.2, 1.6, 0.2],\n        [1.6, 0.2, 1.6, 0.2],\n        [1.5, 0.4, 1.5, 0.4],\n        [1.5, 0.1, 1.5, 0.1],\n        [1.4, 0.2, 1.4, 0.2],\n        [1.5, 0.1, 1.5, 0.1],\n        [1.2, 0.2, 1.2, 0.2],\n        [1.3, 0.2, 1.3, 0.2],\n        [1.5, 0.1, 1.5, 0.1],\n        [1.3, 0.2, 1.3, 0.2],\n        [1.5, 0.2, 1.5, 0.2],\n        [1.3, 0.3, 1.3, 0.3],\n        [1.3, 0.3, 1.3, 0.3],\n        [1.3, 0.2, 1.3, 0.2],\n        [1.6, 0.6, 1.6, 0.6],\n        [1.9, 0.4, 1.9, 0.4],\n        [1.4, 0.3, 1.4, 0.3],\n        [1.6, 0.2, 1.6, 0.2],\n        [1.4, 0.2, 1.4, 0.2],\n        [1.5, 0.2, 1.5, 0.2],\n        [1.4, 0.2, 1.4, 0.2],\n        [4.7, 1.4, 4.7, 1.4],\n        [4.5, 1.5, 4.5, 1.5],\n        [4.9, 1.5, 4.9, 1.5],\n        [4. , 1.3, 4. , 1.3],\n        [4.6, 1.5, 4.6, 1.5],\n        [4.5, 1.3, 4.5, 1.3],\n        [4.7, 1.6, 4.7, 1.6],\n        [3.3, 1. , 3.3, 1. ],\n        [4.6, 1.3, 4.6, 1.3],\n        [3.9, 1.4, 3.9, 1.4],\n        [3.5, 1. , 3.5, 1. ],\n        [4.2, 1.5, 4.2, 1.5],\n        [4. , 1. , 4. , 1. ],\n        [4.7, 1.4, 4.7, 1.4],\n        [3.6, 1.3, 3.6, 1.3],\n        [4.4, 1.4, 4.4, 1.4],\n        [4.5, 1.5, 4.5, 1.5],\n        [4.1, 1. , 4.1, 1. ],\n        [4.5, 1.5, 4.5, 1.5],\n        [3.9, 1.1, 3.9, 1.1],\n        [4.8, 1.8, 4.8, 1.8],\n        [4. , 1.3, 4. , 1.3],\n        [4.9, 1.5, 4.9, 1.5],\n        [4.7, 1.2, 4.7, 1.2],\n        [4.3, 1.3, 4.3, 1.3],\n        [4.4, 1.4, 4.4, 1.4],\n        [4.8, 1.4, 4.8, 1.4],\n        [5. , 1.7, 5. , 1.7],\n        [4.5, 1.5, 4.5, 1.5],\n        [3.5, 1. , 3.5, 1. ],\n        [3.8, 1.1, 3.8, 1.1],\n        [3.7, 1. , 3.7, 1. ],\n        [3.9, 1.2, 3.9, 1.2],\n        [5.1, 1.6, 5.1, 1.6],\n        [4.5, 1.5, 4.5, 1.5],\n        [4.5, 1.6, 4.5, 1.6],\n        [4.7, 1.5, 4.7, 1.5],\n        [4.4, 1.3, 4.4, 1.3],\n        [4.1, 1.3, 4.1, 1.3],\n        [4. , 1.3, 4. , 1.3],\n        [4.4, 1.2, 4.4, 1.2],\n        [4.6, 1.4, 4.6, 1.4],\n        [4. , 1.2, 4. , 1.2],\n        [3.3, 1. , 3.3, 1. ],\n        [4.2, 1.3, 4.2, 1.3],\n        [4.2, 1.2, 4.2, 1.2],\n        [4.2, 1.3, 4.2, 1.3],\n        [4.3, 1.3, 4.3, 1.3],\n        [3. , 1.1, 3. , 1.1],\n        [4.1, 1.3, 4.1, 1.3],\n        [6. , 2.5, 6. , 2.5],\n        [5.1, 1.9, 5.1, 1.9],\n        [5.9, 2.1, 5.9, 2.1],\n        [5.6, 1.8, 5.6, 1.8],\n        [5.8, 2.2, 5.8, 2.2],\n        [6.6, 2.1, 6.6, 2.1],\n        [4.5, 1.7, 4.5, 1.7],\n        [6.3, 1.8, 6.3, 1.8],\n        [5.8, 1.8, 5.8, 1.8],\n        [6.1, 2.5, 6.1, 2.5],\n        [5.1, 2. , 5.1, 2. ],\n        [5.3, 1.9, 5.3, 1.9],\n        [5.5, 2.1, 5.5, 2.1],\n        [5. , 2. , 5. , 2. ],\n        [5.1, 2.4, 5.1, 2.4],\n        [5.3, 2.3, 5.3, 2.3],\n        [5.5, 1.8, 5.5, 1.8],\n        [6.7, 2.2, 6.7, 2.2],\n        [6.9, 2.3, 6.9, 2.3],\n        [5. , 1.5, 5. , 1.5],\n        [5.7, 2.3, 5.7, 2.3],\n        [4.9, 2. , 4.9, 2. ],\n        [6.7, 2. , 6.7, 2. ],\n        [4.9, 1.8, 4.9, 1.8],\n        [5.7, 2.1, 5.7, 2.1],\n        [6. , 1.8, 6. , 1.8],\n        [4.8, 1.8, 4.8, 1.8],\n        [4.9, 1.8, 4.9, 1.8],\n        [5.6, 2.1, 5.6, 2.1],\n        [5.8, 1.6, 5.8, 1.6],\n        [6.1, 1.9, 6.1, 1.9],\n        [6.4, 2. , 6.4, 2. ],\n        [5.6, 2.2, 5.6, 2.2],\n        [5.1, 1.5, 5.1, 1.5],\n        [5.6, 1.4, 5.6, 1.4],\n        [6.1, 2.3, 6.1, 2.3],\n        [5.6, 2.4, 5.6, 2.4],\n        [5.5, 1.8, 5.5, 1.8],\n        [4.8, 1.8, 4.8, 1.8],\n        [5.4, 2.1, 5.4, 2.1],\n        [5.6, 2.4, 5.6, 2.4],\n        [5.1, 2.3, 5.1, 2.3],\n        [5.1, 1.9, 5.1, 1.9],\n        [5.9, 2.3, 5.9, 2.3],\n        [5.7, 2.5, 5.7, 2.5],\n        [5.2, 2.3, 5.2, 2.3],\n        [5. , 1.9, 5. , 1.9],\n        [5.2, 2. , 5.2, 2. ],\n        [5.4, 2.3, 5.4, 2.3],\n        [5.1, 1.8, 5.1, 1.8]], dtype=sorix.float64)</pre> In\u00a0[7]: Copied! <pre>model= Kmeans(n_clusters=3)\nmodel.fit(X)\n</pre> model= Kmeans(n_clusters=3) model.fit(X) In\u00a0[8]: Copied! <pre>pred_labels = model.predict(X)\nplt.figure(figsize=(12,8))\nplt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=pred_labels)\n\nfor centroid in model.centroids:\n    plt.scatter(centroid[0],centroid[1], s=150)\n</pre> pred_labels = model.predict(X) plt.figure(figsize=(12,8)) plt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=pred_labels)  for centroid in model.centroids:     plt.scatter(centroid[0],centroid[1], s=150) In\u00a0[9]: Copied! <pre>pred_labels = model.predict(X)\nplt.figure(figsize=(12,8))\nplt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=pred_labels)\n\nfor centroid in model.centroids:\n    plt.scatter(centroid[0],centroid[1], s=150)\n</pre> pred_labels = model.predict(X) plt.figure(figsize=(12,8)) plt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],s=80, c=pred_labels)  for centroid in model.centroids:     plt.scatter(centroid[0],centroid[1], s=150) In\u00a0[10]: Copied! <pre>inertias = []\n\nfor i in range(2,11):\n    model = Kmeans(n_clusters=i)\n    model.fit(X)\n    inertias.append(model.get_inertia(X))\n\nimport matplotlib.pyplot as plt\nplt.plot(range(2,11), inertias, marker='o')\n</pre> inertias = []  for i in range(2,11):     model = Kmeans(n_clusters=i)     model.fit(X)     inertias.append(model.get_inertia(X))  import matplotlib.pyplot as plt plt.plot(range(2,11), inertias, marker='o') Out[10]: <pre>[&lt;matplotlib.lines.Line2D at 0x7fec8fda1a90&gt;]</pre>"},{"location":"examples/clustering/Iris-dataset/#iris-dataset","title":"Iris Dataset\u00b6","text":""},{"location":"examples/clustering/RandomDataset%2BGPU/","title":"Random Dataset + GPU","text":"In\u00a0[1]: Copied! <pre>#Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> #Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport sorix\nfrom sorix.clustering import Kmeans\n</pre> import numpy as np import matplotlib.pyplot as plt import sorix from sorix.clustering import Kmeans In\u00a0[3]: Copied! <pre>device = 'cuda' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'cuda' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 14.0.1\n</pre> Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre># Semilla para reproducibilidad\nnp.random.seed(42)\n\n# Par\u00e1metros\nn_clusters = 3\nsamples_per_cluster = 1000000\n\n# Centros de los cl\u00fasteres (puedes cambiarlos)\ncentros = np.array([\n    [2, 2],\n    [8, 3],\n    [3, 8]\n])\n\n# Desviaci\u00f3n est\u00e1ndar de los cl\u00fasteres\nstd = 0.8\n\n# Genera los puntos\nX = []\ny = []\nfor label, centro in enumerate(centros):\n    puntos = np.random.randn(samples_per_cluster, 2) * std + centro\n    X.append(puntos)\n    y.append(np.full(samples_per_cluster, label))\n\n# Unimos todos los puntos y etiquetas\nX = np.vstack(X)\ny = np.concatenate(y)\n\n# Visualizaci\u00f3n r\u00e1pida\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"Set1\", s=40, edgecolor='k')\nplt.title(\"Dataset sint\u00e9tico para clustering\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.show()\n</pre> # Semilla para reproducibilidad np.random.seed(42)  # Par\u00e1metros n_clusters = 3 samples_per_cluster = 1000000  # Centros de los cl\u00fasteres (puedes cambiarlos) centros = np.array([     [2, 2],     [8, 3],     [3, 8] ])  # Desviaci\u00f3n est\u00e1ndar de los cl\u00fasteres std = 0.8  # Genera los puntos X = [] y = [] for label, centro in enumerate(centros):     puntos = np.random.randn(samples_per_cluster, 2) * std + centro     X.append(puntos)     y.append(np.full(samples_per_cluster, label))  # Unimos todos los puntos y etiquetas X = np.vstack(X) y = np.concatenate(y)  # Visualizaci\u00f3n r\u00e1pida plt.figure(figsize=(6, 6)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"Set1\", s=40, edgecolor='k') plt.title(\"Dataset sint\u00e9tico para clustering\") plt.xlabel(\"X1\") plt.ylabel(\"X2\") plt.show() In\u00a0[5]: Copied! <pre>X_train = sorix.tensor(X, device=device)\nX_train\n</pre> X_train = sorix.tensor(X, device=device) X_train Out[5]: <pre>tensor([[2.39737132, 1.88938856],\n        [2.51815083, 3.21842389],\n        [1.8126773 , 1.81269043],\n        ...,\n        [3.56752298, 8.4546417 ],\n        [2.8109625 , 7.1025456 ],\n        [4.49822372, 7.9792028 ]], device='cuda:0', dtype=sorix.float64)</pre> In\u00a0[6]: Copied! <pre>kmeans = Kmeans(n_clusters=n_clusters)\nkmeans.fit(X_train)\n</pre> kmeans = Kmeans(n_clusters=n_clusters) kmeans.fit(X_train)"},{"location":"examples/clustering/RandomDataset%2BGPU/#random-dataset-gpu","title":"Random Dataset + GPU\u00b6","text":""},{"location":"examples/nn/1-regression/","title":"Regression","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport joblib\n\nimport sorix\nfrom sorix.nn import ReLU,Linear\nfrom sorix.optim import RMSprop\nfrom sorix import tensor,Tensor\nfrom sorix.nn import Module\nfrom sorix.nn import  MSELoss\nfrom sorix.preprocessing import MinMaxScaler\nfrom sorix.model_selection import train_test_split\nfrom sorix.metrics import r2_score\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import joblib  import sorix from sorix.nn import ReLU,Linear from sorix.optim import RMSprop from sorix import tensor,Tensor from sorix.nn import Module from sorix.nn import  MSELoss from sorix.preprocessing import MinMaxScaler from sorix.model_selection import train_test_split from sorix.metrics import r2_score In\u00a0[3]: Copied! <pre>device  = 'cuda' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device  = 'cuda' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 14.0.1\n</pre> Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre># Datos\npoints = 10000\nx1 = np.linspace(1, 20*np.pi, points)\nx2 = np.linspace(1, 20*np.pi, points)\n\n# Definimos la salida\ny = 20*np.log(x1+1) + -1*x2 + 3*np.random.randn(points)\n\n\ndata = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\ndata.head()\n\nplt.figure(figsize=(20,7))\nplt.scatter(data['x2'], data['y'], s=20)\nplt.show()\n</pre> # Datos points = 10000 x1 = np.linspace(1, 20*np.pi, points) x2 = np.linspace(1, 20*np.pi, points)  # Definimos la salida y = 20*np.log(x1+1) + -1*x2 + 3*np.random.randn(points)   data = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y}) data.head()  plt.figure(figsize=(20,7)) plt.scatter(data['x2'], data['y'], s=20) plt.show()  In\u00a0[5]: Copied! <pre>independent_variables = ['x1', 'x2']\ndependent_variable = ['y']\n</pre> independent_variables = ['x1', 'x2'] dependent_variable = ['y'] In\u00a0[6]: Copied! <pre># Train test split and normalization\ndf_train, df_test = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n\nX_train = df_train[independent_variables]\nY_train = df_train[dependent_variable]\n\nX_test = df_test[independent_variables]\nY_test = df_test[dependent_variable]\n\nX_train\n</pre> # Train test split and normalization df_train, df_test = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)  X_train = df_train[independent_variables] Y_train = df_train[dependent_variable]  X_test = df_test[independent_variables] Y_test = df_test[dependent_variable]  X_train Out[6]: x1 x2 6252 39.661141 39.661141 4684 29.964936 29.964936 1731 11.704164 11.704164 4742 30.323597 30.323597 4521 28.956976 28.956976 ... ... ... 1638 11.129070 11.129070 5891 37.428788 37.428788 7427 46.927110 46.927110 608 4.759753 4.759753 6907 43.711532 43.711532 <p>8000 rows \u00d7 2 columns</p> In\u00a0[7]: Copied! <pre>scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train\n</pre> scaler = MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) X_train Out[7]: <pre>array([[0.62526253, 0.62526253],\n       [0.46844684, 0.46844684],\n       [0.17311731, 0.17311731],\n       ...,\n       [0.74277428, 0.74277428],\n       [0.06080608, 0.06080608],\n       [0.69076908, 0.69076908]], shape=(8000, 2))</pre> In\u00a0[8]: Copied! <pre>plt.Figure(figsize=(20,7))\nplt.scatter(X_train[:,0], Y_train)\nplt.show()\n</pre> plt.Figure(figsize=(20,7)) plt.scatter(X_train[:,0], Y_train) plt.show() In\u00a0[9]: Copied! <pre>X_train = tensor(X_train,dtype=sorix.float32,device=device)\nY_train = tensor(Y_train,dtype=sorix.float32,device=device)\n\nX_test = tensor(X_test, dtype=sorix.float32,device=device)\nY_test = tensor(Y_test, dtype=sorix.float32,device=device)\n\nprint(f\"X_train shape: {X_train.shape}, device: {X_train.device}\")\nprint(f\"Y_train shape: {Y_train.shape}. device: {Y_train.device}\")\nprint(f\"X_test shape: {X_test.shape}, device: {X_test.device}\")\nprint(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\")\n</pre> X_train = tensor(X_train,dtype=sorix.float32,device=device) Y_train = tensor(Y_train,dtype=sorix.float32,device=device)  X_test = tensor(X_test, dtype=sorix.float32,device=device) Y_test = tensor(Y_test, dtype=sorix.float32,device=device)  print(f\"X_train shape: {X_train.shape}, device: {X_train.device}\") print(f\"Y_train shape: {Y_train.shape}. device: {Y_train.device}\") print(f\"X_test shape: {X_test.shape}, device: {X_test.device}\") print(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\") <pre>X_train shape: sorix.Size([8000, 2]), device: cuda:0\nY_train shape: sorix.Size([8000, 1]). device: cuda:0\nX_test shape: sorix.Size([2000, 2]), device: cuda:0\nY_test shape: sorix.Size([2000, 1]), device: cuda:0\n</pre> In\u00a0[10]: Copied! <pre>class Network(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = Linear(2, 64)\n        self.fc2 = Linear(64, 32)\n        self.fc3 = Linear(32, 1)\n        self.relu = ReLU()\n\n    def forward(self, x: tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n    \n\nnet = Network()\nnet.to(device)\nnet\n</pre> class Network(Module):     def __init__(self):         super().__init__()         self.fc1 = Linear(2, 64)         self.fc2 = Linear(64, 32)         self.fc3 = Linear(32, 1)         self.relu = ReLU()      def forward(self, x: tensor) -&gt; Tensor:         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         x = self.relu(x)         x = self.fc3(x)         return x       net = Network() net.to(device) net Out[10]: <pre>Network(\n  (fc1): Linear(in_features=2, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=32, bias=True)\n  (fc3): Linear(in_features=32, out_features=1, bias=True)\n  (relu): ReLU()\n)</pre> In\u00a0[11]: Copied! <pre>criterion = MSELoss()\noptimizer = RMSprop(net.parameters(), lr=1e-2)\n</pre> criterion = MSELoss() optimizer = RMSprop(net.parameters(), lr=1e-2) In\u00a0[12]: Copied! <pre>Y_pred = net(X_train)\nY_pred\n</pre> Y_pred = net(X_train) Y_pred Out[12]: <pre>tensor([[0.6408244 ],\n        [0.48010576],\n        [0.17742594],\n        ...,\n        [0.7612608 ],\n        [0.06231946],\n        [0.7079614 ]], device='cuda:0', requires_grad=True)</pre> In\u00a0[13]: Copied! <pre># %%\n# Bucle de entrenamiento mejorado\nfor epoch in range(1000 + 1):\n    Y_pred = net(X_train)\n    loss = criterion(Y_train,Y_pred)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        r2_train = r2_score(Y_pred, Y_train)\n        with sorix.no_grad():\n            Y_pred = net(X_test)\n            r2_test = r2_score(Y_test, Y_pred)\n        print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | R2_Train: {100*r2_train:5.2f} % | R2_Test: {100*r2_test:5.2f} %\")\n        if r2_test &gt; 0.95:  # Mejoramos el criterio de parada\n            print(f\"Entrenamiento completado en {epoch} epochs!\")\n            break\n</pre> # %% # Bucle de entrenamiento mejorado for epoch in range(1000 + 1):     Y_pred = net(X_train)     loss = criterion(Y_train,Y_pred)      optimizer.zero_grad()     loss.backward()     optimizer.step()      if epoch % 10 == 0:         r2_train = r2_score(Y_pred, Y_train)         with sorix.no_grad():             Y_pred = net(X_test)             r2_test = r2_score(Y_test, Y_pred)         print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | R2_Train: {100*r2_train:5.2f} % | R2_Test: {100*r2_test:5.2f} %\")         if r2_test &gt; 0.95:  # Mejoramos el criterio de parada             print(f\"Entrenamiento completado en {epoch} epochs!\")             break <pre>[cuda] Epoch     0 | Loss: 1140.3505 | R2_Train: -1301600.20 % | R2_Test: -1808.52 %\n[cuda] Epoch    10 | Loss: 329.2321 | R2_Train: -140.60 % | R2_Test: -520.02 %\n[cuda] Epoch    20 | Loss: 270.4900 | R2_Train: -139.12 % | R2_Test: -406.77 %\n[cuda] Epoch    30 | Loss: 189.4035 | R2_Train: -186.28 % | R2_Test: -246.83 %\n[cuda] Epoch    40 | Loss: 102.1791 | R2_Train: -384.18 % | R2_Test: -84.71 %\n[cuda] Epoch    50 | Loss: 52.9447 | R2_Train: -15675.98 % | R2_Test:  0.28 %\n[cuda] Epoch    60 | Loss: 42.1484 | R2_Train: -1226.10 % | R2_Test: 20.00 %\n[cuda] Epoch    70 | Loss: 39.2625 | R2_Train: -468.39 % | R2_Test: 24.57 %\n</pre> <pre>[cuda] Epoch    80 | Loss: 37.2873 | R2_Train: -336.57 % | R2_Test: 28.46 %\n[cuda] Epoch    90 | Loss: 35.8168 | R2_Train: -287.79 % | R2_Test: 31.43 %\n[cuda] Epoch   100 | Loss: 34.3993 | R2_Train: -252.45 % | R2_Test: 34.08 %\n[cuda] Epoch   110 | Loss: 32.8270 | R2_Train: -216.87 % | R2_Test: 37.00 %\n[cuda] Epoch   120 | Loss: 31.1703 | R2_Train: -181.94 % | R2_Test: 40.16 %\n</pre> <pre>[cuda] Epoch   130 | Loss: 29.5663 | R2_Train: -149.00 % | R2_Test: 43.22 %\n[cuda] Epoch   140 | Loss: 28.1988 | R2_Train: -119.26 % | R2_Test: 45.83 %\n[cuda] Epoch   150 | Loss: 28.3124 | R2_Train: -109.88 % | R2_Test: 45.91 %\n[cuda] Epoch   160 | Loss: 24.9430 | R2_Train: -60.24 % | R2_Test: 50.76 %\n[cuda] Epoch   170 | Loss: 22.6962 | R2_Train: -30.16 % | R2_Test: 55.90 %\n[cuda] Epoch   180 | Loss: 21.7877 | R2_Train: -13.72 % | R2_Test: 57.92 %\n[cuda] Epoch   190 | Loss: 20.8847 | R2_Train:  0.87 % | R2_Test: 59.29 %\n[cuda] Epoch   200 | Loss: 19.7388 | R2_Train: 14.52 % | R2_Test: 61.13 %\n[cuda] Epoch   210 | Loss: 18.7742 | R2_Train: 24.76 % | R2_Test: 62.83 %\n</pre> <pre>[cuda] Epoch   220 | Loss: 18.0425 | R2_Train: 32.12 % | R2_Test: 64.06 %\n[cuda] Epoch   230 | Loss: 17.4062 | R2_Train: 37.91 % | R2_Test: 65.05 %\n[cuda] Epoch   240 | Loss: 16.9284 | R2_Train: 42.08 % | R2_Test: 65.76 %\n[cuda] Epoch   250 | Loss: 16.4566 | R2_Train: 45.67 % | R2_Test: 66.44 %\n[cuda] Epoch   260 | Loss: 16.0859 | R2_Train: 48.40 % | R2_Test: 67.07 %\n</pre> <pre>[cuda] Epoch   270 | Loss: 15.8444 | R2_Train: 50.27 % | R2_Test: 67.52 %\n[cuda] Epoch   280 | Loss: 15.6485 | R2_Train: 51.71 % | R2_Test: 67.89 %\n[cuda] Epoch   290 | Loss: 15.4640 | R2_Train: 52.94 % | R2_Test: 68.25 %\n[cuda] Epoch   300 | Loss: 16.7597 | R2_Train: 48.79 % | R2_Test: 65.90 %\n[cuda] Epoch   310 | Loss: 15.3121 | R2_Train: 54.54 % | R2_Test: 67.73 %\n[cuda] Epoch   320 | Loss: 14.5684 | R2_Train: 57.22 % | R2_Test: 69.98 %\n[cuda] Epoch   330 | Loss: 14.7618 | R2_Train: 56.67 % | R2_Test: 69.96 %\n[cuda] Epoch   340 | Loss: 14.9383 | R2_Train: 56.28 % | R2_Test: 69.41 %\n[cuda] Epoch   350 | Loss: 14.7426 | R2_Train: 57.17 % | R2_Test: 69.67 %\n</pre> <pre>[cuda] Epoch   360 | Loss: 14.5411 | R2_Train: 58.02 % | R2_Test: 70.14 %\n[cuda] Epoch   370 | Loss: 14.4747 | R2_Train: 58.39 % | R2_Test: 70.36 %\n[cuda] Epoch   380 | Loss: 14.4389 | R2_Train: 58.64 % | R2_Test: 70.43 %\n[cuda] Epoch   390 | Loss: 14.3542 | R2_Train: 59.08 % | R2_Test: 70.59 %\n[cuda] Epoch   400 | Loss: 14.2599 | R2_Train: 59.54 % | R2_Test: 70.82 %\n</pre> <pre>[cuda] Epoch   410 | Loss: 14.1949 | R2_Train: 59.85 % | R2_Test: 71.00 %\n[cuda] Epoch   420 | Loss: 14.1371 | R2_Train: 60.17 % | R2_Test: 71.14 %\n[cuda] Epoch   430 | Loss: 14.0837 | R2_Train: 60.45 % | R2_Test: 71.28 %\n[cuda] Epoch   440 | Loss: 14.0214 | R2_Train: 60.77 % | R2_Test: 71.42 %\n[cuda] Epoch   450 | Loss: 13.9632 | R2_Train: 61.06 % | R2_Test: 71.57 %\n[cuda] Epoch   460 | Loss: 13.9193 | R2_Train: 61.31 % | R2_Test: 71.69 %\n[cuda] Epoch   470 | Loss: 13.8704 | R2_Train: 61.54 % | R2_Test: 71.81 %\n[cuda] Epoch   480 | Loss: 13.8264 | R2_Train: 61.76 % | R2_Test: 71.91 %\n[cuda] Epoch   490 | Loss: 13.7706 | R2_Train: 62.01 % | R2_Test: 72.05 %\n</pre> <pre>[cuda] Epoch   500 | Loss: 13.7293 | R2_Train: 62.20 % | R2_Test: 72.15 %\n[cuda] Epoch   510 | Loss: 13.6898 | R2_Train: 62.39 % | R2_Test: 72.24 %\n[cuda] Epoch   520 | Loss: 13.6373 | R2_Train: 62.61 % | R2_Test: 72.36 %\n[cuda] Epoch   530 | Loss: 13.5982 | R2_Train: 62.80 % | R2_Test: 72.45 %\n[cuda] Epoch   540 | Loss: 13.5573 | R2_Train: 62.98 % | R2_Test: 72.55 %\n</pre> <pre>[cuda] Epoch   550 | Loss: 13.5242 | R2_Train: 63.14 % | R2_Test: 72.62 %\n[cuda] Epoch   560 | Loss: 13.4681 | R2_Train: 63.36 % | R2_Test: 72.75 %\n[cuda] Epoch   570 | Loss: 13.4330 | R2_Train: 63.52 % | R2_Test: 72.84 %\n[cuda] Epoch   580 | Loss: 13.4033 | R2_Train: 63.68 % | R2_Test: 72.91 %\n[cuda] Epoch   590 | Loss: 13.3600 | R2_Train: 63.87 % | R2_Test: 72.99 %\n[cuda] Epoch   600 | Loss: 13.3088 | R2_Train: 64.08 % | R2_Test: 73.11 %\n[cuda] Epoch   610 | Loss: 13.2809 | R2_Train: 64.24 % | R2_Test: 73.16 %\n[cuda] Epoch   620 | Loss: 13.2180 | R2_Train: 64.52 % | R2_Test: 73.25 %\n[cuda] Epoch   630 | Loss: 13.1996 | R2_Train: 64.64 % | R2_Test: 73.31 %\n</pre> <pre>[cuda] Epoch   640 | Loss: 13.1724 | R2_Train: 64.75 % | R2_Test: 73.37 %\n[cuda] Epoch   650 | Loss: 13.1443 | R2_Train: 64.86 % | R2_Test: 73.45 %\n[cuda] Epoch   660 | Loss: 13.1142 | R2_Train: 64.97 % | R2_Test: 73.53 %\n[cuda] Epoch   670 | Loss: 13.0863 | R2_Train: 65.07 % | R2_Test: 73.60 %\n[cuda] Epoch   680 | Loss: 13.0599 | R2_Train: 65.18 % | R2_Test: 73.66 %\n</pre> <pre>[cuda] Epoch   690 | Loss: 13.0353 | R2_Train: 65.28 % | R2_Test: 73.73 %\n[cuda] Epoch   700 | Loss: 13.0109 | R2_Train: 65.36 % | R2_Test: 73.78 %\n[cuda] Epoch   710 | Loss: 12.9826 | R2_Train: 65.48 % | R2_Test: 73.85 %\n[cuda] Epoch   720 | Loss: 12.9559 | R2_Train: 65.56 % | R2_Test: 73.92 %\n[cuda] Epoch   730 | Loss: 12.9085 | R2_Train: 65.74 % | R2_Test: 74.03 %\n[cuda] Epoch   740 | Loss: 12.9104 | R2_Train: 65.74 % | R2_Test: 74.04 %\n[cuda] Epoch   750 | Loss: 12.8943 | R2_Train: 65.80 % | R2_Test: 74.05 %\n[cuda] Epoch   760 | Loss: 12.8698 | R2_Train: 65.90 % | R2_Test: 74.12 %\n[cuda] Epoch   770 | Loss: 12.8484 | R2_Train: 65.97 % | R2_Test: 74.17 %\n</pre> <pre>[cuda] Epoch   780 | Loss: 12.8317 | R2_Train: 66.02 % | R2_Test: 74.21 %\n[cuda] Epoch   790 | Loss: 12.8202 | R2_Train: 66.07 % | R2_Test: 74.25 %\n[cuda] Epoch   800 | Loss: 12.7981 | R2_Train: 66.16 % | R2_Test: 74.30 %\n[cuda] Epoch   810 | Loss: 12.7800 | R2_Train: 66.21 % | R2_Test: 74.34 %\n[cuda] Epoch   820 | Loss: 12.7647 | R2_Train: 66.25 % | R2_Test: 74.37 %\n</pre> <pre>[cuda] Epoch   830 | Loss: 12.7430 | R2_Train: 66.35 % | R2_Test: 74.42 %\n[cuda] Epoch   840 | Loss: 12.6583 | R2_Train: 66.70 % | R2_Test: 74.57 %\n[cuda] Epoch   850 | Loss: 12.6467 | R2_Train: 66.76 % | R2_Test: 74.64 %\n[cuda] Epoch   860 | Loss: 12.6798 | R2_Train: 66.75 % | R2_Test: 74.53 %\n[cuda] Epoch   870 | Loss: 12.5997 | R2_Train: 67.00 % | R2_Test: 74.68 %\n[cuda] Epoch   880 | Loss: 12.5792 | R2_Train: 67.06 % | R2_Test: 74.76 %\n[cuda] Epoch   890 | Loss: 12.5866 | R2_Train: 67.05 % | R2_Test: 74.76 %\n[cuda] Epoch   900 | Loss: 12.5753 | R2_Train: 67.09 % | R2_Test: 74.77 %\n[cuda] Epoch   910 | Loss: 12.5533 | R2_Train: 67.15 % | R2_Test: 74.82 %\n</pre> <pre>[cuda] Epoch   920 | Loss: 12.5379 | R2_Train: 67.19 % | R2_Test: 74.86 %\n[cuda] Epoch   930 | Loss: 12.5196 | R2_Train: 67.25 % | R2_Test: 74.91 %\n[cuda] Epoch   940 | Loss: 12.4996 | R2_Train: 67.31 % | R2_Test: 74.96 %\n[cuda] Epoch   950 | Loss: 12.4878 | R2_Train: 67.34 % | R2_Test: 74.99 %\n[cuda] Epoch   960 | Loss: 12.4735 | R2_Train: 67.39 % | R2_Test: 75.01 %\n</pre> <pre>[cuda] Epoch   970 | Loss: 12.4563 | R2_Train: 67.44 % | R2_Test: 75.05 %\n[cuda] Epoch   980 | Loss: 12.4381 | R2_Train: 67.49 % | R2_Test: 75.10 %\n[cuda] Epoch   990 | Loss: 12.4216 | R2_Train: 67.54 % | R2_Test: 75.14 %\n[cuda] Epoch  1000 | Loss: 12.4131 | R2_Train: 67.57 % | R2_Test: 75.17 %\n</pre> In\u00a0[14]: Copied! <pre># %%\n# Visualizaci\u00f3n final\n\nwith sorix.no_grad():\n    y_pred = net(X_test)\n\nr2 = r2_score(Y_test, y_pred)\n\nplt.figure(figsize=(20,8))\nplt.scatter(X_test[:,0],Y_test,s=50)\nplt.scatter(X_test[:,0],y_pred)\nplt.title(f'Polinomic Regression on Test Data(Accuracy:{r2*100:.2f}%)')\n</pre> # %% # Visualizaci\u00f3n final  with sorix.no_grad():     y_pred = net(X_test)  r2 = r2_score(Y_test, y_pred)  plt.figure(figsize=(20,8)) plt.scatter(X_test[:,0],Y_test,s=50) plt.scatter(X_test[:,0],y_pred) plt.title(f'Polinomic Regression on Test Data(Accuracy:{r2*100:.2f}%)') Out[14]: <pre>Text(0.5, 1.0, 'Polinomic Regression on Test Data(Accuracy:75.17%)')</pre> In\u00a0[15]: Copied! <pre>sorix.save(net.state_dict(),\"regression_weights.sor\")\n</pre> sorix.save(net.state_dict(),\"regression_weights.sor\") In\u00a0[16]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"regression_weights.sor\"))\n\nif X_test.device == 'cpu':\n    out = net2(X_test)\nif X_test.device == 'cuda':\n    out = net2(X_test.to('cpu'))\n\nout\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"regression_weights.sor\"))  if X_test.device == 'cpu':     out = net2(X_test) if X_test.device == 'cuda':     out = net2(X_test.to('cpu'))  out Out[16]: <pre>tensor([[23.802635],\n        [28.743082],\n        [31.988668],\n        ...,\n        [35.09604 ],\n        [31.792273],\n        [28.65684 ]], requires_grad=True)</pre> In\u00a0[17]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"regression_weights.sor\"))\nnet2.to('cuda')\n\nif X_test.device == 'cpu':\n    with sorix.no_grad():\n        out = net2(X_test.to('cuda'))\nif X_test.device == 'cuda':\n    with sorix.no_grad():\n        out = net2(X_test)\n\nout\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"regression_weights.sor\")) net2.to('cuda')  if X_test.device == 'cpu':     with sorix.no_grad():         out = net2(X_test.to('cuda')) if X_test.device == 'cuda':     with sorix.no_grad():         out = net2(X_test)  out Out[17]: <pre>tensor([[23.802631],\n        [28.743082],\n        [31.988668],\n        ...,\n        [35.096043],\n        [31.792273],\n        [28.65684 ]], device='cuda:0')</pre>"},{"location":"examples/nn/1-regression/#regression","title":"Regression\u00b6","text":""},{"location":"examples/nn/1-regression/#datos","title":"Datos\u00b6","text":""},{"location":"examples/nn/1-regression/#preprocessing","title":"Preprocessing\u00b6","text":""},{"location":"examples/nn/1-regression/#save-and-load-model","title":"Save and Load Model\u00b6","text":""},{"location":"examples/nn/1-regression/#cpu","title":"CPU\u00b6","text":""},{"location":"examples/nn/1-regression/#gpu","title":"GPU\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/","title":"Classification Binary","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import joblib\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\nimport sorix\nfrom sorix.nn import ReLU,Linear,Sigmoid\nfrom sorix.optim import SGDMomentum,RMSprop,Adam,SGD\nfrom sorix import tensor,Tensor\nfrom sorix.nn import Module\nfrom sorix.nn import BCEWithLogitsLoss\nfrom sorix.metrics import confusion_matrix,classification_report\nfrom sorix.model_selection import train_test_split\n</pre> import joblib import seaborn as sns import numpy as np import pandas as pd import matplotlib.pyplot as plt   import sorix from sorix.nn import ReLU,Linear,Sigmoid from sorix.optim import SGDMomentum,RMSprop,Adam,SGD from sorix import tensor,Tensor from sorix.nn import Module from sorix.nn import BCEWithLogitsLoss from sorix.metrics import confusion_matrix,classification_report from sorix.model_selection import train_test_split In\u00a0[3]: Copied! <pre>device = 'cuda' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'cuda' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 14.0.1\n</pre> Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre>r1 = 0.5  \nr2 = 1.5  \n\nnum_points = 10000\nthetas = np.linspace(0, 2 * np.pi, num_points) \n\n\nx1 = r1 * np.cos(thetas) + 0.1 * np.random.randn(num_points)\ny1 = r1 * np.sin(thetas) + 0.1 * np.random.randn(num_points)\n\n\nx2 = r2 * np.cos(thetas) + 0.1 * np.random.randn(num_points)\ny2 = r2 * np.sin(thetas) + 0.1 * np.random.randn(num_points)\n\n\n\nplt.figure(figsize=(8, 7)) \n\n\nplt.scatter(x1, y1, s=50, label=\"Clase A\", alpha=0.8) # Puntos de la Clase A\nplt.scatter(x2, y2, s=50, label=\"Clase B\", alpha=0.8) # Puntos de la Clase B\n\nplt.xlabel(\"Caracter\u00edstica X\") \nplt.ylabel(\"Caracter\u00edstica Y\")\nplt.title(\"Dataset de C\u00edrculos con 3 Clases\")\nplt.legend() \nplt.grid(True, linestyle='--', alpha=0.6)\nplt.axis('equal')\nplt.show() \n</pre> r1 = 0.5   r2 = 1.5    num_points = 10000 thetas = np.linspace(0, 2 * np.pi, num_points)    x1 = r1 * np.cos(thetas) + 0.1 * np.random.randn(num_points) y1 = r1 * np.sin(thetas) + 0.1 * np.random.randn(num_points)   x2 = r2 * np.cos(thetas) + 0.1 * np.random.randn(num_points) y2 = r2 * np.sin(thetas) + 0.1 * np.random.randn(num_points)    plt.figure(figsize=(8, 7))    plt.scatter(x1, y1, s=50, label=\"Clase A\", alpha=0.8) # Puntos de la Clase A plt.scatter(x2, y2, s=50, label=\"Clase B\", alpha=0.8) # Puntos de la Clase B  plt.xlabel(\"Caracter\u00edstica X\")  plt.ylabel(\"Caracter\u00edstica Y\") plt.title(\"Dataset de C\u00edrculos con 3 Clases\") plt.legend()  plt.grid(True, linestyle='--', alpha=0.6) plt.axis('equal') plt.show()   In\u00a0[5]: Copied! <pre>df = pd.DataFrame(\n    {\"x\": x1.tolist()+x2.tolist(),\n    \"y\": y1.tolist()+ y2.tolist(),\n    \"labels\":['A' for _ in range(num_points)]+['B' for _ in range(num_points)]\n    })\n\nlabels = df[\"labels\"].unique()\nlabels2id = {label: i for i, label in enumerate(labels)}\nid2labels = {v: k for k, v in labels2id.items()}\n\ndf[\"labels\"] = df[\"labels\"].map(labels2id)\ndf.head()\n</pre> df = pd.DataFrame(     {\"x\": x1.tolist()+x2.tolist(),     \"y\": y1.tolist()+ y2.tolist(),     \"labels\":['A' for _ in range(num_points)]+['B' for _ in range(num_points)]     })  labels = df[\"labels\"].unique() labels2id = {label: i for i, label in enumerate(labels)} id2labels = {v: k for k, v in labels2id.items()}  df[\"labels\"] = df[\"labels\"].map(labels2id) df.head() Out[5]: x y labels 0 0.552374 -0.033578 0 1 0.511339 -0.026764 0 2 0.455849 0.028676 0 3 0.477596 -0.182449 0 4 0.425940 0.165002 0 In\u00a0[6]: Copied! <pre>df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n\nX_train = tensor(df_train[[\"x\",\"y\"]].values).to(device)\nY_train = tensor(df_train[[\"labels\"]].values).to(device)\n\nX_test = tensor(df_test[[\"x\",\"y\"]].values).to(device)\nY_test = tensor(df_test[[\"labels\"]].values).to(device)\n\n\nprint(f\"X_train shape: {X_train.shape}, device: {X_train.device}\")\nprint(f\"Y_train shape: {Y_train.shape}, device: {Y_train.device}\")\nprint(f\"X_test shape: {X_test.shape}, device: {X_test.device}\")\nprint(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\")\n</pre> df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)   X_train = tensor(df_train[[\"x\",\"y\"]].values).to(device) Y_train = tensor(df_train[[\"labels\"]].values).to(device)  X_test = tensor(df_test[[\"x\",\"y\"]].values).to(device) Y_test = tensor(df_test[[\"labels\"]].values).to(device)   print(f\"X_train shape: {X_train.shape}, device: {X_train.device}\") print(f\"Y_train shape: {Y_train.shape}, device: {Y_train.device}\") print(f\"X_test shape: {X_test.shape}, device: {X_test.device}\") print(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\")  <pre>X_train shape: sorix.Size([16000, 2]), device: cuda:0\nY_train shape: sorix.Size([16000, 1]), device: cuda:0\nX_test shape: sorix.Size([4000, 2]), device: cuda:0\nY_test shape: sorix.Size([4000, 1]), device: cuda:0\n</pre> In\u00a0[7]: Copied! <pre>class Network(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = Linear(2, 4)\n        self.relu = ReLU()\n        self.fc2 = Linear(4, 1)\n\n    def forward(self, x: tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n\nnet = Network().to(device)\nnet.parameters()\n</pre> class Network(Module):     def __init__(self):         super().__init__()         self.fc1 = Linear(2, 4)         self.relu = ReLU()         self.fc2 = Linear(4, 1)      def forward(self, x: tensor) -&gt; Tensor:         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         return x   net = Network().to(device) net.parameters() Out[7]: <pre>[tensor([[-0.17466629, -1.1154755 , -0.40821883, -0.16068657],\n         [-1.3185993 ,  1.0869352 ,  0.5690863 , -1.0485913 ]], device='cuda:0', requires_grad=True),\n tensor([[0., 0., 0., 0.]], device='cuda:0', requires_grad=True),\n tensor([[-1.2580953 ],\n         [ 0.507598  ],\n         [-1.1620189 ],\n         [ 0.05934924]], device='cuda:0', requires_grad=True),\n tensor([[0.]], device='cuda:0', requires_grad=True)]</pre> In\u00a0[8]: Copied! <pre>criterion = BCEWithLogitsLoss()\noptimizer = RMSprop(net.parameters(), lr=1e-2)\n</pre> criterion = BCEWithLogitsLoss() optimizer = RMSprop(net.parameters(), lr=1e-2) In\u00a0[9]: Copied! <pre>logits = net(X_train)\nlogits\n</pre> logits = net(X_train) logits Out[9]: <pre>tensor([[ 0.        ],\n        [-0.07914103],\n        [-0.72190042],\n        ...,\n        [ 0.        ],\n        [-2.27416639],\n        [-0.38330035]], device='cuda:0', dtype=sorix.float64, requires_grad=True)</pre> In\u00a0[10]: Copied! <pre># Bucle de entrenamiento mejorado\nfor epoch in range(1000 + 1):\n    logits = net(X_train)\n    loss = criterion(logits, Y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        probs = sorix.sigmoid(logits)\n        preds = (probs &gt; 0.5).astype('uint8')\n        acc_train = (preds == Y_train).mean()\n        with sorix.no_grad():\n            logits = net(X_test)\n            probs = sorix.sigmoid(logits)\n            preds = (probs &gt; 0.5).astype('uint8')\n            acc_test = (preds == Y_test).mean()\n\n        # Usamos una f-string para formatear y alinear la salida\n        print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")\n\n        if acc_test.item() &gt; 0.98:\n            break\n</pre> # Bucle de entrenamiento mejorado for epoch in range(1000 + 1):     logits = net(X_train)     loss = criterion(logits, Y_train)      optimizer.zero_grad()     loss.backward()     optimizer.step()      if epoch % 10 == 0:         probs = sorix.sigmoid(logits)         preds = (probs &gt; 0.5).astype('uint8')         acc_train = (preds == Y_train).mean()         with sorix.no_grad():             logits = net(X_test)             probs = sorix.sigmoid(logits)             preds = (probs &gt; 0.5).astype('uint8')             acc_test = (preds == Y_test).mean()          # Usamos una f-string para formatear y alinear la salida         print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")          if acc_test.item() &gt; 0.98:             break  <pre>[cuda] Epoch     0 | Loss: 0.9079 | Acc Train: 50.21% | Acc Test: 42.48%\n[cuda] Epoch    10 | Loss: 0.7473 | Acc Train: 47.99% | Acc Test: 48.45%\n[cuda] Epoch    20 | Loss: 0.6654 | Acc Train: 58.76% | Acc Test: 59.17%\n[cuda] Epoch    30 | Loss: 0.6050 | Acc Train: 66.54% | Acc Test: 65.55%\n[cuda] Epoch    40 | Loss: 0.5524 | Acc Train: 83.05% | Acc Test: 82.47%\n[cuda] Epoch    50 | Loss: 0.5014 | Acc Train: 86.21% | Acc Test: 84.70%\n[cuda] Epoch    60 | Loss: 0.4565 | Acc Train: 86.70% | Acc Test: 85.22%\n[cuda] Epoch    70 | Loss: 0.4171 | Acc Train: 86.73% | Acc Test: 92.50%\n[cuda] Epoch    80 | Loss: 0.3817 | Acc Train: 94.99% | Acc Test: 94.45%\n[cuda] Epoch    90 | Loss: 0.3494 | Acc Train: 95.96% | Acc Test: 95.97%\n[cuda] Epoch   100 | Loss: 0.3190 | Acc Train: 96.74% | Acc Test: 96.73%\n</pre> <pre>[cuda] Epoch   110 | Loss: 0.2900 | Acc Train: 97.32% | Acc Test: 97.35%\n[cuda] Epoch   120 | Loss: 0.2627 | Acc Train: 97.79% | Acc Test: 97.75%\n[cuda] Epoch   130 | Loss: 0.2371 | Acc Train: 98.21% | Acc Test: 98.10%\n</pre> In\u00a0[11]: Copied! <pre>with sorix.no_grad():\n    logits = net(X_test)\n    probs = sorix.sigmoid(logits)\n    preds = (probs &gt; 0.5).astype('uint8')\n    acc_test = (preds == Y_test).mean()\n   \n\ny_pred_labels = [id2labels[y.item()] for y in preds]\n\ndf_test['pred_labels'] = y_pred_labels\n\nfor label in df_test['pred_labels'].unique():\n    x = df_test[df_test['pred_labels'] == label]['x']\n    y = df_test[df_test['pred_labels'] == label]['y']\n\n    plt.scatter(x,y,s=50,label=label)\n\nplt.title(f\"Circles Tetst Dataset: Accuracy: {100*acc_test.item():.2f}%\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n</pre> with sorix.no_grad():     logits = net(X_test)     probs = sorix.sigmoid(logits)     preds = (probs &gt; 0.5).astype('uint8')     acc_test = (preds == Y_test).mean()      y_pred_labels = [id2labels[y.item()] for y in preds]  df_test['pred_labels'] = y_pred_labels  for label in df_test['pred_labels'].unique():     x = df_test[df_test['pred_labels'] == label]['x']     y = df_test[df_test['pred_labels'] == label]['y']      plt.scatter(x,y,s=50,label=label)  plt.title(f\"Circles Tetst Dataset: Accuracy: {100*acc_test.item():.2f}%\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend() Out[11]: <pre>&lt;matplotlib.legend.Legend at 0x7f2a5a1cda90&gt;</pre> In\u00a0[12]: Copied! <pre>sns.heatmap(confusion_matrix(Y_test, preds), annot=True, cmap=\"Blues\", fmt=\".0f\")\n</pre> sns.heatmap(confusion_matrix(Y_test, preds), annot=True, cmap=\"Blues\", fmt=\".0f\") Out[12]: <pre>&lt;Axes: &gt;</pre> In\u00a0[13]: Copied! <pre>print(classification_report(Y_test, preds))\n</pre> print(classification_report(Y_test, preds)) <pre>            precision   recall f1-score  support\n0                0.96     1.00     0.98     1956\n1                1.00     0.96     0.98     2044\n\naccuracy                           0.98     4000\nmacro avg        0.98     0.98     0.98     4000\nweighted avg     0.98     0.98     0.98     4000\n</pre> In\u00a0[14]: Copied! <pre>sorix.save(net.state_dict(),\"model_weights.sor\")\n</pre> sorix.save(net.state_dict(),\"model_weights.sor\") In\u00a0[15]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"model_weights.sor\"))\n\nif X_test.device == 'cpu':\n    with sorix.no_grad():\n        out = net2(X_test)\nif X_test.device == 'cuda':\n    with sorix.no_grad():\n        out = net2(X_test.to('cpu'))\n\nout\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"model_weights.sor\"))  if X_test.device == 'cpu':     with sorix.no_grad():         out = net2(X_test) if X_test.device == 'cuda':     with sorix.no_grad():         out = net2(X_test.to('cpu'))  out Out[15]: <pre>tensor([[-1.28221616],\n        [ 0.34796062],\n        [-1.41771823],\n        ...,\n        [-2.16351099],\n        [-1.23679876],\n        [ 0.47857977]], dtype=sorix.float64)</pre> In\u00a0[16]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"model_weights.sor\"))\nnet2.to('cuda')\n\nif X_test.device == 'cpu':\n    with sorix.no_grad():\n        out = net2(X_test.to('cuda'))\nif X_test.device == 'cuda':\n    with sorix.no_grad():\n        out = net2(X_test)\n\nout\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"model_weights.sor\")) net2.to('cuda')  if X_test.device == 'cpu':     with sorix.no_grad():         out = net2(X_test.to('cuda')) if X_test.device == 'cuda':     with sorix.no_grad():         out = net2(X_test)  out Out[16]: <pre>tensor([[-1.28221616],\n        [ 0.34796062],\n        [-1.41771823],\n        ...,\n        [-2.16351099],\n        [-1.23679876],\n        [ 0.47857977]], device='cuda:0', dtype=sorix.float64)</pre>"},{"location":"examples/nn/2.1-classification-binary/#classification-binary","title":"Classification Binary\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#datos","title":"Datos\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#training","title":"Training\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#prediction","title":"Prediction\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#save-and-load-model","title":"Save and Load Model\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#cpu","title":"CPU\u00b6","text":""},{"location":"examples/nn/2.1-classification-binary/#gpu","title":"GPU\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/","title":"Classification Multiclass","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line to install GPU version with Cupy backend\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the following line to install GPU version with Cupy backend #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import joblib\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\nimport sorix\nfrom sorix.nn import ReLU,Linear\nfrom sorix.optim import SGDMomentum,RMSprop,Adam,SGD\nfrom sorix import tensor,Tensor\nfrom sorix.nn import Module\nfrom sorix.nn import CrossEntropyLoss\nfrom sorix.metrics import confusion_matrix,classification_report\nfrom sorix.model_selection import train_test_split\n</pre> import joblib import seaborn as sns import numpy as np import pandas as pd import matplotlib.pyplot as plt   import sorix from sorix.nn import ReLU,Linear from sorix.optim import SGDMomentum,RMSprop,Adam,SGD from sorix import tensor,Tensor from sorix.nn import Module from sorix.nn import CrossEntropyLoss from sorix.metrics import confusion_matrix,classification_report from sorix.model_selection import train_test_split In\u00a0[3]: Copied! <pre>device = 'cuda' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'cuda' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 14.0.1\n</pre> Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre>r1 = 0.5  \nr2 = 1.5  \nr3 = 1  \n\nnum_points = 10000\nthetas = np.linspace(0, 2 * np.pi, num_points) \n\n\nx1 = r1 * np.cos(thetas) + 0.1 * np.random.randn(num_points)\ny1 = r1 * np.sin(thetas) + 0.1 * np.random.randn(num_points)\n\n\nx2 = r2 * np.cos(thetas) + 0.1 * np.random.randn(num_points)\ny2 = r2 * np.sin(thetas) + 0.1 * np.random.randn(num_points)\n\n\nx3 = 5 + r3 * np.cos(thetas) + 0.1 * np.random.randn(num_points)\ny3 = 5 + r3 * np.sin(thetas) + 0.1 * np.random.randn(num_points)\n\n\nplt.figure(figsize=(8, 7)) \n\n\nplt.scatter(x1, y1, s=50, label=\"Clase A\", alpha=0.8) # Puntos de la Clase A\nplt.scatter(x2, y2, s=50, label=\"Clase B\", alpha=0.8) # Puntos de la Clase B\nplt.scatter(x3, y3, s=50, label=\"Clase C\", alpha=0.8) # Puntos de la Clase C\n\nplt.xlabel(\"Caracter\u00edstica X\") \nplt.ylabel(\"Caracter\u00edstica Y\")\nplt.title(\"Dataset de C\u00edrculos con 3 Clases\")\nplt.legend() \nplt.grid(True, linestyle='--', alpha=0.6)\nplt.axis('equal')\nplt.show() \n</pre> r1 = 0.5   r2 = 1.5   r3 = 1    num_points = 10000 thetas = np.linspace(0, 2 * np.pi, num_points)    x1 = r1 * np.cos(thetas) + 0.1 * np.random.randn(num_points) y1 = r1 * np.sin(thetas) + 0.1 * np.random.randn(num_points)   x2 = r2 * np.cos(thetas) + 0.1 * np.random.randn(num_points) y2 = r2 * np.sin(thetas) + 0.1 * np.random.randn(num_points)   x3 = 5 + r3 * np.cos(thetas) + 0.1 * np.random.randn(num_points) y3 = 5 + r3 * np.sin(thetas) + 0.1 * np.random.randn(num_points)   plt.figure(figsize=(8, 7))    plt.scatter(x1, y1, s=50, label=\"Clase A\", alpha=0.8) # Puntos de la Clase A plt.scatter(x2, y2, s=50, label=\"Clase B\", alpha=0.8) # Puntos de la Clase B plt.scatter(x3, y3, s=50, label=\"Clase C\", alpha=0.8) # Puntos de la Clase C  plt.xlabel(\"Caracter\u00edstica X\")  plt.ylabel(\"Caracter\u00edstica Y\") plt.title(\"Dataset de C\u00edrculos con 3 Clases\") plt.legend()  plt.grid(True, linestyle='--', alpha=0.6) plt.axis('equal') plt.show()   In\u00a0[5]: Copied! <pre>df = pd.DataFrame(\n    {\"x\": x1.tolist()+x2.tolist()+x3.tolist(),\n    \"y\": y1.tolist()+y2.tolist()+y3.tolist(),\n    \"labels\":['A' for _ in range(num_points)]+['B' for _ in range(num_points)]+['C' for _ in range(num_points)]\n    })\n\nlabels = df[\"labels\"].unique()\nlabels2id = {label: i for i, label in enumerate(labels)}\nid2labels = {v: k for k, v in labels2id.items()}\n\ndf[\"labels\"] = df[\"labels\"].map(labels2id)\ndf.head()\n</pre> df = pd.DataFrame(     {\"x\": x1.tolist()+x2.tolist()+x3.tolist(),     \"y\": y1.tolist()+y2.tolist()+y3.tolist(),     \"labels\":['A' for _ in range(num_points)]+['B' for _ in range(num_points)]+['C' for _ in range(num_points)]     })  labels = df[\"labels\"].unique() labels2id = {label: i for i, label in enumerate(labels)} id2labels = {v: k for k, v in labels2id.items()}  df[\"labels\"] = df[\"labels\"].map(labels2id) df.head() Out[5]: x y labels 0 0.502291 0.023374 0 1 0.471920 0.121372 0 2 0.531328 -0.078208 0 3 0.471919 -0.123413 0 4 0.363109 -0.034028 0 In\u00a0[6]: Copied! <pre>df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n\nX_train = tensor(df_train[[\"x\",\"y\"]].values).to(device)\nY_train = tensor(df_train[[\"labels\"]].values).to(device)\nX_test = tensor(df_test[[\"x\",\"y\"]].values).to(device)\nY_test = tensor(df_test[[\"labels\"]].values).to(device)\n\nprint(f\"X_train shape: {X_train.shape}, device: {X_train.device}\")\nprint(f\"Y_train shape: {Y_train.shape}, device: {Y_train.device}\")\nprint(f\"X_test shape: {X_test.shape}, device: {X_test.device}\")\nprint(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\")\n</pre> df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)   X_train = tensor(df_train[[\"x\",\"y\"]].values).to(device) Y_train = tensor(df_train[[\"labels\"]].values).to(device) X_test = tensor(df_test[[\"x\",\"y\"]].values).to(device) Y_test = tensor(df_test[[\"labels\"]].values).to(device)  print(f\"X_train shape: {X_train.shape}, device: {X_train.device}\") print(f\"Y_train shape: {Y_train.shape}, device: {Y_train.device}\") print(f\"X_test shape: {X_test.shape}, device: {X_test.device}\") print(f\"Y_test shape: {Y_test.shape}, device: {Y_test.device}\")  <pre>X_train shape: sorix.Size([24000, 2]), device: cuda:0\nY_train shape: sorix.Size([24000, 1]), device: cuda:0\nX_test shape: sorix.Size([6000, 2]), device: cuda:0\nY_test shape: sorix.Size([6000, 1]), device: cuda:0\n</pre> In\u00a0[7]: Copied! <pre>X_train.grad\n</pre> X_train.grad In\u00a0[8]: Copied! <pre>class Network(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = Linear(2, 4)\n        self.relu = ReLU()\n        self.fc2 = Linear(4, 4)\n        self.fc3 = Linear(4, 3)\n\n    def forward(self, x: tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n\nnet = Network().to(device)\nnet.parameters()\n</pre> class Network(Module):     def __init__(self):         super().__init__()         self.fc1 = Linear(2, 4)         self.relu = ReLU()         self.fc2 = Linear(4, 4)         self.fc3 = Linear(4, 3)      def forward(self, x: tensor) -&gt; Tensor:         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         x = self.relu(x)         x = self.fc3(x)         return x   net = Network().to(device) net.parameters() Out[8]: <pre>[tensor([[ 0.8218481 ,  1.6584973 , -0.21329844, -0.22710423],\n         [-1.283966  ,  1.0882515 , -0.06972136, -0.03704953]], device='cuda:0', requires_grad=True),\n tensor([[0., 0., 0., 0.]], device='cuda:0', requires_grad=True),\n tensor([[ 0.30648765,  0.21343027, -0.09919398,  0.9340736 ],\n         [-1.133978  ,  0.14940731,  0.28791216, -0.29160607],\n         [ 0.43589577,  0.4688745 ,  0.78475726, -0.0788846 ],\n         [-0.15354739,  0.38126808, -0.13964173, -1.0757526 ]], device='cuda:0', requires_grad=True),\n tensor([[0., 0., 0., 0.]], device='cuda:0', requires_grad=True),\n tensor([[ 0.60982543, -1.2970355 ,  0.28402847],\n         [ 1.7949859 ,  0.37703976, -0.49080712],\n         [-0.31916267,  0.26207057, -0.01511212],\n         [ 0.5347513 ,  1.025364  ,  0.9294297 ]], device='cuda:0', requires_grad=True),\n tensor([[0., 0., 0.]], device='cuda:0', requires_grad=True)]</pre> In\u00a0[9]: Copied! <pre>criterion = CrossEntropyLoss()\noptimizer = RMSprop(net.parameters(), lr=1e-2)\n</pre> criterion = CrossEntropyLoss() optimizer = RMSprop(net.parameters(), lr=1e-2) In\u00a0[10]: Copied! <pre>logits = net(X_train)\nlogits\n</pre> logits = net(X_train) logits Out[10]: <pre>tensor([[ 0.12976858,  0.0970069 , -0.05718071],\n        [ 2.64313017,  1.9758392 , -1.16465833],\n        [ 2.45234262,  1.83321833, -1.08059047],\n        ...,\n        [ 2.22724158,  1.66494684, -0.98140284],\n        [ 2.10826844,  1.57600994, -0.92897899],\n        [ 0.28654481,  0.15230036, -0.10700498]], device='cuda:0', dtype=sorix.float64, requires_grad=True)</pre> In\u00a0[11]: Copied! <pre>preds = sorix.argmax(logits, axis=1, keepdims=True)\npreds\n</pre> preds = sorix.argmax(logits, axis=1, keepdims=True) preds Out[11]: <pre>tensor([[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]], device='cuda:0', dtype=sorix.int64)</pre> In\u00a0[12]: Copied! <pre># Bucle de entrenamiento mejorado\nfor epoch in range(10000 + 1):\n    logits = net(X_train)\n    loss = criterion(logits, Y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        preds = sorix.argmax(logits, axis=1, keepdims=True)\n        acc_train = (preds == Y_train).mean()\n        with sorix.no_grad():\n            logits = net(X_test)\n            preds = sorix.argmax(logits, axis=1, keepdims=True)\n            acc_test = (preds == Y_test).mean()\n\n        # Usamos una f-string para formatear y alinear la salida\n        print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")\n\n        if acc_test.item() &gt;= 0.98:  # Mejoramos el criterio de parada\n            print(f\"Entrenamiento completado en {epoch} epochs!\")\n            break\n</pre> # Bucle de entrenamiento mejorado for epoch in range(10000 + 1):     logits = net(X_train)     loss = criterion(logits, Y_train)      optimizer.zero_grad()     loss.backward()     optimizer.step()      if epoch % 10 == 0:         preds = sorix.argmax(logits, axis=1, keepdims=True)         acc_train = (preds == Y_train).mean()         with sorix.no_grad():             logits = net(X_test)             preds = sorix.argmax(logits, axis=1, keepdims=True)             acc_test = (preds == Y_test).mean()          # Usamos una f-string para formatear y alinear la salida         print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")          if acc_test.item() &gt;= 0.98:  # Mejoramos el criterio de parada             print(f\"Entrenamiento completado en {epoch} epochs!\")             break  <pre>[cuda] Epoch     0 | Loss: 2.0356 | Acc Train: 33.73% | Acc Test: 20.73%\n[cuda] Epoch    10 | Loss: 0.8267 | Acc Train: 77.54% | Acc Test: 78.52%\n[cuda] Epoch    20 | Loss: 0.5944 | Acc Train: 79.47% | Acc Test: 80.15%\n[cuda] Epoch    30 | Loss: 0.4896 | Acc Train: 80.84% | Acc Test: 81.70%\n[cuda] Epoch    40 | Loss: 0.4190 | Acc Train: 83.32% | Acc Test: 83.98%\n[cuda] Epoch    50 | Loss: 0.3617 | Acc Train: 85.93% | Acc Test: 86.53%\n[cuda] Epoch    60 | Loss: 0.3075 | Acc Train: 88.50% | Acc Test: 88.35%\n[cuda] Epoch    70 | Loss: 0.2525 | Acc Train: 94.22% | Acc Test: 94.20%\n[cuda] Epoch    80 | Loss: 0.2022 | Acc Train: 96.98% | Acc Test: 96.72%\n[cuda] Epoch    90 | Loss: 0.1609 | Acc Train: 98.27% | Acc Test: 98.05%\nEntrenamiento completado en 90 epochs!\n</pre> In\u00a0[13]: Copied! <pre>with sorix.no_grad():\n    logits = net(X_test)\n\npreds = sorix.argmax(logits, axis=1, keepdims=True)\nacc = (preds == Y_test).mean()   \n\n\ny_pred_labels = [id2labels[y.item()] for y in preds]\n\ndf_test['pred_labels'] = y_pred_labels\n\nfor label in df_test['pred_labels'].unique():\n    x = df_test[df_test['pred_labels'] == label]['x']\n    y = df_test[df_test['pred_labels'] == label]['y']\n\n    plt.scatter(x,y,s=50,label=label)\n\nplt.title(f\"Circles Tetst Dataset: Accuracy: {100*acc.item():.2f}%\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n</pre> with sorix.no_grad():     logits = net(X_test)  preds = sorix.argmax(logits, axis=1, keepdims=True) acc = (preds == Y_test).mean()      y_pred_labels = [id2labels[y.item()] for y in preds]  df_test['pred_labels'] = y_pred_labels  for label in df_test['pred_labels'].unique():     x = df_test[df_test['pred_labels'] == label]['x']     y = df_test[df_test['pred_labels'] == label]['y']      plt.scatter(x,y,s=50,label=label)  plt.title(f\"Circles Tetst Dataset: Accuracy: {100*acc.item():.2f}%\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend() Out[13]: <pre>&lt;matplotlib.legend.Legend at 0x7fe4d6bdc7d0&gt;</pre> In\u00a0[14]: Copied! <pre>sns.heatmap(confusion_matrix(Y_test, preds), annot=True, cmap=\"Blues\", fmt=\".0f\")\n</pre> sns.heatmap(confusion_matrix(Y_test, preds), annot=True, cmap=\"Blues\", fmt=\".0f\") Out[14]: <pre>&lt;Axes: &gt;</pre> In\u00a0[15]: Copied! <pre>print(classification_report(Y_test, preds))\n</pre> print(classification_report(Y_test, preds)) <pre>            precision   recall f1-score  support\n0                0.94     1.00     0.97     1917\n1                1.00     0.94     0.97     1989\n2                1.00     1.00     1.00     2094\n\naccuracy                           0.98     6000\nmacro avg        0.98     0.98     0.98     6000\nweighted avg     0.98     0.98     0.98     6000\n</pre> In\u00a0[16]: Copied! <pre>sorix.save(net.state_dict(),\"model_weights.sor\")\n</pre> sorix.save(net.state_dict(),\"model_weights.sor\") In\u00a0[17]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"model_weights.sor\"))\n\nif X_test.device == 'cpu':\n    with sorix.no_grad():\n        logits = net2(X_test)\nif X_test.device == 'cuda':\n    with sorix.no_grad():\n        logits = net2(X_test.to('cpu'))\n\nlogits\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"model_weights.sor\"))  if X_test.device == 'cpu':     with sorix.no_grad():         logits = net2(X_test) if X_test.device == 'cuda':     with sorix.no_grad():         logits = net2(X_test.to('cpu'))  logits Out[17]: <pre>tensor([[-17.36036001,   3.77189297,   8.35657141],\n        [-19.42832721,   4.38193053,   9.66869124],\n        [-17.07820553,   3.54820426,   8.250104  ],\n        ...,\n        [  4.48506094,   2.25405481,  -2.85032539],\n        [  1.70595854,   1.15646953,  -1.60182957],\n        [-22.02415832,   5.51679405,  11.22599272]], dtype=sorix.float64)</pre> In\u00a0[18]: Copied! <pre>net2 = Network()\nnet2.load_state_dict(sorix.load(\"model_weights.sor\"))\nnet2.to('cuda')\n\nif X_test.device == 'cpu':\n    with sorix.no_grad():\n        logits = net2(X_test.to('cuda'))\nif X_test.device == 'cuda':\n    with sorix.no_grad():\n        logits = net2(X_test)\n\nlogits\n</pre> net2 = Network() net2.load_state_dict(sorix.load(\"model_weights.sor\")) net2.to('cuda')  if X_test.device == 'cpu':     with sorix.no_grad():         logits = net2(X_test.to('cuda')) if X_test.device == 'cuda':     with sorix.no_grad():         logits = net2(X_test)  logits Out[18]: <pre>tensor([[-17.36036001,   3.77189297,   8.35657141],\n        [-19.42832721,   4.38193053,   9.66869124],\n        [-17.07820553,   3.54820426,   8.250104  ],\n        ...,\n        [  4.48506094,   2.25405481,  -2.85032539],\n        [  1.70595854,   1.15646953,  -1.60182957],\n        [-22.02415832,   5.51679405,  11.22599272]], device='cuda:0', dtype=sorix.float64)</pre>"},{"location":"examples/nn/2.2-classification-multiclass/#classification-multiclass","title":"Classification Multiclass\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#datos","title":"Datos\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#training","title":"Training\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#prediction","title":"Prediction\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#save-and-load-model","title":"Save and Load Model\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#cpu","title":"CPU\u00b6","text":""},{"location":"examples/nn/2.2-classification-multiclass/#gpu","title":"GPU\u00b6","text":""},{"location":"examples/nn/3-Iris_dataset/","title":"Iris Dataset","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line to install GPU version with Cupy backend\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the following line to install GPU version with Cupy backend #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom typing import Union\n\nimport sorix\nfrom sorix.model_selection import train_test_split\nfrom sorix.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\nfrom sorix.nn import Module\nfrom sorix import tensor,Tensor\nfrom sorix.nn import CrossEntropyLoss\nfrom sorix.metrics import accuracy_score\nfrom sorix.optim import RMSprop\nfrom sorix.nn import Linear,ReLU\nfrom sorix.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from typing import Union  import sorix from sorix.model_selection import train_test_split from sorix.preprocessing import StandardScaler,MinMaxScaler,RobustScaler from sorix.nn import Module from sorix import tensor,Tensor from sorix.nn import CrossEntropyLoss from sorix.metrics import accuracy_score from sorix.optim import RMSprop from sorix.nn import Linear,ReLU from sorix.metrics import confusion_matrix, classification_report import seaborn as sns In\u00a0[3]: Copied! <pre>device = 'cuda' if sorix.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = 'cuda' if sorix.cuda.is_available() else \"cpu\" device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 14.0.1\n</pre> Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre>df  = pd.read_csv(\"../data/Iris.csv\")\ndf.head()\n</pre> df  = pd.read_csv(\"../data/Iris.csv\") df.head() Out[4]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[5]: Copied! <pre>labels = df[\"Species\"].unique()\n\nlabels2id = {label: i for i, label in enumerate(labels)}\nid2labels = {i: label for i, label in enumerate(labels)}\n</pre> labels = df[\"Species\"].unique()  labels2id = {label: i for i, label in enumerate(labels)} id2labels = {i: label for i, label in enumerate(labels)} In\u00a0[6]: Copied! <pre>df['labels'] = df['Species'].map(labels2id)\ndf.head()\n</pre> df['labels'] = df['Species'].map(labels2id) df.head() Out[6]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species labels 0 1 5.1 3.5 1.4 0.2 Iris-setosa 0 1 2 4.9 3.0 1.4 0.2 Iris-setosa 0 2 3 4.7 3.2 1.3 0.2 Iris-setosa 0 3 4 4.6 3.1 1.5 0.2 Iris-setosa 0 4 5 5.0 3.6 1.4 0.2 Iris-setosa 0 In\u00a0[7]: Copied! <pre>df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n</pre> df_train, df_test = train_test_split(df, test_size=0.2, random_state=42) In\u00a0[8]: Copied! <pre>features = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\nlabels = [\"labels\"]\n\nX_train = df_train[features]\ny_train = df_train[labels]\n\nX_test = df_test[features]\ny_test = df_test[labels]\n</pre> features = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"] labels = [\"labels\"]  X_train = df_train[features] y_train = df_train[labels]  X_test = df_test[features] y_test = df_test[labels] In\u00a0[9]: Copied! <pre>plt.scatter(X_train['SepalLengthCm'], X_train['SepalWidthCm'], c=y_train['labels'])\n</pre> plt.scatter(X_train['SepalLengthCm'], X_train['SepalWidthCm'], c=y_train['labels']) Out[9]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f8ce2186900&gt;</pre> In\u00a0[10]: Copied! <pre>scaler = StandardScaler()\n\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</pre> scaler = StandardScaler()  scaler.fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) In\u00a0[11]: Copied! <pre>X_train_scaled.shape, y_train.shape, X_test_scaled.shape, y_test.shape\n</pre> X_train_scaled.shape, y_train.shape, X_test_scaled.shape, y_test.shape Out[11]: <pre>((120, 4), (120, 1), (30, 4), (30, 1))</pre> In\u00a0[12]: Copied! <pre>X_train_tensor = tensor(X_train_scaled).to(device)\nY_train_tensor = tensor(y_train).to(device)\n\nX_test_tensor = tensor(X_test_scaled).to(device)\nY_test_tensor = tensor(y_test).to(device)\n\nprint(f\"X_train shape: {X_train_tensor.shape}, device: {X_train_tensor.device}\")\nprint(f\"Y_train shape: {Y_train_tensor.shape}, device: {Y_train_tensor.device}\")\nprint(f\"X_test shape: {X_test_tensor.shape}, device: {X_test_tensor.device}\")\nprint(f\"Y_test shape: {Y_test_tensor.shape}, device: {Y_test_tensor.device}\")\n</pre>  X_train_tensor = tensor(X_train_scaled).to(device) Y_train_tensor = tensor(y_train).to(device)  X_test_tensor = tensor(X_test_scaled).to(device) Y_test_tensor = tensor(y_test).to(device)  print(f\"X_train shape: {X_train_tensor.shape}, device: {X_train_tensor.device}\") print(f\"Y_train shape: {Y_train_tensor.shape}, device: {Y_train_tensor.device}\") print(f\"X_test shape: {X_test_tensor.shape}, device: {X_test_tensor.device}\") print(f\"Y_test shape: {Y_test_tensor.shape}, device: {Y_test_tensor.device}\") <pre>X_train shape: sorix.Size([120, 4]), device: cuda:0\nY_train shape: sorix.Size([120, 1]), device: cuda:0\nX_test shape: sorix.Size([30, 4]), device: cuda:0\nY_test shape: sorix.Size([30, 1]), device: cuda:0\n</pre> In\u00a0[13]: Copied! <pre>class Net(Module):\n\n    def __init__(self):\n        super().__init__()\n        self.fc1 = Linear(4, 8)\n        self.fc2 = Linear(8, 4)\n        self.fc3 = Linear(4, 3)\n        self.relu = ReLU()\n\n    def forward(self, x: tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n    \n\nnet = Net().to(device)\ncriterion = CrossEntropyLoss()\noptimizer = RMSprop(net.parameters(), lr=1e-2)\n</pre> class Net(Module):      def __init__(self):         super().__init__()         self.fc1 = Linear(4, 8)         self.fc2 = Linear(8, 4)         self.fc3 = Linear(4, 3)         self.relu = ReLU()      def forward(self, x: tensor) -&gt; Tensor:         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         x = self.relu(x)         x = self.fc3(x)         return x       net = Net().to(device) criterion = CrossEntropyLoss() optimizer = RMSprop(net.parameters(), lr=1e-2)  In\u00a0[14]: Copied! <pre>logits = net(X_train_tensor)\nlogits\n</pre> logits = net(X_train_tensor) logits Out[14]: <pre>tensor([[ 3.18380770e-02,  5.91685631e-02, -7.86396504e-03],\n        [ 3.13370197e+00,  5.74086691e+00, -7.45613334e-01],\n        [-6.27524252e-02, -1.36923843e+00,  1.58636662e+00],\n        [ 3.68812375e-03, -3.08446122e-01,  4.18230142e-01],\n        [-2.65776979e-02, -2.79022078e-01,  2.54321886e-01],\n        [ 2.34201072e+00,  4.24755254e+00, -5.85250038e-01],\n        [ 8.82216808e-03,  2.42461865e-03,  1.40565934e-02],\n        [ 8.13916013e-02, -1.05761437e+00,  1.66882102e+00],\n        [-2.64715199e-02, -1.49514641e-01,  9.24898444e-02],\n        [-5.54242181e-04, -1.86377704e-02, -4.84749159e-02],\n        [ 3.60032830e-02, -8.28358602e-01,  1.21232245e+00],\n        [ 2.06400820e+00,  3.68548434e+00, -5.50437986e-01],\n        [ 2.88001819e+00,  5.23662302e+00, -7.55856685e-01],\n        [ 2.21337046e+00,  3.97225913e+00, -5.78986114e-01],\n        [ 3.18288505e+00,  5.80945838e+00, -6.86235603e-01],\n        [ 1.45123381e-02, -5.24926676e-01,  7.39887747e-01],\n        [ 2.46062169e-02, -1.00287913e+00,  1.40291056e+00],\n        [-5.21423812e-02, -1.27879173e-01, -4.72920072e-02],\n        [-5.65749397e-03, -3.95742734e-02,  3.27227844e-02],\n        [ 2.88399195e-02, -9.18760118e-01,  1.30674497e+00],\n        [ 2.19221991e+00,  3.93667524e+00, -5.17301622e-01],\n        [ 8.97614835e-03, -5.72666380e-01,  7.83763085e-01],\n        [ 2.29760228e+00,  4.15590958e+00, -5.26909164e-01],\n        [ 2.39446310e-02, -8.61873039e-01,  1.21521530e+00],\n        [-3.89442505e-02, -1.43634853e+00,  1.75593019e+00],\n        [ 6.55337450e-02, -1.01456623e+00,  1.55805238e+00],\n        [-5.04178076e-02, -6.63503494e-01,  7.00384973e-01],\n        [ 4.32010985e-02, -1.16846247e+00,  1.68417142e+00],\n        [ 1.77281707e+00,  3.15398165e+00, -4.43245852e-01],\n        [ 2.02470363e+00,  3.62813623e+00, -4.86150842e-01],\n        [ 3.13682903e+00,  5.67645254e+00, -7.30882150e-01],\n        [ 4.04362152e+00,  7.43207193e+00, -7.80373135e-01],\n        [-5.05755007e-02, -4.29726041e-01,  3.92407766e-01],\n        [ 2.54596961e+00,  4.60337048e+00, -5.74363675e-01],\n        [ 2.30134087e+00,  4.11463922e+00, -5.71982983e-01],\n        [ 2.10389747e-02, -6.10826781e-01,  8.75143434e-01],\n        [-9.43018398e-04, -4.67122438e-01,  6.11088082e-01],\n        [ 2.78381905e+00,  5.05903683e+00, -6.53340586e-01],\n        [ 2.97422700e+00,  5.40743424e+00, -6.47728008e-01],\n        [ 3.95251834e+00,  7.24417637e+00, -8.03992697e-01],\n        [-8.61208329e-03, -5.44252653e-01,  6.86330607e-01],\n        [-1.79378373e-03, -4.57687607e-01,  5.95774957e-01],\n        [-3.87303344e-02, -5.21625127e-01,  5.53716051e-01],\n        [ 3.30825152e+00,  6.05562187e+00, -7.02709854e-01],\n        [ 3.14721013e+00,  5.74610699e+00, -7.31635734e-01],\n        [-9.77670537e-02, -2.30847597e-01, -8.92878186e-02],\n        [-3.39432586e-02, -3.70475437e-01,  3.71288841e-01],\n        [ 6.65223663e-03, -7.32668999e-01,  9.86244692e-01],\n        [-4.89626813e-02, -3.82594034e-01,  3.35932843e-01],\n        [ 6.60773547e-02, -1.46109646e+00,  2.14713684e+00],\n        [-1.15041971e-02, -8.57852344e-02,  7.35272471e-02],\n        [-5.57455970e-02, -1.26605881e+00,  1.47460515e+00],\n        [-2.70143290e-02, -2.49851595e-01,  2.36320584e-01],\n        [ 2.80564147e+00,  5.09444191e+00, -6.36834297e-01],\n        [-9.94160128e-02, -1.16481896e+00,  1.19232601e+00],\n        [ 8.81469045e-02,  1.41017470e-01, -8.47095075e-02],\n        [ 2.64496766e+00,  4.79361724e+00, -6.51898193e-01],\n        [ 2.70862079e+00,  4.91054523e+00, -6.09610678e-01],\n        [ 2.91926941e+00,  5.33392086e+00, -6.28574545e-01],\n        [-5.69323871e-02, -1.83996163e-01, -4.85782950e-02],\n        [ 1.91095232e-02, -5.71609068e-01,  8.16979266e-01],\n        [ 2.44987675e+00,  4.43411215e+00, -5.48691117e-01],\n        [ 1.87454068e+00,  3.34866676e+00, -5.28320474e-01],\n        [ 1.95499250e+00,  3.52732934e+00, -4.56857253e-01],\n        [-1.39832846e-02, -3.81138219e-02, -1.24192564e-02],\n        [ 2.60212352e+00,  4.71550175e+00, -6.14193012e-01],\n        [-1.74164042e-03, -3.97214924e-03, -1.60025383e-03],\n        [ 8.84677919e-03, -1.56237158e+00,  2.08487385e+00],\n        [ 2.25639293e+00,  4.04456673e+00, -5.41551958e-01],\n        [-2.13781899e-02, -1.49540828e-01,  1.23650843e-01],\n        [ 1.83769871e-02, -4.92952115e-01,  7.11036506e-01],\n        [ 4.01361073e+00,  7.36482486e+00, -8.13147441e-01],\n        [ 5.43918374e-03, -5.39444581e-01,  7.27994249e-01],\n        [-8.61208329e-03, -5.44252653e-01,  6.86330607e-01],\n        [ 1.84389463e-01,  2.88705400e-01, -1.94539596e-01],\n        [-2.61323801e-02, -1.82796475e-01,  1.51148944e-01],\n        [ 1.42086436e-02, -1.02002889e+00,  1.38995515e+00],\n        [ 1.62906834e-02, -5.67155660e-05,  1.75329364e-02],\n        [ 3.15816858e+00,  5.78813885e+00, -6.55640220e-01],\n        [-7.37799912e-02, -2.10869806e-01, -6.48543265e-02],\n        [ 1.97121320e-02, -6.98977705e-01,  9.86538615e-01],\n        [ 2.45276126e+00,  4.42872696e+00, -5.85644883e-01],\n        [ 2.34624314e+00,  4.22006766e+00, -5.94801937e-01],\n        [-4.58528401e-02, -1.23924859e-01, -4.07969126e-02],\n        [-1.55761148e-02, -6.71523850e-01,  8.29920866e-01],\n        [ 2.21337046e+00,  3.97225913e+00, -5.78986114e-01],\n        [ 1.37889201e-02, -8.47158929e-01,  1.16118199e+00],\n        [ 2.59161266e+00,  4.69414655e+00, -5.87087276e-01],\n        [ 2.24495156e+00,  4.06386263e+00, -5.36941918e-01],\n        [-5.86844805e-03, -5.57119991e-01,  7.12622161e-01],\n        [-5.34681391e-02, -1.62438706e-01, -4.63365464e-02],\n        [ 5.75974294e-02, -1.15981793e+00,  1.72196818e+00],\n        [-7.66682471e-03, -3.74621636e-01,  4.66478519e-01],\n        [-1.93847917e-02, -1.35596971e-01,  1.12121085e-01],\n        [-1.66016791e-02, -1.43525842e+00,  1.83079905e+00],\n        [ 1.07662092e-02, -2.58287947e-03,  1.17626312e-02],\n        [ 1.71668511e+00,  3.02509832e+00, -4.54053266e-01],\n        [ 2.12903590e+00,  3.77705174e+00, -5.44621435e-01],\n        [-1.16346873e-02, -9.66011259e-02,  8.10755029e-02],\n        [-4.86084197e-02, -9.53046868e-01,  1.08734018e+00],\n        [ 2.04438529e+00,  3.65162275e+00, -4.99071343e-01],\n        [ 2.66797028e+00,  4.84329065e+00, -5.88822143e-01],\n        [ 1.93382898e+00,  3.42756963e+00, -4.99609979e-01],\n        [-3.85869265e-02, -2.69916254e-01,  2.23185687e-01],\n        [ 1.90211079e-02, -1.12409321e+00,  1.54324434e+00],\n        [ 2.50570150e+00,  4.51384544e+00, -6.00848088e-01],\n        [ 4.05301890e-02, -9.79425855e-01,  1.42644957e+00],\n        [ 1.15126805e-02, -1.21833120e+00,  1.64153374e+00],\n        [ 2.28844858e+00,  4.10618547e+00, -5.43055515e-01],\n        [-1.89513001e-02, -2.51221781e-01,  2.65658676e-01],\n        [-4.04414381e-02, -4.00419444e-01,  3.88475909e-01],\n        [ 2.39035689e-02, -5.41035522e-01,  7.93144484e-01],\n        [-7.62890172e-02, -5.34633177e-01,  4.42555595e-01],\n        [ 3.62928176e-02, -1.00766438e+00,  1.44911469e+00],\n        [ 3.26473531e+00,  5.96281259e+00, -6.88166431e-01],\n        [ 4.13973561e-02, -1.01511886e+00,  1.47635059e+00],\n        [ 5.31529751e-03, -2.65784310e-01,  3.67682955e-01],\n        [ 3.65343726e-02, -8.05372287e-01,  1.18390708e+00],\n        [-8.60919474e-02, -2.00252294e-01, -8.98781550e-02],\n        [-2.30541384e-02, -6.82638008e-02, -2.01015647e-02]], device='cuda:0', dtype=sorix.float64, requires_grad=True)</pre> In\u00a0[15]: Copied! <pre># Bucle de entrenamiento mejorado\nfor epoch in range(1000 + 1):\n    logits = net(X_train_tensor)\n    loss = criterion(logits, Y_train_tensor)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        preds = sorix.argmax(logits, axis=1, keepdims=True)\n        acc_train = (preds== Y_train_tensor).mean()\n        with sorix.no_grad():\n            logits = net(X_test_tensor)\n            preds = sorix.argmax(logits, axis=1, keepdims=True)\n            acc_test = (preds == Y_test_tensor).mean()\n\n        # Usamos una f-string para formatear y alinear la salida\n        print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")\n\n        if acc_test.item() &gt;= 0.96:  # Mejoramos el criterio de parada\n            print(f\"Entrenamiento completado en {epoch} epochs!\")\n            break\n</pre> # Bucle de entrenamiento mejorado for epoch in range(1000 + 1):     logits = net(X_train_tensor)     loss = criterion(logits, Y_train_tensor)      optimizer.zero_grad()     loss.backward()     optimizer.step()      if epoch % 10 == 0:         preds = sorix.argmax(logits, axis=1, keepdims=True)         acc_train = (preds== Y_train_tensor).mean()         with sorix.no_grad():             logits = net(X_test_tensor)             preds = sorix.argmax(logits, axis=1, keepdims=True)             acc_test = (preds == Y_test_tensor).mean()          # Usamos una f-string para formatear y alinear la salida         print(f\"[{device}] Epoch {epoch:5d} | Loss: {loss.item():.4f} | Acc Train: {acc_train.item()*100:.2f}% | Acc Test: {acc_test.item()*100:.2f}%\")          if acc_test.item() &gt;= 0.96:  # Mejoramos el criterio de parada             print(f\"Entrenamiento completado en {epoch} epochs!\")             break  <pre>[cuda] Epoch     0 | Loss: 1.3640 | Acc Train: 34.17% | Acc Test: 50.00%\n</pre> <pre>[cuda] Epoch    10 | Loss: 0.4880 | Acc Train: 85.83% | Acc Test: 93.33%\n[cuda] Epoch    20 | Loss: 0.2957 | Acc Train: 90.83% | Acc Test: 93.33%\n[cuda] Epoch    30 | Loss: 0.1825 | Acc Train: 94.17% | Acc Test: 100.00%\nEntrenamiento completado en 30 epochs!\n</pre> In\u00a0[16]: Copied! <pre>with sorix.no_grad():\n    logits = net(X_test_tensor)\n    preds = sorix.argmax(logits, axis=1, keepdims=True)\n</pre> with sorix.no_grad():     logits = net(X_test_tensor)     preds = sorix.argmax(logits, axis=1, keepdims=True) In\u00a0[17]: Copied! <pre>sns.heatmap(confusion_matrix(Y_test_tensor, preds), annot=True, cmap=\"Blues\")\n</pre> sns.heatmap(confusion_matrix(Y_test_tensor, preds), annot=True, cmap=\"Blues\") Out[17]: <pre>&lt;Axes: &gt;</pre> In\u00a0[18]: Copied! <pre>print(classification_report(Y_test_tensor, preds))\n</pre> print(classification_report(Y_test_tensor, preds)) <pre>            precision   recall f1-score  support\n0                1.00     1.00     1.00        7\n1                1.00     1.00     1.00       11\n2                1.00     1.00     1.00       12\n\naccuracy                           1.00       30\nmacro avg        1.00     1.00     1.00       30\nweighted avg     1.00     1.00     1.00       30\n</pre>"},{"location":"examples/nn/3-Iris_dataset/#iris-dataset","title":"Iris Dataset\u00b6","text":""},{"location":"examples/nn/4-digit-recognizer/","title":"MNIST Dataset","text":"In\u00a0[12]: Copied! <pre># Uncomment the following line to install GPU version with Cupy backend\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the following line to install GPU version with Cupy backend #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[13]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sorix\nfrom sorix import tensor\nfrom sorix.nn import Module,Linear, CrossEntropyLoss,ReLU,BatchNorm1d,Dropout\nfrom sorix.optim import SGDMomentum, RMSprop, Adam\nfrom sorix.model_selection import train_test_split\nfrom sorix.utils.data import Dataset, DataLoader\nfrom sorix.metrics import confusion_matrix,classification_report\nfrom datetime import datetime\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns  import sorix from sorix import tensor from sorix.nn import Module,Linear, CrossEntropyLoss,ReLU,BatchNorm1d,Dropout from sorix.optim import SGDMomentum, RMSprop, Adam from sorix.model_selection import train_test_split from sorix.utils.data import Dataset, DataLoader from sorix.metrics import confusion_matrix,classification_report from datetime import datetime In\u00a0[14]: Copied! <pre>device = 'cuda' if sorix.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = 'cuda' if sorix.cuda.is_available() else \"cpu\" device <pre>\u274c CuPy is not installed.\n</pre> Out[14]: <pre>'cpu'</pre> In\u00a0[15]: Copied! <pre>data = pd.read_csv(\"../data/digit-recognizer/train.csv\")\n</pre> data = pd.read_csv(\"../data/digit-recognizer/train.csv\") In\u00a0[16]: Copied! <pre>data.head()\n</pre> data.head() Out[16]: label pixel0 pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ... pixel774 pixel775 pixel776 pixel777 pixel778 pixel779 pixel780 pixel781 pixel782 pixel783 0 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 <p>5 rows \u00d7 785 columns</p> In\u00a0[17]: Copied! <pre>data_train,data_test = train_test_split(data,test_size=0.2)\n</pre> data_train,data_test = train_test_split(data,test_size=0.2) In\u00a0[18]: Copied! <pre>X_train = data_train.drop(\"label\",axis=1).values\nY_train = data_train[[\"label\"]].values\n\nX_test = data_test.drop(\"label\",axis=1).values\nY_test = data_test[[\"label\"]].values\n\ntrain_dataset = Dataset(X_train,Y_train)    \ntest_dataset = Dataset(X_test,Y_test)\n\ntrain_dataloader = DataLoader(train_dataset,batch_size=64)\ntest_dataloader = DataLoader(test_dataset,batch_size=64)\n</pre> X_train = data_train.drop(\"label\",axis=1).values Y_train = data_train[[\"label\"]].values  X_test = data_test.drop(\"label\",axis=1).values Y_test = data_test[[\"label\"]].values  train_dataset = Dataset(X_train,Y_train)     test_dataset = Dataset(X_test,Y_test)  train_dataloader = DataLoader(train_dataset,batch_size=64) test_dataloader = DataLoader(test_dataset,batch_size=64) In\u00a0[19]: Copied! <pre>for x,y in train_dataloader:\n    print(x.shape,y.shape)\n    break\n</pre> for x,y in train_dataloader:     print(x.shape,y.shape)     break <pre>(64, 784) (64, 1)\n</pre> In\u00a0[20]: Copied! <pre>class Model(Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = Linear(784,128,bias=False)\n        self.bn1 = BatchNorm1d(128)\n        self.linear2 = Linear(128,64)\n        self.linear3 = Linear(64,10)\n        self.relu = ReLU()\n        self.dropout = Dropout(p=0.2)\n        \n    def forward(self,x):\n        x = self.linear1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear3(x)\n        return x\n    \n\nmodel = Model()\nmodel.to(device)\nloss_fn = CrossEntropyLoss()\n\noptimizer = RMSprop(model.parameters(), lr=1e-3)\n</pre> class Model(Module):     def __init__(self):         super().__init__()         self.linear1 = Linear(784,128,bias=False)         self.bn1 = BatchNorm1d(128)         self.linear2 = Linear(128,64)         self.linear3 = Linear(64,10)         self.relu = ReLU()         self.dropout = Dropout(p=0.2)              def forward(self,x):         x = self.linear1(x)         x = self.bn1(x)         x = self.relu(x)         x = self.linear2(x)         x = self.relu(x)         x = self.dropout(x)         x = self.linear3(x)         return x       model = Model() model.to(device) loss_fn = CrossEntropyLoss()  optimizer = RMSprop(model.parameters(), lr=1e-3)  In\u00a0[21]: Copied! <pre>for X,Y in train_dataloader:\n    X_tensor = tensor(X, device=device)\n    Y_tensor = tensor(Y, device=device)\n    print(X_tensor.shape,Y_tensor.shape)\n    break\n</pre> for X,Y in train_dataloader:     X_tensor = tensor(X, device=device)     Y_tensor = tensor(Y, device=device)     print(X_tensor.shape,Y_tensor.shape)     break <pre>sorix.Size([64, 784]) sorix.Size([64, 1])\n</pre> In\u00a0[22]: Copied! <pre>start = datetime.now()\n\nepochs = 100\n\nfor epoch in range(epochs+1):\n    model.train()\n    total_train_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    for x, y in train_dataloader:\n        x = tensor(x, device=device)\n        y = tensor(y, device=device)\n\n        # Forward\n        logits = model(x)\n        loss = loss_fn(logits, y)\n\n        # Backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Acumular loss y accuracy\n        total_train_loss += loss.item() * len(y)  # ponderar por tama\u00f1o del batch\n        preds = sorix.argmax(logits,axis=1,keepdims=True)\n        correct_train += (preds == y).sum().item()\n        total_train += len(y)\n\n    avg_train_loss = total_train_loss / total_train\n    avg_train_acc = correct_train / total_train\n\n    # --- Validaci\u00f3n/Test ---\n    if epoch % 5 == 0:\n        with sorix.no_grad():\n            model.eval()\n            total_test_loss = 0.0\n            correct_test = 0\n            total_test = 0\n\n            for x, y in test_dataloader:\n                x = tensor(x, device=device)\n                y = tensor(y, device=device)\n\n                logits = model(x)\n                loss = loss_fn(logits,y)\n\n                total_test_loss += loss.item() * len(y)\n                preds = sorix.argmax(logits,axis=1,keepdims=True)\n                correct_test += (preds == y).sum().item()\n                total_test += len(y)\n\n            avg_test_loss = total_test_loss / total_test\n            avg_test_acc = correct_test / total_test\n            \n            print(f\"[{device}] [{epoch:3d}/{epochs:3d}] | Loss: {avg_test_loss:.4f} | Acc Train: {100*avg_train_acc:.2f}% | Acc Test: {100*avg_test_acc:.2f}%\")\n\n        if avg_test_acc &gt; 0.97:\n            break\nend = datetime.now()\n\ndelta = end-start\ntiempo = delta.total_seconds()\nprint(f\"Tiempo:{tiempo} segundos = {tiempo/60:.2f} min \")\n</pre> start = datetime.now()  epochs = 100  for epoch in range(epochs+1):     model.train()     total_train_loss = 0.0     correct_train = 0     total_train = 0      for x, y in train_dataloader:         x = tensor(x, device=device)         y = tensor(y, device=device)          # Forward         logits = model(x)         loss = loss_fn(logits, y)          # Backprop         optimizer.zero_grad()         loss.backward()         optimizer.step()          # Acumular loss y accuracy         total_train_loss += loss.item() * len(y)  # ponderar por tama\u00f1o del batch         preds = sorix.argmax(logits,axis=1,keepdims=True)         correct_train += (preds == y).sum().item()         total_train += len(y)      avg_train_loss = total_train_loss / total_train     avg_train_acc = correct_train / total_train      # --- Validaci\u00f3n/Test ---     if epoch % 5 == 0:         with sorix.no_grad():             model.eval()             total_test_loss = 0.0             correct_test = 0             total_test = 0              for x, y in test_dataloader:                 x = tensor(x, device=device)                 y = tensor(y, device=device)                  logits = model(x)                 loss = loss_fn(logits,y)                  total_test_loss += loss.item() * len(y)                 preds = sorix.argmax(logits,axis=1,keepdims=True)                 correct_test += (preds == y).sum().item()                 total_test += len(y)              avg_test_loss = total_test_loss / total_test             avg_test_acc = correct_test / total_test                          print(f\"[{device}] [{epoch:3d}/{epochs:3d}] | Loss: {avg_test_loss:.4f} | Acc Train: {100*avg_train_acc:.2f}% | Acc Test: {100*avg_test_acc:.2f}%\")          if avg_test_acc &gt; 0.97:             break end = datetime.now()  delta = end-start tiempo = delta.total_seconds() print(f\"Tiempo:{tiempo} segundos = {tiempo/60:.2f} min \") <pre>[cpu] [  0/100] | Loss: 0.2013 | Acc Train: 86.80% | Acc Test: 94.27%\n[cpu] [  5/100] | Loss: 0.1168 | Acc Train: 96.76% | Acc Test: 96.63%\n[cpu] [ 10/100] | Loss: 0.1189 | Acc Train: 97.62% | Acc Test: 96.89%\n[cpu] [ 15/100] | Loss: 0.1124 | Acc Train: 98.15% | Acc Test: 97.23%\nTiempo:36.436893 segundos = 0.61 min \n</pre> In\u00a0[23]: Copied! <pre>all_preds = np.array([])\nall_targets = np.array([])\n\nwith sorix.no_grad():\n    model.eval()\n    for x, y in test_dataloader:\n        x = tensor(x, device=device)\n        y = tensor(y, device=device)\n        # Predicciones\n        logits = model(x)\n        preds = sorix.argmax(logits,axis=1,keepdims=True).cpu()\n        # Guardar predicciones y targets\n        all_preds = np.append(all_preds,preds)\n        all_targets = np.append(all_targets,y.cpu())\n</pre> all_preds = np.array([]) all_targets = np.array([])  with sorix.no_grad():     model.eval()     for x, y in test_dataloader:         x = tensor(x, device=device)         y = tensor(y, device=device)         # Predicciones         logits = model(x)         preds = sorix.argmax(logits,axis=1,keepdims=True).cpu()         # Guardar predicciones y targets         all_preds = np.append(all_preds,preds)         all_targets = np.append(all_targets,y.cpu())  In\u00a0[24]: Copied! <pre>print(classification_report(all_targets,all_preds))\n</pre> print(classification_report(all_targets,all_preds)) <pre>            precision   recall f1-score  support\n0.0              0.98     0.98     0.98      798\n1.0              0.97     0.99     0.98      920\n2.0              0.98     0.98     0.98      797\n3.0              0.97     0.97     0.97      858\n4.0              0.96     0.98     0.97      813\n5.0              0.98     0.97     0.97      772\n6.0              0.99     0.98     0.98      849\n7.0              0.98     0.97     0.98      929\n8.0              0.96     0.95     0.96      810\n9.0              0.96     0.95     0.95      854\n\naccuracy                           0.97     8400\nmacro avg        0.97     0.97     0.97     8400\nweighted avg     0.97     0.97     0.97     8400\n</pre> In\u00a0[25]: Copied! <pre>plt.figure(figsize=(8,5))\nsns.heatmap(confusion_matrix(all_targets,all_preds), annot=True, cmap=\"Blues\")\n</pre> plt.figure(figsize=(8,5)) sns.heatmap(confusion_matrix(all_targets,all_preds), annot=True, cmap=\"Blues\") Out[25]: <pre>&lt;Axes: &gt;</pre>"},{"location":"examples/nn/4-digit-recognizer/#mnist-dataset","title":"MNIST Dataset\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/","title":"Preprocessing","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sorix.preprocessing import OneHotEncoder\nfrom sorix.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\nfrom sorix.preprocessing import ColumnTransformer\n</pre> import numpy as np import pandas as pd from sorix.preprocessing import OneHotEncoder from sorix.preprocessing import StandardScaler,MinMaxScaler,RobustScaler from sorix.preprocessing import ColumnTransformer In\u00a0[3]: Copied! <pre>data = {'Edad': [25, 30, 45, 50, 35, 60, 20, 40],\n        'Ingresos': [30000, 50000, 100000, 120000, 70000, 150000, 20000, 80000],\n        'Pais': ['EE. UU.', 'Canad\u00e1', 'M\u00e9xico', 'EE. UU.', 'Canad\u00e1', 'M\u00e9xico', 'EE. UU.', 'Canad\u00e1'],\n        'Ciudades': ['New York', 'Toronto', 'Mexico City', 'New York', 'Toronto', 'Mexico City', 'New York', 'Toronto'],\n        'Compra': [0, 1, 1, 1, 0, 1, 0, 1]}\ndf = pd.DataFrame(data)\n\nX = df.drop('Compra', axis=1)\ny = df['Compra']\nX\n</pre> data = {'Edad': [25, 30, 45, 50, 35, 60, 20, 40],         'Ingresos': [30000, 50000, 100000, 120000, 70000, 150000, 20000, 80000],         'Pais': ['EE. UU.', 'Canad\u00e1', 'M\u00e9xico', 'EE. UU.', 'Canad\u00e1', 'M\u00e9xico', 'EE. UU.', 'Canad\u00e1'],         'Ciudades': ['New York', 'Toronto', 'Mexico City', 'New York', 'Toronto', 'Mexico City', 'New York', 'Toronto'],         'Compra': [0, 1, 1, 1, 0, 1, 0, 1]} df = pd.DataFrame(data)  X = df.drop('Compra', axis=1) y = df['Compra'] X Out[3]: Edad Ingresos Pais Ciudades 0 25 30000 EE. UU. New York 1 30 50000 Canad\u00e1 Toronto 2 45 100000 M\u00e9xico Mexico City 3 50 120000 EE. UU. New York 4 35 70000 Canad\u00e1 Toronto 5 60 150000 M\u00e9xico Mexico City 6 20 20000 EE. UU. New York 7 40 80000 Canad\u00e1 Toronto In\u00a0[4]: Copied! <pre>X.shape\n</pre> X.shape Out[4]: <pre>(8, 4)</pre> In\u00a0[5]: Copied! <pre>categorical_features = ['Pais', 'Ciudades']\nnumeric_features = ['Edad', 'Ingresos'] \n</pre> categorical_features = ['Pais', 'Ciudades'] numeric_features = ['Edad', 'Ingresos']  In\u00a0[6]: Copied! <pre>encoder = OneHotEncoder()\nX_encoded = encoder.fit_transform(X[categorical_features])\npd.DataFrame(X_encoded, columns=encoder.get_features_names())\n</pre> encoder = OneHotEncoder() X_encoded = encoder.fit_transform(X[categorical_features]) pd.DataFrame(X_encoded, columns=encoder.get_features_names()) Out[6]: Pais_Canad\u00e1 Pais_EE. UU. Pais_M\u00e9xico Ciudades_Mexico City Ciudades_New York Ciudades_Toronto 0 0.0 1.0 0.0 0.0 1.0 0.0 1 1.0 0.0 0.0 0.0 0.0 1.0 2 0.0 0.0 1.0 1.0 0.0 0.0 3 0.0 1.0 0.0 0.0 1.0 0.0 4 1.0 0.0 0.0 0.0 0.0 1.0 5 0.0 0.0 1.0 1.0 0.0 0.0 6 0.0 1.0 0.0 0.0 1.0 0.0 7 1.0 0.0 0.0 0.0 0.0 1.0 In\u00a0[7]: Copied! <pre>scaler = StandardScaler() \nX_scaled = scaler.fit_transform(X[numeric_features]) \npd.DataFrame(X_scaled, columns=scaler.get_features_names())\n</pre> scaler = StandardScaler()  X_scaled = scaler.fit_transform(X[numeric_features])  pd.DataFrame(X_scaled, columns=scaler.get_features_names()) Out[7]: Edad Ingresos 0 -1.051315 -1.137500 1 -0.650814 -0.658553 2 0.550689 0.538816 3 0.951190 1.017763 4 -0.250313 -0.179605 5 1.752192 1.736185 6 -1.451816 -1.376974 7 0.150188 0.059868 In\u00a0[8]: Copied! <pre>encoder = OneHotEncoder()\nscaler = StandardScaler() \nX_encoded = encoder.fit_transform(X[categorical_features])\nX_scaled = scaler.fit_transform(X[numeric_features]) \nX_train = np.hstack((X_encoded, X_scaled))\npd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names())\n</pre> encoder = OneHotEncoder() scaler = StandardScaler()  X_encoded = encoder.fit_transform(X[categorical_features]) X_scaled = scaler.fit_transform(X[numeric_features])  X_train = np.hstack((X_encoded, X_scaled)) pd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names()) Out[8]: Pais_Canad\u00e1 Pais_EE. UU. Pais_M\u00e9xico Ciudades_Mexico City Ciudades_New York Ciudades_Toronto Edad Ingresos 0 0.0 1.0 0.0 0.0 1.0 0.0 -1.051315 -1.137500 1 1.0 0.0 0.0 0.0 0.0 1.0 -0.650814 -0.658553 2 0.0 0.0 1.0 1.0 0.0 0.0 0.550689 0.538816 3 0.0 1.0 0.0 0.0 1.0 0.0 0.951190 1.017763 4 1.0 0.0 0.0 0.0 0.0 1.0 -0.250313 -0.179605 5 0.0 0.0 1.0 1.0 0.0 0.0 1.752192 1.736185 6 0.0 1.0 0.0 0.0 1.0 0.0 -1.451816 -1.376974 7 1.0 0.0 0.0 0.0 0.0 1.0 0.150188 0.059868 In\u00a0[9]: Copied! <pre>encoder = OneHotEncoder()\nscaler = MinMaxScaler()\nX_encoded = encoder.fit_transform(X[categorical_features])\nX_scaled = scaler.fit_transform(X[numeric_features])\nX_train = np.hstack((X_encoded, X_scaled))\npd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names())\n</pre> encoder = OneHotEncoder() scaler = MinMaxScaler() X_encoded = encoder.fit_transform(X[categorical_features]) X_scaled = scaler.fit_transform(X[numeric_features]) X_train = np.hstack((X_encoded, X_scaled)) pd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names()) Out[9]: Pais_Canad\u00e1 Pais_EE. UU. Pais_M\u00e9xico Ciudades_Mexico City Ciudades_New York Ciudades_Toronto Edad Ingresos 0 0.0 1.0 0.0 0.0 1.0 0.0 0.125 0.076923 1 1.0 0.0 0.0 0.0 0.0 1.0 0.250 0.230769 2 0.0 0.0 1.0 1.0 0.0 0.0 0.625 0.615385 3 0.0 1.0 0.0 0.0 1.0 0.0 0.750 0.769231 4 1.0 0.0 0.0 0.0 0.0 1.0 0.375 0.384615 5 0.0 0.0 1.0 1.0 0.0 0.0 1.000 1.000000 6 0.0 1.0 0.0 0.0 1.0 0.0 0.000 0.000000 7 1.0 0.0 0.0 0.0 0.0 1.0 0.500 0.461538 In\u00a0[10]: Copied! <pre>encoder = OneHotEncoder()\nscaler = RobustScaler()\nX_encoded = encoder.fit_transform(X[categorical_features])\nX_scaled = scaler.fit_transform(X[numeric_features])\nX_train = np.hstack((X_encoded, X_scaled))\npd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names())\n</pre> encoder = OneHotEncoder() scaler = RobustScaler() X_encoded = encoder.fit_transform(X[categorical_features]) X_scaled = scaler.fit_transform(X[numeric_features]) X_train = np.hstack((X_encoded, X_scaled)) pd.DataFrame(X_train, columns=encoder.get_features_names() + scaler.get_features_names()) Out[10]: Pais_Canad\u00e1 Pais_EE. UU. Pais_M\u00e9xico Ciudades_Mexico City Ciudades_New York Ciudades_Toronto Edad Ingresos 0 0.0 1.0 0.0 0.0 1.0 0.0 -0.714286 -0.750000 1 1.0 0.0 0.0 0.0 0.0 1.0 -0.428571 -0.416667 2 0.0 0.0 1.0 1.0 0.0 0.0 0.428571 0.416667 3 0.0 1.0 0.0 0.0 1.0 0.0 0.714286 0.750000 4 1.0 0.0 0.0 0.0 0.0 1.0 -0.142857 -0.083333 5 0.0 0.0 1.0 1.0 0.0 0.0 1.285714 1.250000 6 0.0 1.0 0.0 0.0 1.0 0.0 -1.000000 -0.916667 7 1.0 0.0 0.0 0.0 0.0 1.0 0.142857 0.083333 In\u00a0[11]: Copied! <pre>column_transformer = ColumnTransformer(\n    transformers = [\n        ('cat', OneHotEncoder(), categorical_features),\n        ('num', StandardScaler(), numeric_features)\n    ]\n)\n\nX_train = column_transformer.fit_transform(X)\n\npd.DataFrame(X_train, columns=column_transformer.get_features_names())\n</pre> column_transformer = ColumnTransformer(     transformers = [         ('cat', OneHotEncoder(), categorical_features),         ('num', StandardScaler(), numeric_features)     ] )  X_train = column_transformer.fit_transform(X)  pd.DataFrame(X_train, columns=column_transformer.get_features_names()) Out[11]: cat_Pais_Canad\u00e1 cat_Pais_EE. UU. cat_Pais_M\u00e9xico cat_Ciudades_Mexico City cat_Ciudades_New York cat_Ciudades_Toronto num_Edad num_Ingresos 0 0.0 1.0 0.0 0.0 1.0 0.0 -1.051315 -1.137500 1 1.0 0.0 0.0 0.0 0.0 1.0 -0.650814 -0.658553 2 0.0 0.0 1.0 1.0 0.0 0.0 0.550689 0.538816 3 0.0 1.0 0.0 0.0 1.0 0.0 0.951190 1.017763 4 1.0 0.0 0.0 0.0 0.0 1.0 -0.250313 -0.179605 5 0.0 0.0 1.0 1.0 0.0 0.0 1.752192 1.736185 6 0.0 1.0 0.0 0.0 1.0 0.0 -1.451816 -1.376974 7 1.0 0.0 0.0 0.0 0.0 1.0 0.150188 0.059868"},{"location":"examples/preprocessing/encoders-scalers-transformers/#preprocessing","title":"Preprocessing\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#one-hot-encoder","title":"One Hot Encoder\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#standard-scaler","title":"Standard Scaler\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#one-hot-ecoder-standard-scaler","title":"One Hot Ecoder + Standard Scaler\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#one-hot-ecoder-minmax-scaler","title":"One Hot Ecoder + MinMax Scaler\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#one-hot-ecoder-robust-scaler","title":"One Hot Ecoder + Robust Scaler\u00b6","text":""},{"location":"examples/preprocessing/encoders-scalers-transformers/#column-transformer","title":"Column Transformer\u00b6","text":""},{"location":"examples/regression/1-linear-regression/","title":"Linear Regression","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport sorix\nfrom sorix.nn import Linear, MSELoss\nfrom sorix.optim import RMSprop, Adam\nfrom sorix.metrics import regression_report,r2_score\nfrom sorix.model_selection import train_test_split\n</pre> import os import numpy as np import matplotlib.pyplot as plt import pandas as pd  import sorix from sorix.nn import Linear, MSELoss from sorix.optim import RMSprop, Adam from sorix.metrics import regression_report,r2_score from sorix.model_selection import train_test_split In\u00a0[3]: Copied! <pre>device = 'cuda' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'cuda' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 14.0.1\n</pre> Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre>n = 10000\nx=np.linspace(2,20,n)\ny=2*x + 1 + 2*np.sin(x) + np.random.randn(n)\n\ndata = pd.DataFrame({'x':x, 'y':y})\ndata.head()\n</pre> n = 10000 x=np.linspace(2,20,n) y=2*x + 1 + 2*np.sin(x) + np.random.randn(n)  data = pd.DataFrame({'x':x, 'y':y}) data.head() Out[4]: x y 0 2.000000 7.159186 1 2.001800 4.045550 2 2.003600 5.935932 3 2.005401 6.201989 4 2.007201 6.444046 In\u00a0[5]: Copied! <pre>plt.figure(figsize=(12,8))\nplt.scatter(data['x'],data['y'],s=50)\n</pre> plt.figure(figsize=(12,8)) plt.scatter(data['x'],data['y'],s=50) Out[5]: <pre>&lt;matplotlib.collections.PathCollection at 0x7fc03d996a50&gt;</pre> In\u00a0[6]: Copied! <pre>data_train, data_test = train_test_split(data, test_size=0.2)\n\nX_train = data_train['x'].values.reshape(-1,1)\ny_train = data_train['y'].values.reshape(-1,1)\n\nX_test = data_test['x'].values.reshape(-1,1)\ny_test = data_test['y'].values.reshape(-1,1)\n</pre> data_train, data_test = train_test_split(data, test_size=0.2)  X_train = data_train['x'].values.reshape(-1,1) y_train = data_train['y'].values.reshape(-1,1)  X_test = data_test['x'].values.reshape(-1,1) y_test = data_test['y'].values.reshape(-1,1) In\u00a0[7]: Copied! <pre>X_train_tensor = sorix.tensor(X_train).to(device)\ny_train_tensor = sorix.tensor(y_train).to(device)\n\nX_test_tensor = sorix.tensor(X_test).to(device)\ny_test_tensor = sorix.tensor(y_test).to(device)\n\nprint(X_train_tensor.shape, y_train_tensor.shape, X_test_tensor.shape, y_test_tensor.shape)\n</pre> X_train_tensor = sorix.tensor(X_train).to(device) y_train_tensor = sorix.tensor(y_train).to(device)  X_test_tensor = sorix.tensor(X_test).to(device) y_test_tensor = sorix.tensor(y_test).to(device)  print(X_train_tensor.shape, y_train_tensor.shape, X_test_tensor.shape, y_test_tensor.shape) <pre>sorix.Size([8000, 1]) sorix.Size([8000, 1]) sorix.Size([2000, 1]) sorix.Size([2000, 1])\n</pre> In\u00a0[8]: Copied! <pre>model = Linear(1,1).to(device)\n\nloss_fn = MSELoss()\noptimizer = Adam(model.parameters(), lr=0.01)\n</pre> model = Linear(1,1).to(device)  loss_fn = MSELoss() optimizer = Adam(model.parameters(), lr=0.01) In\u00a0[9]: Copied! <pre>for itr in range(1000+1):\n    y_pred = model(X_train_tensor)\n    loss = loss_fn(y_pred, y_train_tensor)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if itr % 100 == 0:\n        print(f\"Epoch: {itr:5d} | Loss: {loss.data:.6f}\")\n</pre> for itr in range(1000+1):     y_pred = model(X_train_tensor)     loss = loss_fn(y_pred, y_train_tensor)     optimizer.zero_grad()     loss.backward()     optimizer.step()          if itr % 100 == 0:         print(f\"Epoch: {itr:5d} | Loss: {loss.data:.6f}\") <pre>Epoch:     0 | Loss: 10.089654\n</pre> <pre>Epoch:   100 | Loss: 3.048744\nEpoch:   200 | Loss: 2.966232\n</pre> <pre>Epoch:   300 | Loss: 2.916608\n</pre> <pre>Epoch:   400 | Loss: 2.894161\nEpoch:   500 | Loss: 2.886091\n</pre> <pre>Epoch:   600 | Loss: 2.883740\n</pre> <pre>Epoch:   700 | Loss: 2.883180\nEpoch:   800 | Loss: 2.883071\n</pre> <pre>Epoch:   900 | Loss: 2.883054\n</pre> <pre>Epoch:  1000 | Loss: 2.883052\n</pre> In\u00a0[10]: Copied! <pre>with sorix.no_grad():\n    y_pred_train = model(X_train_tensor)\n    y_pred_test = model(X_test_tensor)\n\nprint(\"Train\")\nprint(regression_report(y_train_tensor, y_pred_train))\nprint(\"\\nTest\")\nprint(regression_report(y_test_tensor, y_pred_test))\n</pre> with sorix.no_grad():     y_pred_train = model(X_train_tensor)     y_pred_test = model(X_test_tensor)  print(\"Train\") print(regression_report(y_train_tensor, y_pred_train)) print(\"\\nTest\") print(regression_report(y_test_tensor, y_pred_test))  <pre>Train\nMetric |     Score |    Range\n-----------------------------\nR2     |    0.9740 | [0,   1]\nMAE    |    1.4127 | [0,  \u221e) \nMSE    |    2.8831 | [0,  \u221e) \nRMSE   |    1.6980 | [0,  \u221e) \nMAPE   |    8.4022 | [0, 100]\n\nTest\nMetric |     Score |    Range\n-----------------------------\nR2     |    0.9733 | [0,   1]\nMAE    |    1.4344 | [0,  \u221e) \nMSE    |    2.9589 | [0,  \u221e) \nRMSE   |    1.7201 | [0,  \u221e) \nMAPE   |    8.5066 | [0, 100]\n</pre> In\u00a0[11]: Copied! <pre>model.coef_, model.intercept_\n</pre> model.coef_, model.intercept_ Out[11]: <pre>(array([1.998056], dtype=float32), 0.9240713715553284)</pre> In\u00a0[12]: Copied! <pre>r2_test = r2_score(y_test_tensor, y_pred_test)\n\nplt.scatter(X_test_tensor,y_test_tensor,s=50)\nplt.scatter(X_test_tensor,y_pred_test,s=50)\nplt.title(f'Linear Regression on Test Data(Accuracy:{r2_test*100:.3f}%)')\nplt.text(5, 5, f'y = {model.coef_[0]:.2f}x + {model.intercept_:.3f}')\n</pre> r2_test = r2_score(y_test_tensor, y_pred_test)  plt.scatter(X_test_tensor,y_test_tensor,s=50) plt.scatter(X_test_tensor,y_pred_test,s=50) plt.title(f'Linear Regression on Test Data(Accuracy:{r2_test*100:.3f}%)') plt.text(5, 5, f'y = {model.coef_[0]:.2f}x + {model.intercept_:.3f}') Out[12]: <pre>Text(5, 5, 'y = 2.00x + 0.924')</pre> In\u00a0[13]: Copied! <pre>sorix.save(model.state_dict(),\"regression_model.sor\")\n</pre> sorix.save(model.state_dict(),\"regression_model.sor\") In\u00a0[14]: Copied! <pre>model2 = Linear(1,1)\nmodel2.load_state_dict(sorix.load(\"regression_model.sor\"))\nmodel2.to(device)\n</pre> model2 = Linear(1,1) model2.load_state_dict(sorix.load(\"regression_model.sor\")) model2.to(device) Out[14]: <pre>Linear(in_features=1, out_features=1, bias=True)</pre> In\u00a0[15]: Copied! <pre>with sorix.no_grad():\n    r2_train = r2_score(y_train_tensor, model2(X_train_tensor))\n    r2_test =  r2_score(y_test_tensor, model2(X_test_tensor))\n    print(f\"R2 Train: {100*r2_train:5.2f} % | R2 Test: {100*r2_test:5.2f} %\")\n</pre> with sorix.no_grad():     r2_train = r2_score(y_train_tensor, model2(X_train_tensor))     r2_test =  r2_score(y_test_tensor, model2(X_test_tensor))     print(f\"R2 Train: {100*r2_train:5.2f} % | R2 Test: {100*r2_test:5.2f} %\") <pre>R2 Train: 97.40 % | R2 Test: 97.33 %\n</pre>"},{"location":"examples/regression/1-linear-regression/#linear-regression","title":"Linear Regression\u00b6","text":""},{"location":"examples/regression/1-linear-regression/#save-model","title":"Save Model\u00b6","text":""},{"location":"examples/regression/2-logistic-regression/","title":"Logistic Regression","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n</pre> import os import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns In\u00a0[3]: Copied! <pre>import sorix\nfrom sorix.model_selection import train_test_split\nfrom sorix.metrics import classification_report, confusion_matrix\nfrom sorix.nn import Linear, BCEWithLogitsLoss\nfrom sorix.optim import RMSprop, Adam\n</pre> import sorix from sorix.model_selection import train_test_split from sorix.metrics import classification_report, confusion_matrix from sorix.nn import Linear, BCEWithLogitsLoss from sorix.optim import RMSprop, Adam In\u00a0[4]: Copied! <pre>device = 'cuda' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'cuda' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 14.0.1\n</pre> Out[4]: <pre>'cuda'</pre> In\u00a0[5]: Copied! <pre>data=pd.read_csv(\"../data/Iris.csv\")\ndata.head()\n</pre> data=pd.read_csv(\"../data/Iris.csv\") data.head() Out[5]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[6]: Copied! <pre>data['Species'].unique()\ndata = data[data['Species'].isin(['Iris-setosa', 'Iris-versicolor'])]\ndata\n</pre> data['Species'].unique() data = data[data['Species'].isin(['Iris-setosa', 'Iris-versicolor'])] data Out[6]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa ... ... ... ... ... ... ... 95 96 5.7 3.0 4.2 1.2 Iris-versicolor 96 97 5.7 2.9 4.2 1.3 Iris-versicolor 97 98 6.2 2.9 4.3 1.3 Iris-versicolor 98 99 5.1 2.5 3.0 1.1 Iris-versicolor 99 100 5.7 2.8 4.1 1.3 Iris-versicolor <p>100 rows \u00d7 6 columns</p> In\u00a0[7]: Copied! <pre>labels2id = {label: i for i, label in enumerate(data['Species'].unique())}\nid2labels = {i: label for i, label in enumerate(data['Species'].unique())}\n</pre> labels2id = {label: i for i, label in enumerate(data['Species'].unique())} id2labels = {i: label for i, label in enumerate(data['Species'].unique())} In\u00a0[8]: Copied! <pre>data['labels']=data['Species'].map(labels2id)\ndata = data[data['labels'].isin([0,1])]\ndata.head()\n</pre> data['labels']=data['Species'].map(labels2id) data = data[data['labels'].isin([0,1])] data.head() Out[8]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species labels 0 1 5.1 3.5 1.4 0.2 Iris-setosa 0 1 2 4.9 3.0 1.4 0.2 Iris-setosa 0 2 3 4.7 3.2 1.3 0.2 Iris-setosa 0 3 4 4.6 3.1 1.5 0.2 Iris-setosa 0 4 5 5.0 3.6 1.4 0.2 Iris-setosa 0 In\u00a0[9]: Copied! <pre>plt.figure(figsize=(10,6))\nfor specie in data['Species'].unique():\n    specie_data = data[data['Species'] == specie]\n    plt.scatter(specie_data['PetalLengthCm'],specie_data['SepalWidthCm'],s=80 ,label=specie )\n    plt.legend()\n    plt.xlabel(\"Petal Length\")\n    plt.ylabel(\"Sepal Width\")\n</pre> plt.figure(figsize=(10,6)) for specie in data['Species'].unique():     specie_data = data[data['Species'] == specie]     plt.scatter(specie_data['PetalLengthCm'],specie_data['SepalWidthCm'],s=80 ,label=specie )     plt.legend()     plt.xlabel(\"Petal Length\")     plt.ylabel(\"Sepal Width\") In\u00a0[10]: Copied! <pre>independent_features=['PetalLengthCm','SepalWidthCm','PetalWidthCm','SepalLengthCm']\ndependent_feature=['labels']\n</pre> independent_features=['PetalLengthCm','SepalWidthCm','PetalWidthCm','SepalLengthCm'] dependent_feature=['labels'] In\u00a0[11]: Copied! <pre>data_train,data_test=train_test_split(data,test_size=0.2)\n\nX_train=sorix.tensor(data_train[independent_features],device=device)\nY_train=sorix.tensor(data_train[dependent_feature],device=device)\n\nX_test=sorix.tensor(data_test[independent_features],device=device)\nY_test=sorix.tensor(data_test[dependent_feature],device=device)\n\nX_train.shape,Y_train.shape,X_test.shape,Y_test.shape\n</pre>  data_train,data_test=train_test_split(data,test_size=0.2)  X_train=sorix.tensor(data_train[independent_features],device=device) Y_train=sorix.tensor(data_train[dependent_feature],device=device)  X_test=sorix.tensor(data_test[independent_features],device=device) Y_test=sorix.tensor(data_test[dependent_feature],device=device)  X_train.shape,Y_train.shape,X_test.shape,Y_test.shape Out[11]: <pre>(sorix.Size([80, 4]),\n sorix.Size([80, 1]),\n sorix.Size([20, 4]),\n sorix.Size([20, 1]))</pre> In\u00a0[12]: Copied! <pre>model = Linear(4, 1).to(device)\n\nloss_fn = BCEWithLogitsLoss()\noptimizer = Adam(model.parameters(), lr=0.01)\n</pre> model = Linear(4, 1).to(device)  loss_fn = BCEWithLogitsLoss() optimizer = Adam(model.parameters(), lr=0.01)  In\u00a0[13]: Copied! <pre>for epoch in range(1000+1):\n    y_pred = model(X_train)\n    loss = loss_fn(y_pred, Y_train)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss.item():.4f}\")\n</pre> for epoch in range(1000+1):     y_pred = model(X_train)     loss = loss_fn(y_pred, Y_train)     optimizer.zero_grad()     loss.backward()     optimizer.step()     if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Loss: {loss.item():.4f}\") <pre>Epoch: 0 | Loss: 1.6742\nEpoch: 100 | Loss: 0.2143\nEpoch: 200 | Loss: 0.1048\nEpoch: 300 | Loss: 0.0634\nEpoch: 400 | Loss: 0.0433\nEpoch: 500 | Loss: 0.0319\nEpoch: 600 | Loss: 0.0247\nEpoch: 700 | Loss: 0.0198\nEpoch: 800 | Loss: 0.0163\nEpoch: 900 | Loss: 0.0137\nEpoch: 1000 | Loss: 0.0117\n</pre> In\u00a0[14]: Copied! <pre>with sorix.no_grad():\n    logits = model(X_test)\nprobs = sorix.sigmoid(logits)\npreds = (probs &gt; 0.5).astype('uint8')\nacc_test = (preds == Y_test).mean()\nplt.scatter(X_test[:,0],X_test[:,1],c=preds)\nplt.title(f\"Logistic Regression on Test Data(Accuracy:{100*acc_test.item():.2f}%)\")\nplt.show()\n</pre> with sorix.no_grad():     logits = model(X_test) probs = sorix.sigmoid(logits) preds = (probs &gt; 0.5).astype('uint8') acc_test = (preds == Y_test).mean() plt.scatter(X_test[:,0],X_test[:,1],c=preds) plt.title(f\"Logistic Regression on Test Data(Accuracy:{100*acc_test.item():.2f}%)\") plt.show() In\u00a0[15]: Copied! <pre>sorix.save(model.state_dict(),\"logistic_model.sor\")\n</pre> sorix.save(model.state_dict(),\"logistic_model.sor\") In\u00a0[16]: Copied! <pre>model2 = Linear(4, 1)\nmodel2.load_state_dict(sorix.load(\"logistic_model.sor\"))\nmodel2.to(device)\n</pre> model2 = Linear(4, 1) model2.load_state_dict(sorix.load(\"logistic_model.sor\")) model2.to(device) Out[16]: <pre>Linear(in_features=4, out_features=1, bias=True)</pre> In\u00a0[17]: Copied! <pre>with sorix.no_grad():\n    logits = model2(X_test)\nprobs = sorix.sigmoid(logits)\npreds = (probs.data &gt; 0.5).astype('uint8')\nacc_test = (preds == Y_test.data).mean()\npreds = preds.get() if device == 'cuda' else preds\n\nplt.scatter(X_test[:,0],X_test[:,1],c=preds)\nplt.title(f\"Logistic Regression on Test Data(Accuracy:{100*acc_test:.2f}%)\")\nplt.show()\n</pre> with sorix.no_grad():     logits = model2(X_test) probs = sorix.sigmoid(logits) preds = (probs.data &gt; 0.5).astype('uint8') acc_test = (preds == Y_test.data).mean() preds = preds.get() if device == 'cuda' else preds  plt.scatter(X_test[:,0],X_test[:,1],c=preds) plt.title(f\"Logistic Regression on Test Data(Accuracy:{100*acc_test:.2f}%)\") plt.show()"},{"location":"examples/regression/2-logistic-regression/#logistic-regression","title":"Logistic Regression\u00b6","text":""},{"location":"examples/regression/2-logistic-regression/#save-model","title":"Save Model\u00b6","text":""},{"location":"learn/basics/01-tensor/","title":"Tensor","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import sorix\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</pre> import sorix import numpy as np import pandas as pd import matplotlib.pyplot as plt In\u00a0[3]: Copied! <pre># from list\na = sorix.tensor([1,2,3])\na\n</pre> # from list a = sorix.tensor([1,2,3]) a Out[3]: <pre>tensor([1, 2, 3])</pre> In\u00a0[4]: Copied! <pre>#from numpy\na = sorix.tensor(np.random.rand(5,5),dtype=sorix.float32)\na\n</pre> #from numpy a = sorix.tensor(np.random.rand(5,5),dtype=sorix.float32) a Out[4]: <pre>tensor([[0.6804612 , 0.23954263, 0.04631325, 0.45137417, 0.40532094],\n        [0.6169146 , 0.9748863 , 0.0304742 , 0.53584003, 0.945324  ],\n        [0.28255406, 0.6641686 , 0.27768067, 0.6058376 , 0.744641  ],\n        [0.0051461 , 0.2487435 , 0.19999287, 0.7273436 , 0.5725697 ],\n        [0.27568677, 0.04579547, 0.7581017 , 0.8880476 , 0.02218847]])</pre> In\u00a0[5]: Copied! <pre># from pandas\n\ndata = pd.DataFrame({\n    'a': [0.464307, 0.182403, 0.664873, 0.906638, 0.725385],\n    'b': [0.278199, 0.187902, 0.887387, 0.473387, 0.904510],\n    'c': [0.793136, 0.957675, 0.035765, 0.639977, 0.622032],\n    'd': [0.618634, 0.784397, 0.841349, 0.352944, 0.783273],\n    'e': [0.729128, 0.467162, 0.687347, 0.432614, 0.980809]\n})\n\nt = sorix.tensor(data)\nt\n</pre> # from pandas  data = pd.DataFrame({     'a': [0.464307, 0.182403, 0.664873, 0.906638, 0.725385],     'b': [0.278199, 0.187902, 0.887387, 0.473387, 0.904510],     'c': [0.793136, 0.957675, 0.035765, 0.639977, 0.622032],     'd': [0.618634, 0.784397, 0.841349, 0.352944, 0.783273],     'e': [0.729128, 0.467162, 0.687347, 0.432614, 0.980809] })  t = sorix.tensor(data) t Out[5]: <pre>tensor([[0.464307, 0.278199, 0.793136, 0.618634, 0.729128],\n        [0.182403, 0.187902, 0.957675, 0.784397, 0.467162],\n        [0.664873, 0.887387, 0.035765, 0.841349, 0.687347],\n        [0.906638, 0.473387, 0.639977, 0.352944, 0.432614],\n        [0.725385, 0.90451 , 0.622032, 0.783273, 0.980809]], dtype=sorix.float64)</pre> <p>To access the underlying NumPy array within a Sorix tensor, you can use the <code>data</code> attribute and apply any NumPy operation directly to it.</p> In\u00a0[6]: Copied! <pre>t.data\n</pre> t.data Out[6]: <pre>array([[0.464307, 0.278199, 0.793136, 0.618634, 0.729128],\n       [0.182403, 0.187902, 0.957675, 0.784397, 0.467162],\n       [0.664873, 0.887387, 0.035765, 0.841349, 0.687347],\n       [0.906638, 0.473387, 0.639977, 0.352944, 0.432614],\n       [0.725385, 0.90451 , 0.622032, 0.783273, 0.980809]])</pre> In\u00a0[7]: Copied! <pre>type(t.data)\n</pre> type(t.data) Out[7]: <pre>numpy.ndarray</pre> In\u00a0[8]: Copied! <pre>t = sorix.as_tensor([1,2,3])\nt\n</pre> t = sorix.as_tensor([1,2,3]) t Out[8]: <pre>tensor([1, 2, 3])</pre> In\u00a0[9]: Copied! <pre>t = sorix.randn(3,4)\nt\n</pre> t = sorix.randn(3,4) t Out[9]: <pre>tensor([[-0.57018307, -0.94452469,  0.59222032, -0.54727245],\n        [ 1.80325528,  0.3600453 ,  2.75890741,  0.16609284],\n        [ 1.30125956,  0.3921614 ,  1.30620691,  1.12232971]], dtype=sorix.float64)</pre> In\u00a0[10]: Copied! <pre>t = sorix.zeros((3,4))\nt\n</pre> t = sorix.zeros((3,4)) t Out[10]: <pre>tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]], dtype=sorix.float64)</pre> In\u00a0[11]: Copied! <pre>t = sorix.ones((3,4))\nt\n</pre> t = sorix.ones((3,4)) t Out[11]: <pre>tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]], dtype=sorix.float64)</pre> In\u00a0[12]: Copied! <pre>t = sorix.eye(3)\nt\n</pre> t = sorix.eye(3) t Out[12]: <pre>tensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]], dtype=sorix.float64)</pre> In\u00a0[13]: Copied! <pre>t = sorix.diag([1,2,3])\nt\n</pre> t = sorix.diag([1,2,3]) t Out[13]: <pre>tensor([[1, 0, 0],\n        [0, 2, 0],\n        [0, 0, 3]])</pre> In\u00a0[14]: Copied! <pre>t = sorix.randint(0,10,(3,4))\nt\n</pre> t = sorix.randint(0,10,(3,4)) t Out[14]: <pre>tensor([[8, 6, 4, 1],\n        [7, 4, 2, 3],\n        [9, 0, 2, 7]])</pre> In\u00a0[15]: Copied! <pre>t = sorix.arange(0,10)\nt\n</pre> t = sorix.arange(0,10) t Out[15]: <pre>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> In\u00a0[16]: Copied! <pre>t = sorix.linspace(0,10,5)\nt\n</pre> t = sorix.linspace(0,10,5) t Out[16]: <pre>tensor([ 0. ,  2.5,  5. ,  7.5, 10. ], dtype=sorix.float64)</pre> In\u00a0[17]: Copied! <pre>t = sorix.logspace(0,10,5)\nt\n</pre> t = sorix.logspace(0,10,5) t Out[17]: <pre>tensor([1.00000000e+00, 3.16227766e+02, 1.00000000e+05, 3.16227766e+07,\n        1.00000000e+10], dtype=sorix.float64)</pre> In\u00a0[18]: Copied! <pre>t = sorix.randperm(5)\nt\n</pre> t = sorix.randperm(5) t Out[18]: <pre>tensor([3, 2, 1, 4, 0])</pre> In\u00a0[19]: Copied! <pre>a = sorix.tensor([1,2,3])\nb = sorix.tensor([3,4,5])\n\nprint(a)\nprint(b)\n</pre> a = sorix.tensor([1,2,3]) b = sorix.tensor([3,4,5])  print(a) print(b) <pre>tensor([1, 2, 3])\ntensor([3, 4, 5])\n</pre> In\u00a0[20]: Copied! <pre>c = a + b\nc\n</pre> c = a + b c Out[20]: <pre>tensor([4, 6, 8])</pre> In\u00a0[21]: Copied! <pre>c = a - b\nc\n</pre> c = a - b c Out[21]: <pre>tensor([-2, -2, -2])</pre> In\u00a0[22]: Copied! <pre>c = a * b\nc\n</pre> c = a * b c Out[22]: <pre>tensor([ 3,  8, 15])</pre> In\u00a0[23]: Copied! <pre>c = a@b\nc\n</pre> c = a@b c Out[23]: <pre>tensor(26)</pre> In\u00a0[24]: Copied! <pre>c = a**2\nc\n</pre> c = a**2 c Out[24]: <pre>tensor([1, 4, 9])</pre> In\u00a0[25]: Copied! <pre>a = sorix.tensor(np.random.rand(5,5))\na\n</pre> a = sorix.tensor(np.random.rand(5,5)) a Out[25]: <pre>tensor([[0.26867713, 0.57266612, 0.3146687 , 0.14633559, 0.11369866],\n        [0.53272895, 0.35194128, 0.37647936, 0.9327882 , 0.73812835],\n        [0.70697115, 0.44447529, 0.39878382, 0.38391531, 0.59910094],\n        [0.17398816, 0.52291319, 0.44279404, 0.45232494, 0.52277749],\n        [0.55449761, 0.72156408, 0.00394382, 0.45557545, 0.92729364]], dtype=sorix.float64)</pre> In\u00a0[26]: Copied! <pre>a[3,:]\n</pre> a[3,:] Out[26]: <pre>tensor([0.17398816, 0.52291319, 0.44279404, 0.45232494, 0.52277749], dtype=sorix.float64)</pre> In\u00a0[27]: Copied! <pre>a[3,3]\n</pre> a[3,3] Out[27]: <pre>tensor(0.45232493)</pre> In\u00a0[28]: Copied! <pre>a[:,3]\n</pre> a[:,3] Out[28]: <pre>tensor([0.14633559, 0.9327882 , 0.38391531, 0.45232494, 0.45557545], dtype=sorix.float64)</pre> In\u00a0[29]: Copied! <pre>device = 'cuda' if sorix.cuda.is_available() else 'cpu'\ndevice\n</pre> device = 'cuda' if sorix.cuda.is_available() else 'cpu' device <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 14.0.1\n</pre> Out[29]: <pre>'cuda'</pre> In\u00a0[30]: Copied! <pre>a = sorix.tensor(np.random.rand(5,5), device=device)\nb = sorix.tensor(np.random.rand(5,5), device=device)\nc = a + b\nc\n</pre> a = sorix.tensor(np.random.rand(5,5), device=device) b = sorix.tensor(np.random.rand(5,5), device=device) c = a + b c Out[30]: <pre>tensor([[1.23187749, 1.43711433, 0.72693987, 1.15844634, 0.47046409],\n        [1.28477485, 1.2041812 , 1.08623305, 0.33980393, 1.44577789],\n        [1.03952929, 1.95658748, 1.42215147, 1.30195997, 0.9172764 ],\n        [0.91563421, 1.73307578, 1.0237092 , 1.46267897, 1.00282323],\n        [0.45580533, 1.57156608, 1.31385239, 1.19407027, 1.25425271]], device='cuda:0', dtype=sorix.float64)</pre>"},{"location":"learn/basics/01-tensor/#tensor","title":"Tensor\u00b6","text":"<p>The Tensor is Sorix's core data structure, analogous to NumPy arrays but with the added ability to record every operation within a computational graph. This operation tracking is what enables the automatic computation of gradients(Autograd).</p>"},{"location":"learn/basics/01-tensor/#create-a-tensor","title":"Create a Tensor\u00b6","text":"<p>A tensor can be initialized from a NumPy array, a pandas DataFrame, or a Python list. Internally, Sorix converts any supported input into a NumPy array.</p>"},{"location":"learn/basics/01-tensor/#sorix-utils-to-create-tensors","title":"Sorix utils to create tensors\u00b6","text":""},{"location":"learn/basics/01-tensor/#basic-operations","title":"Basic Operations\u00b6","text":""},{"location":"learn/basics/01-tensor/#slicing","title":"Slicing\u00b6","text":""},{"location":"learn/basics/01-tensor/#using-gpu","title":"Using GPU\u00b6","text":"<p>When running on a GPU, Sorix uses CuPy arrays instead of NumPy. You can enable GPU execution by setting the <code>device</code> parameter to <code>'cuda'</code> (the default is <code>'cpu'</code>). When <code>'cuda'</code> is specified, Sorix creates CuPy-based tensors and executes all operations on the GPU.</p> <p>To check whether a GPU is available, you can call <code>sorix.cuda.is_available()</code>. Refer to the examples below.</p>"},{"location":"learn/basics/02-graph/","title":"Computational Graph","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>from sorix import tensor\nimport numpy as np\n</pre> from sorix import tensor import numpy as np <p>Defining functions to plot</p> In\u00a0[3]: Copied! <pre>from graphviz import Digraph\nimport shutil\n\ndef trace_graph(root):\n    ops = {}\n    edges = []\n\n    def build(t):\n        tid = id(t)\n\n        if t._op and tid not in ops:\n            ops[tid] = t\n\n        for child in t._prev:\n            edges.append((child, t))\n            build(child)\n\n    build(root)\n    return ops, edges\n\n\ndef tensor_label(t):\n    if hasattr(t, \"numpy\"):\n        arr = t.numpy()\n    elif hasattr(t.data, \"get\"):\n        arr = t.data.get()\n    else:\n        arr = np.array(t.data)\n    if arr.ndim == 0:\n        return str(arr.item())\n    if arr.size &lt;= 6:\n        return str(arr.round(2))\n    return f\"{arr.shape}\"\n\n\ndef draw_graph_ops_forward(root):\n    dot = Digraph(graph_attr={\"rankdir\": \"LR\"})\n\n    ops, edges = trace_graph(root)\n\n    for op_id, tensor_obj in ops.items():\n        label = tensor_obj._op\n        dot.node(str(op_id), label, shape=\"circle\",\n                 style=\"filled\", color=\"#F7DC6F\")\n\n    for parent_tensor, child_tensor in edges:\n        label = tensor_label(parent_tensor)\n\n        edge_color = \"green\" if parent_tensor.requires_grad else \"#888888\"\n\n        if parent_tensor._op:\n            parent_node = str(id(parent_tensor))\n        else:\n            parent_node = f\"INPUT_{id(parent_tensor)}\"\n            dot.node(parent_node, \"\", shape=\"point\", width=\"0.1\")\n\n\n        child_node = str(id(child_tensor))\n\n        dot.edge(parent_node, child_node,\n                 label=f'data={label}', color=edge_color)\n\n    final_label = tensor_label(root)\n    final_color = \"green\" if root.requires_grad else \"#888888\"\n\n    final_node = f\"FINAL_{id(root)}\"\n    dot.node(final_node, \"\", shape=\"point\", width=\"0.1\")\n\n    if root._op:\n        dot.edge(str(id(root)), final_node,\n                 label=final_label, color=final_color)\n\n    if shutil.which(\"dot\") is None:\n        print(\"Warning: 'dot' executable from Graphviz not found. Graph rendering skipped.\")\n        print(\"To see the graph, please install Graphviz (e.g., 'sudo apt install graphviz' on Linux or 'brew install graphviz' on macOS).\")\n        return None\n\n    \n    return dot\n\n\ndef gradient_label(t):\n    \"\"\"Muestra el gradiente del tensor en lugar de su valor\"\"\"\n    if t.grad is None:\n        return \"grad=None\"\n    \n\n    if hasattr(t.grad, \"get\"):\n        grad_arr = t.grad.get()\n    else:\n        grad_arr = np.array(t.grad)\n    \n    if grad_arr.ndim == 0:\n        return f\"grad={grad_arr.item():.2f}\"\n    if grad_arr.size &lt;= 6:\n\n        grad_list = grad_arr.round(2)\n        return f\"grad={grad_list}\"\n    return f\"grad.shape={grad_arr.shape}\"\n\ndef draw_graph_ops_backward(root):\n    dot = Digraph(graph_attr={\"rankdir\": \"RL\"})\n    ops, edges = trace_graph(root)\n    \n\n    for op_id, tensor_obj in ops.items():\n        label = tensor_obj._op\n        dot.node(str(op_id), label, shape=\"circle\",\n                 style=\"filled\", color=\"#F7DC6F\")\n    \n\n    for parent_tensor, child_tensor in edges:\n        label = gradient_label(parent_tensor)\n        \n\n        edge_color = \"green\" if parent_tensor.grad is not None else \"#888888\"\n\n        child_node = str(id(child_tensor))\n\n        if parent_tensor._op:\n            parent_node = str(id(parent_tensor))\n        else:\n            parent_node = f\"INPUT_{id(parent_tensor)}\"\n            dot.node(parent_node, \"\", shape=\"point\", width=\"0.1\")\n\n        dot.edge(child_node, parent_node,\n                 label=label, color=edge_color)\n\n    final_label = gradient_label(root)\n    final_color = \"green\" if root.grad is not None else \"#888888\"\n\n    final_node = f\"FINAL_{id(root)}\"\n    dot.node(final_node, \"\", shape=\"point\", width=\"0.1\")\n    \n    if root._op:\n        dot.edge(final_node, str(id(root)),\n                 label=final_label, color=final_color)\n    \n    if shutil.which(\"dot\") is None:\n        print(\"Warning: 'dot' executable from Graphviz not found. Graph rendering skipped.\")\n        print(\"To see the graph, please install Graphviz (e.g., 'sudo apt install graphviz' on Linux or 'brew install graphviz' on macOS).\")\n        return None\n\n    \n    return dot\n</pre> from graphviz import Digraph import shutil  def trace_graph(root):     ops = {}     edges = []      def build(t):         tid = id(t)          if t._op and tid not in ops:             ops[tid] = t          for child in t._prev:             edges.append((child, t))             build(child)      build(root)     return ops, edges   def tensor_label(t):     if hasattr(t, \"numpy\"):         arr = t.numpy()     elif hasattr(t.data, \"get\"):         arr = t.data.get()     else:         arr = np.array(t.data)     if arr.ndim == 0:         return str(arr.item())     if arr.size &lt;= 6:         return str(arr.round(2))     return f\"{arr.shape}\"   def draw_graph_ops_forward(root):     dot = Digraph(graph_attr={\"rankdir\": \"LR\"})      ops, edges = trace_graph(root)      for op_id, tensor_obj in ops.items():         label = tensor_obj._op         dot.node(str(op_id), label, shape=\"circle\",                  style=\"filled\", color=\"#F7DC6F\")      for parent_tensor, child_tensor in edges:         label = tensor_label(parent_tensor)          edge_color = \"green\" if parent_tensor.requires_grad else \"#888888\"          if parent_tensor._op:             parent_node = str(id(parent_tensor))         else:             parent_node = f\"INPUT_{id(parent_tensor)}\"             dot.node(parent_node, \"\", shape=\"point\", width=\"0.1\")           child_node = str(id(child_tensor))          dot.edge(parent_node, child_node,                  label=f'data={label}', color=edge_color)      final_label = tensor_label(root)     final_color = \"green\" if root.requires_grad else \"#888888\"      final_node = f\"FINAL_{id(root)}\"     dot.node(final_node, \"\", shape=\"point\", width=\"0.1\")      if root._op:         dot.edge(str(id(root)), final_node,                  label=final_label, color=final_color)      if shutil.which(\"dot\") is None:         print(\"Warning: 'dot' executable from Graphviz not found. Graph rendering skipped.\")         print(\"To see the graph, please install Graphviz (e.g., 'sudo apt install graphviz' on Linux or 'brew install graphviz' on macOS).\")         return None           return dot   def gradient_label(t):     \"\"\"Muestra el gradiente del tensor en lugar de su valor\"\"\"     if t.grad is None:         return \"grad=None\"           if hasattr(t.grad, \"get\"):         grad_arr = t.grad.get()     else:         grad_arr = np.array(t.grad)          if grad_arr.ndim == 0:         return f\"grad={grad_arr.item():.2f}\"     if grad_arr.size &lt;= 6:          grad_list = grad_arr.round(2)         return f\"grad={grad_list}\"     return f\"grad.shape={grad_arr.shape}\"  def draw_graph_ops_backward(root):     dot = Digraph(graph_attr={\"rankdir\": \"RL\"})     ops, edges = trace_graph(root)           for op_id, tensor_obj in ops.items():         label = tensor_obj._op         dot.node(str(op_id), label, shape=\"circle\",                  style=\"filled\", color=\"#F7DC6F\")           for parent_tensor, child_tensor in edges:         label = gradient_label(parent_tensor)                   edge_color = \"green\" if parent_tensor.grad is not None else \"#888888\"          child_node = str(id(child_tensor))          if parent_tensor._op:             parent_node = str(id(parent_tensor))         else:             parent_node = f\"INPUT_{id(parent_tensor)}\"             dot.node(parent_node, \"\", shape=\"point\", width=\"0.1\")          dot.edge(child_node, parent_node,                  label=label, color=edge_color)      final_label = gradient_label(root)     final_color = \"green\" if root.grad is not None else \"#888888\"      final_node = f\"FINAL_{id(root)}\"     dot.node(final_node, \"\", shape=\"point\", width=\"0.1\")          if root._op:         dot.edge(final_node, str(id(root)),                  label=final_label, color=final_color)          if shutil.which(\"dot\") is None:         print(\"Warning: 'dot' executable from Graphviz not found. Graph rendering skipped.\")         print(\"To see the graph, please install Graphviz (e.g., 'sudo apt install graphviz' on Linux or 'brew install graphviz' on macOS).\")         return None           return dot In\u00a0[4]: Copied! <pre>x = tensor([3.0], requires_grad=True)\ny = tensor([4.0], requires_grad=True)\n\nf = x**2 + y**2\n\ndraw_graph_ops_forward(f)\n</pre> x = tensor([3.0], requires_grad=True) y = tensor([4.0], requires_grad=True)  f = x**2 + y**2  draw_graph_ops_forward(f)  Out[4]: In\u00a0[5]: Copied! <pre>f.backward()\nprint(f\"df/dx: {x.grad}\")\nprint(f\"df/dy: {y.grad}\")\n</pre>  f.backward() print(f\"df/dx: {x.grad}\") print(f\"df/dy: {y.grad}\") <pre>df/dx: [6.]\ndf/dy: [8.]\n</pre> In\u00a0[6]: Copied! <pre>draw_graph_ops_backward(f)\n</pre> draw_graph_ops_backward(f) Out[6]: In\u00a0[7]: Copied! <pre>x = tensor(np.random.rand(2, 2).round(2), requires_grad=True)\ny = tensor(np.random.rand(2, 2).round(2), requires_grad=True)\n\nf = x**2 @ y**2\n\ndraw_graph_ops_forward(f)\n</pre> x = tensor(np.random.rand(2, 2).round(2), requires_grad=True) y = tensor(np.random.rand(2, 2).round(2), requires_grad=True)  f = x**2 @ y**2  draw_graph_ops_forward(f) Out[7]: In\u00a0[8]: Copied! <pre>f.backward()\n\nprint(f\"df/dx: {x.grad}\")\nprint(f\"df/dy: {y.grad}\")\n</pre> f.backward()  print(f\"df/dx: {x.grad}\") print(f\"df/dy: {y.grad}\") <pre>df/dx: [[1.722   0.13005]\n [0.738   0.05508]]\ndf/dy: [[1.58688  0.58464 ]\n [0.460134 0.102252]]\n</pre> <p>Backward</p> In\u00a0[9]: Copied! <pre>draw_graph_ops_backward(f)\n</pre> draw_graph_ops_backward(f) Out[9]: <p>In summary, the computational graph is the structural foundation that enables Sorix to perform precise, efficient, and fully automatic differentiation without requiring any manual derivation of gradients.</p>"},{"location":"learn/basics/02-graph/#computational-graph","title":"Computational Graph\u00b6","text":"<p>Sorix constructs a directed acyclic computational graph (DAG) that records every operation applied to tensors created with the parameter <code>requires_grad=True</code>. In this graph, each node represents an elementary operation\u2014such as addition, multiplication, exponentiation, or matrix multiplication\u2014while the edges represent the tensors flowing between those operations.</p> <p>This mechanism allows Sorix to capture the complete functional dependency between the input variables and all intermediate expressions. Every time an operation is executed (e.g., <code>x + y</code>, <code>x * y</code>, <code>x**2</code>), Sorix creates a new node in the graph and connects it to the nodes corresponding to the operands involved in the computation.</p> <p>As a result:</p> <ul> <li>The graph encodes the exact sequence of algebraic transformations performed.</li> <li>It preserves the structural dependencies among all tensors participating in the computation.</li> <li>It grows dynamically only when <code>requires_grad=True</code>, avoiding unnecessary overhead when automatic differentiation is not required.</li> </ul>"},{"location":"learn/basics/02-graph/#example","title":"Example\u00b6","text":""},{"location":"learn/basics/02-graph/#the-computational-graph-of-fxy-x2-y2","title":"The computational graph of $f(x,y) = x^2 + y^2$\u00b6","text":"<p>Where:</p> <ul> <li>x = 3</li> <li>y = 4</li> </ul>"},{"location":"learn/basics/02-graph/#foward","title":"Foward\u00b6","text":"<p>Sorix creates a computational graph</p>"},{"location":"learn/basics/02-graph/#backward","title":"Backward\u00b6","text":"<p>When <code>f_output.backward()</code> is executed, Sorix traverses the previously constructed computational graph and applies the chain rule to compute all required derivatives. The resulting partial derivatives are written into the <code>.grad</code> attribute of the input Tensors $x$ and $y$.</p> <ul> <li>$ \\frac{\\partial f}{\\partial x} = 2x = 6.0 $</li> <li>$ \\frac{\\partial f}{\\partial y} = 2y = 8.0 $</li> </ul>"},{"location":"learn/basics/02-graph/#matrix-example","title":"Matrix example\u00b6","text":"<p>Foward</p>"},{"location":"learn/basics/03-autograd/","title":"Autograd","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-2, 2, 100)\n\nX, Y = np.meshgrid(x, y)\nZ = X**2 + Y**2\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis')\n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title(r'$Z=f(X,Y) = X^2 + Y^2$')\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  x = np.linspace(-2, 2, 100) y = np.linspace(-2, 2, 100)  X, Y = np.meshgrid(x, y) Z = X**2 + Y**2  fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot(111, projection='3d') ax.plot_surface(X, Y, Z, cmap='viridis')  ax.set_xlabel('X') ax.set_ylabel('Y') ax.set_title(r'$Z=f(X,Y) = X^2 + Y^2$') plt.show() In\u00a0[3]: Copied! <pre>from sorix import tensor\n\n# Gradient descent hyperparameters\nlr = 0.1\niters = 50\n\n# Initial variables\nx = tensor([3.0], requires_grad=True)\ny = tensor([4.0], requires_grad=True)\n\n# Initial computation\nf = x**2 + y**2\nf.backward()\n\ndx0 = x.grad.item()\ndy0 = y.grad.item()\n\nprint(\"Iter |     x    |     y    |  df/dx   |  df/dy   |   f(x,y)\")\nprint(\"--------------------------------------------------------------------\")\nprint(f\"{0:4d} | {x.item():8.4f} | {y.item():8.4f} | {dx0:8.4f} | {dy0:8.4f} | {f.item():9.4f}\")\n\n# Cleanup before loop\nx.grad = 0\ny.grad = 0\n\nfor i in range(1, iters + 1):\n\n    # Forward compute f(x,y)\n    f = x**2 + y**2\n\n    # Backward compute df/dx and df/dy\n    f.backward()\n    \n    dx = x.grad\n    dy = y.grad\n\n    # Update x and y values\n    x -= lr * x.grad\n    y -= lr * y.grad\n\n    # Periodic print\n    if i % 5 == 0:\n        print(f\"{i:4d} | {x.item():8.4f} | {y.item():8.4f} | {dx.item():8.4f} | {dy.item():8.4f} | {f.item():9.4f}\")\n\n    # Reset\n    x.grad = 0\n    y.grad = 0\n\n# Final result\nf_final = x**2 + y**2\nprint(\"\\nFinal result:\")\nprint(f\"x = {x.item():.6f}\")\nprint(f\"y = {y.item():.6f}\")\nprint(f\"f(x,y) = {f_final.item():.6f}\")\n</pre> from sorix import tensor  # Gradient descent hyperparameters lr = 0.1 iters = 50  # Initial variables x = tensor([3.0], requires_grad=True) y = tensor([4.0], requires_grad=True)  # Initial computation f = x**2 + y**2 f.backward()  dx0 = x.grad.item() dy0 = y.grad.item()  print(\"Iter |     x    |     y    |  df/dx   |  df/dy   |   f(x,y)\") print(\"--------------------------------------------------------------------\") print(f\"{0:4d} | {x.item():8.4f} | {y.item():8.4f} | {dx0:8.4f} | {dy0:8.4f} | {f.item():9.4f}\")  # Cleanup before loop x.grad = 0 y.grad = 0  for i in range(1, iters + 1):      # Forward compute f(x,y)     f = x**2 + y**2      # Backward compute df/dx and df/dy     f.backward()          dx = x.grad     dy = y.grad      # Update x and y values     x -= lr * x.grad     y -= lr * y.grad      # Periodic print     if i % 5 == 0:         print(f\"{i:4d} | {x.item():8.4f} | {y.item():8.4f} | {dx.item():8.4f} | {dy.item():8.4f} | {f.item():9.4f}\")      # Reset     x.grad = 0     y.grad = 0  # Final result f_final = x**2 + y**2 print(\"\\nFinal result:\") print(f\"x = {x.item():.6f}\") print(f\"y = {y.item():.6f}\") print(f\"f(x,y) = {f_final.item():.6f}\")  <pre>Iter |     x    |     y    |  df/dx   |  df/dy   |   f(x,y)\n--------------------------------------------------------------------\n   0 |   3.0000 |   4.0000 |   6.0000 |   8.0000 |   25.0000\n   5 |   0.9830 |   1.3107 |   2.4576 |   3.2768 |    4.1943\n  10 |   0.3221 |   0.4295 |   0.8053 |   1.0737 |    0.4504\n  15 |   0.1056 |   0.1407 |   0.2639 |   0.3518 |    0.0484\n  20 |   0.0346 |   0.0461 |   0.0865 |   0.1153 |    0.0052\n  25 |   0.0113 |   0.0151 |   0.0283 |   0.0378 |    0.0006\n  30 |   0.0037 |   0.0050 |   0.0093 |   0.0124 |    0.0001\n  35 |   0.0012 |   0.0016 |   0.0030 |   0.0041 |    0.0000\n  40 |   0.0004 |   0.0005 |   0.0010 |   0.0013 |    0.0000\n  45 |   0.0001 |   0.0002 |   0.0003 |   0.0004 |    0.0000\n  50 |   0.0000 |   0.0001 |   0.0001 |   0.0001 |    0.0000\n\nFinal result:\nx = 0.000043\ny = 0.000057\nf(x,y) = 0.000000\n</pre> In\u00a0[4]: Copied! <pre>from sorix import tensor\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Gradient descent hyperparameters\nlr = 0.1\niters = 50\n\n# Initial variables\nx = tensor([3.0], requires_grad=True)\ny = tensor([4.0], requires_grad=True)\nf = x**2 + y**2\n\n# History for plotting\nxs = [x.item()]\nys = [y.item()]\nfs = [f.item()]\n\n# Initial evaluation\nf.backward()\n\n# Initial reset\nx.grad = 0\ny.grad = 0\n\nfor i in range(1, iters + 1):\n\n    # Forward\n    f = x**2 + y**2\n\n    # Backward\n    f.backward()\n\n    # Update\n    x -= lr * x.grad\n    y -= lr * y.grad\n\n    # Save trajectory\n    xs.append(x.item())\n    ys.append(y.item())\n    fs.append(f.item())\n\n    # Reset gradients\n    x.grad = 0\n    y.grad = 0\n\n# -----------------------------\n# 3D plot of the surface + trajectory\n# -----------------------------\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\n\n# Surface f(x,y) = x^2 + y^2\nX = np.linspace(-5, 5, 200)\nY = np.linspace(-5, 5, 200)\nXX, YY = np.meshgrid(X, Y)\nZZ = XX**2 + YY**2\n\nax.plot_surface(XX, YY, ZZ, alpha=0.5, cmap='viridis')\n\n# Gradient descent trajectory\nax.plot(xs, ys, fs, marker='o', color='red', markersize=4, label=\"Trajectory\")\n\n# Final point\nax.scatter(xs[-1], ys[-1], fs[-1], color='black', s=60)\n\nax.set_title(\"Gradient Descent on f(x,y) = x\u00b2 + y\u00b2 (3D Plot)\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"f(x,y)\")\nax.legend()\n\nplt.show()\n</pre> from sorix import tensor import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D  # Gradient descent hyperparameters lr = 0.1 iters = 50  # Initial variables x = tensor([3.0], requires_grad=True) y = tensor([4.0], requires_grad=True) f = x**2 + y**2  # History for plotting xs = [x.item()] ys = [y.item()] fs = [f.item()]  # Initial evaluation f.backward()  # Initial reset x.grad = 0 y.grad = 0  for i in range(1, iters + 1):      # Forward     f = x**2 + y**2      # Backward     f.backward()      # Update     x -= lr * x.grad     y -= lr * y.grad      # Save trajectory     xs.append(x.item())     ys.append(y.item())     fs.append(f.item())      # Reset gradients     x.grad = 0     y.grad = 0  # ----------------------------- # 3D plot of the surface + trajectory # ----------------------------- fig = plt.figure(figsize=(8,6)) ax = fig.add_subplot(111, projection='3d')  # Surface f(x,y) = x^2 + y^2 X = np.linspace(-5, 5, 200) Y = np.linspace(-5, 5, 200) XX, YY = np.meshgrid(X, Y) ZZ = XX**2 + YY**2  ax.plot_surface(XX, YY, ZZ, alpha=0.5, cmap='viridis')  # Gradient descent trajectory ax.plot(xs, ys, fs, marker='o', color='red', markersize=4, label=\"Trajectory\")  # Final point ax.scatter(xs[-1], ys[-1], fs[-1], color='black', s=60)  ax.set_title(\"Gradient Descent on f(x,y) = x\u00b2 + y\u00b2 (3D Plot)\") ax.set_xlabel(\"x\") ax.set_ylabel(\"y\") ax.set_zlabel(\"f(x,y)\") ax.legend()  plt.show()"},{"location":"learn/basics/03-autograd/#autograd","title":"Autograd\u00b6","text":""},{"location":"learn/basics/03-autograd/#the-loss-function","title":"The Loss Function\u00b6","text":"<p>In machine learning, the loss function $ J(\\mathbf{w}) $ quantifies how well a model with parameters $ \\mathbf{w} $ fits the data. Formally, it maps the model\u2019s predictions and the true target values to a single real number that measures the discrepancy between them.</p> <p>A smaller value of $ J(\\mathbf{w}) $ indicates better model performance, while large values signal poor predictions. Because learning consists of adjusting the parameters so that the model improves, training a model is equivalent to solving an optimization problem:</p> <p>$$ \\min_{\\mathbf{w}} J(\\mathbf{w}) $$</p>"},{"location":"learn/basics/03-autograd/#why-the-loss-function-becomes-difficult-to-minimize","title":"Why the loss function becomes difficult to minimize\u00b6","text":"<p>In realistic models, especially neural networks, $ J(\\mathbf{w}) $ is not a simple curve but a high-dimensional surface defined over potentially millions of parameters. Each parameter contributes a dimension, which means:</p> <ul> <li>The surface is extremely complex and non-linear.</li> <li>The gradient $ \\nabla J(\\mathbf{w}) $ contains one partial derivative per parameter.</li> <li>The equations $\\frac{\\partial J}{\\partial \\mathbf{w}} = 0$ form a massive coupled system that cannot be solved analytically.</li> </ul> <p>Patterns such as curvature, ridges, valleys, flat regions, and local minima make the surface far too intricate for direct algebraic minimization.</p> <p>For instance, even writing the stationary condition explicitly: $$ \\nabla J(\\mathbf{w}) = 0 $$</p> <p>produces a system of tens of thousands (or millions) of equations with the same number of unknowns\u2014something mathematically intractable to solve in closed form.</p>"},{"location":"learn/basics/03-autograd/#why-iterative-methods-are-required","title":"Why Iterative Methods Are Required\u00b6","text":"<p>Because analytic solutions are impossible for high-dimensional, non-linear loss surfaces, we use iterative numerical optimization methods. These methods do not attempt to solve the system of equations directly. Instead, they progressively update the parameter vector:</p> <p>$$ \\mathbf{w}_0 \\rightarrow \\mathbf{w}_1 \\rightarrow \\mathbf{w}_2 \\rightarrow \\cdots $$</p> <p>moving step-by-step toward regions where $ J(\\mathbf{w})$  decreases.</p> <p>Among these iterative methods, the most fundamental and widely used is Gradient Descent.</p>"},{"location":"learn/basics/03-autograd/#the-gradient-descent-method","title":"The Gradient Descent Method\u00b6","text":"<p>Gradient Descent is a first-order optimization algorithm used to find the local minimum of a function. It is based on the principle that the direction of maximum increase of a function at a point is the direction of the gradient vector ($\\nabla f$).</p> <p>Therefore, the direction of maximum decrease is the opposite of the gradient ($-\\nabla f$).</p>"},{"location":"learn/basics/03-autograd/#algorithm-steps","title":"Algorithm Steps\u00b6","text":"<ol> <li><p>Initialization: Choose a random initial point $\\mathbf{w}_0 $ on the cost function surface $J(\\mathbf{w}) $.</p> </li> <li><p>Gradient Calculation: At the current point $\\mathbf{w}_t $, compute the gradient $\\nabla J(\\mathbf{w}_t)$.</p> </li> <li><p>Parameter Update: Move the parameters in the direction opposite to the gradient, using the update rule:</p> <p>$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\cdot \\nabla J(\\mathbf{w}_t) $$</p> <p>Where:</p> <ul> <li><p>$ \\mathbf{w}_{t+1} $ is the new set of parameters (weights).</p> </li> <li><p>$ \\mathbf{w}_t $ is the current set of parameters.</p> </li> <li><p>$ \\nabla J(\\mathbf{w}_t) $ is the gradient (vector of partial derivatives) of the cost function at $ \\mathbf{w}_t $.</p> </li> <li><p>$ \\eta $ is the learning rate. This is a crucial hyperparameter controlling the size of the \u201cstep\u201d toward the minimum.</p> </li> <li><p>If $ \\eta $ is too large, the algorithm may overshoot the minimum (divergence).</p> </li> <li><p>If $ \\eta $ is too small, convergence becomes very slow.</p> </li> </ul> </li> <li><p>Convergence: This process is repeated iteratively (epochs) until the update steps become negligible (i.e., the gradient approaches zero) or a maximum number of iterations is reached, indicating that a local minimum has been found.</p> </li> </ol>"},{"location":"learn/basics/03-autograd/#sorixs-automatic-differentiation-engine","title":"Sorix\u2019s Automatic Differentiation Engine\u00b6","text":"<p>Sorix provides a fully automated mechanism for computing derivatives through its autograd engine, removing the need to compute gradients manually\u2014an essential capability for optimization methods such as Gradient Descent.</p> <p>At the core of this system is the Tensor, Sorix\u2019s fundamental data structure. While it behaves similarly to a NumPy array, a Tensor can also build a Computational Graph when initialized with <code>requires_grad=True</code>. From that moment, every arithmetic or functional operation is recorded as nodes and edges in the graph, capturing both the intermediate values and the dependencies between them.</p> <p>When the <code>.backward()</code> method is invoked, Sorix traverses this graph in reverse, applying the chain rule to compute the gradients of the output with respect to all Tensors that participated in the computation. This automatic differentiation process makes any function expressed through Sorix Tensors differentiable, enabling efficient and accurate gradient-based optimization without explicitly defining derivative formulas.</p>"},{"location":"learn/basics/03-autograd/#example-finding-the-minimum-of-fx-y-x2-y2","title":"Example: Finding the Minimum of $f(x, y) = x^2 + y^2$\u00b6","text":""},{"location":"learn/basics/03-autograd/#algebraic-search-for-the-minimum-critical-points","title":"Algebraic Search for the Minimum (Critical Points)\u00b6","text":"<p>A critical point is any point where the gradient of the function is zero:</p> <p>$$ \\nabla f = \\mathbf{0}. $$</p> <p>Computing the partial derivatives:</p> <p>$$ \\frac{\\partial f}{\\partial x} = 2x = 0 \\quad \\Rightarrow \\quad x = 0, $$ $$ \\frac{\\partial f}{\\partial y} = 2y = 0 \\quad \\Rightarrow \\quad y = 0. $$</p> <p>Thus, the only critical point is $(0,0)$. Evaluating the function there confirms it is the global minimum:</p> <p>$$ f(0,0)=0. $$</p> <p>This direct solution is possible because $f(x,y)$ is simple, convex, and differentiable everywhere.</p>"},{"location":"learn/basics/03-autograd/#finding-the-minimum-of-fxyx2y2-with-gradient-descent","title":"Finding the Minimum of $ f(x,y)=x^2+y^2 $ with Gradient Descent\u00b6","text":"<p>The gradient descent update rule is:</p> <p>$$ (x,y)_{t+1} = (x,y)_t - \\eta \\nabla f(x,y), \\qquad t=0,1,2,\\dots,n. $$</p> <p>$$ (x,y)_{t+1} = (x,y)_t - 2\\eta(x,y)_t, \\qquad t=0,1,2,\\dots,n. $$</p> <p>Given:</p> <ul> <li>Initial point: $ (x,y)_0 = (3,4) $</li> <li>Learning rate($lr$): $ \\eta = 0.1 $</li> </ul> <p>the algorithm progressively moves the parameters toward the origin, decreasing the value of the function at each step.</p>"},{"location":"learn/basics/03-autograd/#graphically","title":"Graphically\u00b6","text":""},{"location":"learn/basics/03-autograd/#explanation-of-the-gradient-descent-plot","title":"Explanation of the Gradient Descent Plot\u00b6","text":"<p>The plot depicts the iterative behavior of gradient descent applied to $f(x,y)=x^{2}+y^{2}$. The surface represents the values of $f(x,y)$ over the plane, forming a convex paraboloid in which elevated regions correspond to larger function values and the lowest point at the center corresponds to the global minimum at $(0,0)$.</p> <p>The red trajectory shows the sequence of iterates $(x_t,y_t)$. It begins at $(3.0,4.0)$, located on a high region of the surface where $f(x,y)$ is large. At each step, the algorithm updates the variables in the direction opposite to the gradient, producing a monotonic descent toward smaller values of $f(x,y)$. The initial segments are long because the gradient is large in steep regions, while the steps progressively shorten as the iterates approach the origin, where the surface becomes flatter and the gradient diminishes. The trajectory follows an almost radial path, reflecting the symmetry of the function.</p> <p>The final point lies essentially at $(0,0)$, indicating convergence to the global minimum where $f(x,y)=0$.</p>"},{"location":"learn/layers/","title":"Layers","text":"<p>The layers are implemented as Python classes that encapsulate one or more fundamental tensor operations. Each layer defines its trainable parameters as class attributes represented by <code>tensor</code> objects, which are initialized with <code>requires_grad=True</code> by default to enable automatic differentiation.</p> <p>These layers are designed to be composable and device-aware, supporting execution on both CPU and GPU backends. Parameter initialization follows standard schemes (e.g., He or Xavier initialization), and all learnable parameters are exposed through a unified interface for optimization. Non-trainable quantities, such as running statistics in normalization layers, are handled as buffers and are therefore excluded from gradient computation.</p> <p>At a high level, the available layers include linear transformations, nonlinear activation functions, and normalization modules. Each class defines the forward computation via the <code>__call__</code> interface, while gradient propagation is handled internally through the underlying tensor autograd mechanism. </p>"},{"location":"learn/layers/#available-layers","title":"Available Layers","text":"<p>Sorix provides several fundamental layers:</p> <ul> <li>Linear Layer: Implements standard fully-connected transformations.</li> <li>BatchNorm1d: Normalizes inputs based on batch statistics.</li> <li>ReLU Activation: The Rectified Linear Unit activation function.</li> <li>Sigmoid Activation: S-shaped activation function with a numerically stable implementation.</li> <li>Tanh Activation: Hyperbolic tangent activation function.</li> <li>Dropout: Training logic for regularization.</li> </ul> <p>If you want to implement your own layer, check out the Module Base Class documentation.</p> <p>Detailed mathematical formulations and implementation specifics of each layer can be found in the notebooks linked above.</p>"},{"location":"learn/layers/01-Linear/","title":"Linear","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>from sorix import tensor\nfrom sorix.nn import Linear\nimport numpy as np\n</pre> from sorix import tensor from sorix.nn import Linear import numpy as np In\u00a0[3]: Copied! <pre># create random input data\nsamples = 10\nfeatures = 3\nneurons = 2\n\n# X \u2208 \u211d^(samples \u00d7 features)\nX = tensor(np.random.randn(samples, features))\nX\n</pre> # create random input data samples = 10 features = 3 neurons = 2  # X \u2208 \u211d^(samples \u00d7 features) X = tensor(np.random.randn(samples, features)) X Out[3]: <pre>tensor([[-0.68245878, -1.51091108, -1.81919191],\n        [-0.71944098, -0.29123973, -0.59063962],\n        [-0.5165242 ,  1.26550573, -0.3165631 ],\n        [-0.47727734, -0.8987275 ,  0.23155946],\n        [-0.28909791, -0.80714485, -0.93510151],\n        [ 0.77912457, -0.85347223, -1.664577  ],\n        [ 0.63505287,  2.09173128,  0.52771808],\n        [-0.50557509, -1.37366385,  0.7768541 ],\n        [-0.42453686, -0.65460723, -1.43567044],\n        [ 2.43236421, -0.539992  , -0.1724795 ]], dtype=sorix.float64)</pre> In\u00a0[4]: Copied! <pre># instantiate a Linear layer: \u211d^(samples \u00d7 features) \u2192 \u211d^(samples \u00d7 neurons)\nlinear = Linear(features, neurons)\n\n# weight matrix W \u2208 \u211d^(features \u00d7 neurons)\nprint(linear.W)\n\n# bias vector b \u2208 \u211d^(1 \u00d7 neurons)\nprint(linear.b)\n</pre> # instantiate a Linear layer: \u211d^(samples \u00d7 features) \u2192 \u211d^(samples \u00d7 neurons) linear = Linear(features, neurons)  # weight matrix W \u2208 \u211d^(features \u00d7 neurons) print(linear.W)  # bias vector b \u2208 \u211d^(1 \u00d7 neurons) print(linear.b) <pre>tensor([[ 0.36082658, -0.7080932 ],\n        [-1.5694878 ,  0.30932558],\n        [ 0.11454388, -1.246289  ]], requires_grad=True)\ntensor([[0., 0.]], requires_grad=True)\n</pre> In\u00a0[5]: Copied! <pre># forward pass:\n# Y \u2208 \u211d^(samples \u00d7 neurons) = X @ W + b\nY = linear(X)\nprint(Y)\n</pre> # forward pass: # Y \u2208 \u211d^(samples \u00d7 neurons) = X @ W + b Y = linear(X) print(Y)  <pre>tensor([[ 1.91672996,  2.28311989],\n        [ 0.12984963,  1.15545106],\n        [-2.20883185,  1.1517297 ],\n        [ 1.26485123, -0.22863256],\n        [ 1.05537964,  1.12044447],\n        [ 1.42997601,  1.25885041],\n        [-2.99335591, -0.4603399 ],\n        [ 2.06250762, -1.0350998 ],\n        [ 0.70976662,  1.88738522],\n        [ 1.70541605, -1.67441465]], dtype=sorix.float64, requires_grad=True)\n</pre>"},{"location":"learn/layers/01-Linear/#linear","title":"Linear\u00b6","text":"<p>The Linear layer implements an affine transformation between finite-dimensional real vector spaces and constitutes a fundamental operator in deep learning architectures. Formally, it defines a linear mapping from an input feature space to an output representation space, optionally augmented by a bias term. This transformation is applied independently to each element of a batch.</p>"},{"location":"learn/layers/01-Linear/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>Let  $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ be an input tensor representing a batch of $N$ samples, where each sample is a vector in a $d$-dimensional feature space. The Linear layer defines the affine transformation</p> <p>$$\\mathbf{Y} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}$$</p> <p>where the involved quantities have the following dimensions:</p> <ul> <li>$ \\mathbf{X} \\in \\mathbb{R}^{N \\times d} $ : input batch matrix</li> <li>$ \\mathbf{W} \\in \\mathbb{R}^{d \\times m} $ : weight matrix (trainable parameters)</li> <li>$ \\mathbf{b} \\in \\mathbb{R}^{1 \\times m} $ : bias vector associated with the output neurons</li> <li>$ \\mathbf{Y} \\in \\mathbb{R}^{N \\times m} $ : output tensor</li> <li>$ m $ : number of neurons, i.e., the dimensionality of the output space</li> </ul> <p>From a dimensional analysis standpoint, the matrix product</p> <p>$$ \\mathbf{X}\\mathbf{W} : \\mathbb{R}^{N \\times d} \\times \\mathbb{R}^{d \\times m} \\;\\longrightarrow\\; \\mathbb{R}^{N \\times m} $$</p> <p>is well-defined. The bias term $\\mathbf{b}$ is then added column-wise to the resulting matrix, meaning that each component $b_j$ is added to all entries of the $j$-th output column. Explicitly,</p> <p>$$ Y_{ij} = (\\mathbf{X}\\mathbf{W})_{ij} + b_j, \\quad i = 1,\\dots,N,\\; j = 1,\\dots,m. $$</p>"},{"location":"learn/layers/01-Linear/#interpretation-as-a-linear-mapping","title":"Interpretation as a linear mapping\u00b6","text":"<p>At the level of individual samples, for each $ i \\in \\{1, \\dots, N\\}, $ the transformation can be written as</p> <p>$$ \\mathbf{y}_i = \\mathbf{x}_i \\mathbf{W} + \\mathbf{b}, \\quad \\mathbf{x}_i \\in \\mathbb{R}^{1 \\times d},\\; \\mathbf{y}_i \\in \\mathbb{R}^{1 \\times m}. $$</p> <p>Thus, each output vector $\\mathbf{y}_i$ is obtained as a linear combination of the input features, defined by the columns of $\\mathbf{W}$, followed by a translation in the output space determined by the bias vector $\\mathbf{b}$.</p>"},{"location":"learn/layers/01-Linear/#functional-view","title":"Functional view\u00b6","text":"<p>The Linear layer realizes the mapping</p> <p>$$ \\text{Linear}:\\; \\mathbb{R}^{N \\times d} \\;\\longrightarrow\\; \\mathbb{R}^{N \\times m}, $$</p> <p>where the same affine transformation is applied independently to each sample in the batch. This operator forms the mathematical foundation upon which more complex nonlinear models are constructed when composed with activation and normalization layers.</p>"},{"location":"learn/layers/01-Linear/#parameterization-and-gradients","title":"Parameterization and gradients\u00b6","text":"<p>The parameters $\\mathbf{W}$ and $\\mathbf{b}$ are represented as <code>tensor</code> objects with <code>requires_grad=True</code>, enabling automatic gradient computation via automatic differentiation. During backpropagation, the following gradients are computed:</p> <p>$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}}, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}}, $$</p> <p>where $\\mathcal{L}$ denotes the global loss function of the model.</p>"},{"location":"learn/layers/01-Linear/#parameter-initialization","title":"Parameter initialization\u00b6","text":"<p>The weight matrix $\\mathbf{W}$ is initialized from a zero-mean normal distribution with a standard deviation determined by the chosen initialization scheme:</p> <ul> <li><p>He initialization (recommended for ReLU-like activations): $$ \\sigma = \\sqrt{\\frac{2}{d}}. $$</p> </li> <li><p>Xavier initialization (suitable for symmetric activations such as $\\tanh$): $$ \\sigma = \\sqrt{\\frac{2}{d + m}}. $$</p> </li> </ul> <p>Formally, $$ W_{ij} \\sim \\mathcal{N}(0, \\sigma^2). $$</p> <p>When present, the bias vector $\\mathbf{b}$ is initialized to zero.</p>"},{"location":"learn/layers/01-Linear/#forward-computation","title":"Forward computation\u00b6","text":"<p>Given an input tensor $\\mathbf{X}$, the forward evaluation of the layer is performed through the matrix operation</p> <p>$$ \\text{Linear}(\\mathbf{X}) = \\mathbf{X}\\mathbf{W} + \\mathbf{b}. $$</p> <p>In the implementation, this computation is exposed via the <code>__call__</code> method, enabling a concise and functional syntax consistent with the rest of the framework.</p>"},{"location":"learn/layers/01-Linear/#multi-device-support","title":"Multi-device support\u00b6","text":"<p>The Linear layer is device-aware. Parameters and computations may reside on either CPU or GPU, using NumPy or CuPy as the numerical backend, respectively. The <code>to(device)</code> method ensures consistent parameter transfer across devices while preserving the mathematical semantics of the transformation.</p>"},{"location":"learn/layers/01-Linear/#parameter-interface","title":"Parameter interface\u00b6","text":"<p>The trainable parameters of the layer are exposed through the <code>parameters()</code> method, which returns the set</p> <p>$$ \\{\\mathbf{W}, \\mathbf{b}\\}, $$</p> <p>or only $\\mathbf{W}$ when the bias term is disabled. This abstraction allows direct integration with gradient-based optimization algorithms.</p>"},{"location":"learn/layers/01-Linear/#statistical-interpretation","title":"Statistical interpretation\u00b6","text":"<p>From a statistical perspective, the Linear layer can be interpreted as a multivariate linear regression model, where each output neuron represents a linear combination of the input features. In this context, the coefficients of the weight matrix $\\mathbf{W}$ and the bias vector $\\mathbf{b}$ define hyperplanes in the output space that approximate the relationship between input and output variables.</p>"},{"location":"learn/layers/02-BatchNorm1d/","title":"BatchNorm1d","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>from sorix import tensor\nfrom sorix.nn import BatchNorm1d\nimport numpy as np\n</pre> from sorix import tensor from sorix.nn import BatchNorm1d import numpy as np In\u00a0[3]: Copied! <pre># number of samples and features\nsamples = 8\nfeatures = 3\n\n# input tensor X \u2208 \u211d^(samples \u00d7 features)\nX = tensor(np.random.randn(samples, features))\nX\n</pre> # number of samples and features samples = 8 features = 3  # input tensor X \u2208 \u211d^(samples \u00d7 features) X = tensor(np.random.randn(samples, features)) X  Out[3]: <pre>tensor([[-0.79076557, -0.09530421, -2.24122608],\n        [ 0.48085172, -0.62549223, -2.1529319 ],\n        [-0.13736248,  0.21993719,  0.82125192],\n        [-0.33432386, -0.21491704,  0.07399757],\n        [-0.3230639 ,  0.97823966,  0.14454357],\n        [-0.24306372, -1.85875525,  0.32994193],\n        [ 1.22507434,  0.33410779, -1.34611515],\n        [-0.16913842,  0.05868427, -0.04777623]], dtype=sorix.float64)</pre> In\u00a0[4]: Copied! <pre>bn = BatchNorm1d(features)\n\n# \u03b3 \u2208 \u211d^(1 \u00d7 features), \u03b2 \u2208 \u211d^(1 \u00d7 features)\nprint(bn.gamma)\nprint(bn.beta)\n</pre> bn = BatchNorm1d(features)  # \u03b3 \u2208 \u211d^(1 \u00d7 features), \u03b2 \u2208 \u211d^(1 \u00d7 features) print(bn.gamma) print(bn.beta)  <pre>tensor([[1., 1., 1.]], requires_grad=True)\ntensor([[0., 0., 0.]], requires_grad=True)\n</pre> In\u00a0[5]: Copied! <pre># forward pass (training mode)\nY = bn(X)\nY\n</pre> # forward pass (training mode) Y = bn(X) Y  Out[5]: <pre>tensor([[-1.30578473,  0.07087535, -1.52270086],\n        [ 0.89556349, -0.61069615, -1.44309716],\n        [-0.17465216,  0.47612697,  1.23834854],\n        [-0.51561998, -0.08289027,  0.56464372],\n        [-0.49612741,  1.45094598,  0.62824614],\n        [-0.35763586, -2.19609021,  0.79539641],\n        [ 2.18391742,  0.62289647, -0.71569245],\n        [-0.22966077,  0.26883185,  0.45485567]], dtype=sorix.float64, requires_grad=True)</pre> In\u00a0[6]: Copied! <pre># running statistics after the forward pass\nprint(bn.running_mean)\nprint(bn.running_var)\n</pre> # running statistics after the forward pass print(bn.running_mean) print(bn.running_var)  <pre>tensor([[-0.0036474 , -0.01504375, -0.05522893]], dtype=sorix.float64)\ntensor([[0.93336737, 0.96051034, 1.02302517]], dtype=sorix.float64)\n</pre> In\u00a0[7]: Copied! <pre># inference mode\nbn.training = False\nY_eval = bn(X)\nY_eval\n</pre> # inference mode bn.training = False Y_eval = bn(X) Y_eval  Out[7]: <pre>tensor([[-0.81472549, -0.0818933 , -2.16124653],\n        [ 0.5014924 , -0.62286759, -2.07395204],\n        [-0.13840499,  0.23976144,  0.86655703],\n        [-0.34227458, -0.20393956,  0.12776335],\n        [-0.33061969,  1.013491  ,  0.19751061],\n        [-0.24781359, -1.88122041,  0.38080982],\n        [ 1.27181782,  0.35625476, -1.27627035],\n        [-0.17129544,  0.07522796,  0.00736832]], dtype=sorix.float64, requires_grad=True)</pre>"},{"location":"learn/layers/02-BatchNorm1d/#batchnorm1d","title":"BatchNorm1d\u00b6","text":"<p>The BatchNorm1d layer implements batch normalization, a technique designed to stabilize and accelerate the training of deep neural networks by reducing internal covariate shift. This is achieved by normalizing intermediate activations across the batch dimension and subsequently applying a learnable affine transformation.</p>"},{"location":"learn/layers/02-BatchNorm1d/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>Let $ \\mathbf{X} \\in \\mathbb{R}^{N \\times d} $ be an input tensor representing a batch of $N$ samples, where each sample has $d$ features. Batch normalization operates feature-wise, normalizing each feature independently across the batch.</p> <p>During training, the batch-wise mean and variance are computed as</p> <p>$$ \\boldsymbol{\\mu}_B = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{x}_i \\;\\in\\; \\mathbb{R}^{1 \\times d}, $$</p> <p>$$ \\boldsymbol{\\sigma}_B^2 = \\frac{1}{N} \\sum_{i=1}^{N} (\\mathbf{x}_i - \\boldsymbol{\\mu}_B)^2 \\;\\in\\; \\mathbb{R}^{1 \\times d}, $$</p> <p>where $\\mathbf{x}_i \\in \\mathbb{R}^{1 \\times d}$ denotes the $i$-th sample in the batch.</p>"},{"location":"learn/layers/02-BatchNorm1d/#normalization-step","title":"Normalization step\u00b6","text":"<p>Each input sample is normalized using the batch statistics:</p> <p>$$ \\widehat{\\mathbf{X}} = \\frac{\\mathbf{X} - \\boldsymbol{\\mu}_B} {\\sqrt{\\boldsymbol{\\sigma}_B^2 + \\varepsilon}}, \\quad \\widehat{\\mathbf{X}} \\in \\mathbb{R}^{N \\times d}, $$</p> <p>where $\\varepsilon &gt; 0$ is a small constant introduced for numerical stability.</p>"},{"location":"learn/layers/02-BatchNorm1d/#learnable-affine-transformation","title":"Learnable affine transformation\u00b6","text":"<p>To preserve the representational capacity of the network, batch normalization introduces two learnable parameters:</p> <ul> <li>Scale parameter: $ \\boldsymbol{\\gamma} \\in \\mathbb{R}^{1 \\times d} $</li> <li>Shift parameter: $ \\boldsymbol{\\beta} \\in \\mathbb{R}^{1 \\times d} $</li> </ul> <p>The final output of the layer is given by</p> <p>$$ \\mathbf{Y} = \\boldsymbol{\\gamma} \\odot \\widehat{\\mathbf{X}} + \\boldsymbol{\\beta}, \\quad \\mathbf{Y} \\in \\mathbb{R}^{N \\times d}, $$</p> <p>where $\\odot$ denotes element-wise multiplication applied column-wise, i.e., independently to each feature.</p>"},{"location":"learn/layers/02-BatchNorm1d/#running-statistics-and-inference-mode","title":"Running statistics and inference mode\u00b6","text":"<p>In addition to batch statistics, BatchNorm1d maintains running estimates of the mean and variance:</p> <p>$$ \\boldsymbol{\\mu}_{\\text{run}} \\in \\mathbb{R}^{1 \\times d}, \\quad \\boldsymbol{\\sigma}^2_{\\text{run}} \\in \\mathbb{R}^{1 \\times d}. $$</p> <p>These statistics are updated during training using an exponential moving average:</p> <p>$$ \\boldsymbol{\\mu}_{\\text{run}} \\leftarrow \\alpha \\boldsymbol{\\mu}_{\\text{run}} + (1 - \\alpha)\\boldsymbol{\\mu}_B, $$</p> <p>$$ \\boldsymbol{\\sigma}^2_{\\text{run}} \\leftarrow \\alpha \\boldsymbol{\\sigma}^2_{\\text{run}} + (1 - \\alpha)\\boldsymbol{\\sigma}^2_B, $$</p> <p>where $\\alpha \\in (0,1)$ is the momentum parameter controlling the update rate.</p> <p>During inference (evaluation mode), normalization is performed using these accumulated running statistics instead of the batch statistics, ensuring deterministic behavior:</p> <p>$$ \\widehat{\\mathbf{X}} = \\frac{\\mathbf{X} - \\boldsymbol{\\mu}_{\\text{run}}} {\\sqrt{\\boldsymbol{\\sigma}^2_{\\text{run}} + \\varepsilon}}. $$</p>"},{"location":"learn/layers/02-BatchNorm1d/#functional-view","title":"Functional view\u00b6","text":"<p>The BatchNorm1d layer realizes the mapping</p> <p>$$ \\text{BatchNorm1d}:\\; \\mathbb{R}^{N \\times d} \\;\\longrightarrow\\; \\mathbb{R}^{N \\times d}, $$</p> <p>where normalization and affine reparameterization are applied independently to each feature across the batch.</p>"},{"location":"learn/layers/02-BatchNorm1d/#parameterization-and-gradients","title":"Parameterization and gradients\u00b6","text":"<p>The learnable parameters $\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\beta}$ are represented as <code>tensor</code> objects with <code>requires_grad=True</code>. Gradients are computed with respect to these parameters, as well as with respect to the input tensor $\\mathbf{X}$, during backpropagation:</p> <p>$$ \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\gamma}}, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\beta}}, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}}. $$</p> <p>The running statistics are treated as buffers and do not participate in gradient computation.</p>"},{"location":"learn/layers/02-BatchNorm1d/#multi-device-support","title":"Multi-device support\u00b6","text":"<p>BatchNorm1d is device-aware and supports execution on CPU and GPU backends. Learnable parameters are stored as tensors on the selected device, while running statistics are maintained as NumPy or CuPy arrays and transferred consistently when changing devices via the <code>to(device)</code> method.</p>"},{"location":"learn/layers/02-BatchNorm1d/#parameter-interface","title":"Parameter interface\u00b6","text":"<p>The trainable parameters of the layer are exposed through the <code>parameters()</code> method, which returns</p> <p>$$ \\{\\boldsymbol{\\gamma}, \\boldsymbol{\\beta}\\}. $$</p>"},{"location":"learn/layers/02-BatchNorm1d/#statistical-interpretation","title":"Statistical interpretation\u00b6","text":"<p>From a statistical perspective, BatchNorm1d performs a feature-wise standardization of the input distribution, followed by a learned affine transformation. This can be interpreted as dynamically re-centering and re-scaling the feature space, which improves numerical conditioning and facilitates optimization in deep networks.</p>"},{"location":"learn/layers/03-ReLU/","title":"ReLU","text":"In\u00a0[34]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[35]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sorix import tensor\nfrom sorix.nn import ReLU\nimport sorix\n\nplt.style.use('ggplot')\n</pre> import numpy as np import matplotlib.pyplot as plt from sorix import tensor from sorix.nn import ReLU import sorix  plt.style.use('ggplot') In\u00a0[39]: Copied! <pre>x_vals = np.linspace(-5, 5, 100)\nX = tensor(x_vals, requires_grad=True)\nrelu = ReLU()\nY = relu(X)\n\nplt.figure(figsize=(10, 5))\nplt.plot(x_vals, Y.numpy(), label='$ReLU(x) = \\\\max(0, x)$', color='#e74c3c', lw=2)\nplt.title(\"ReLU Activation Function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"$\\\\max(0, x)$\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n</pre> x_vals = np.linspace(-5, 5, 100) X = tensor(x_vals, requires_grad=True) relu = ReLU() Y = relu(X)  plt.figure(figsize=(10, 5)) plt.plot(x_vals, Y.numpy(), label='$ReLU(x) = \\\\max(0, x)$', color='#e74c3c', lw=2) plt.title(\"ReLU Activation Function\") plt.xlabel(\"x\") plt.ylabel(\"$\\\\max(0, x)$\") plt.grid(True, alpha=0.3) plt.legend() plt.show() In\u00a0[37]: Copied! <pre>X = tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\nY = relu(X)\nY.backward()\n\nprint(f\"Input:     {X.numpy()}\")\nprint(f\"Output:    {Y.numpy()}\")\nprint(f\"Gradients: {X.grad}\")\n</pre> X = tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True) Y = relu(X) Y.backward()  print(f\"Input:     {X.numpy()}\") print(f\"Output:    {Y.numpy()}\") print(f\"Gradients: {X.grad}\") <pre>Input:     [-2. -1.  0.  1.  2.]\nOutput:    [0. 0. 0. 1. 2.]\nGradients: [0. 0. 0. 1. 1.]\n</pre>"},{"location":"learn/layers/03-ReLU/#relu","title":"ReLU\u00b6","text":"<p>The ReLU layer implements the Rectified Linear Unit activation function, one of the most widely used nonlinearities in deep learning due to its simplicity, computational efficiency, and favorable gradient properties. ReLU introduces nonlinearity by applying an element-wise thresholding operation to its input.</p>"},{"location":"learn/layers/03-ReLU/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>Let $ \\mathbf{X} \\in \\mathbb{R}^{N \\times d} $ be an input tensor representing a batch of $N$ samples with $d$ features. The ReLU activation is defined element-wise as</p> <p>$$ \\operatorname{ReLU}(x) = \\max(0, x). $$</p> <p>Applied to a tensor, this yields</p> <p>$$ \\mathbf{Y} = \\operatorname{ReLU}(\\mathbf{X}), \\quad Y_{ij} = \\max(0, X_{ij}) $$</p>"},{"location":"learn/layers/03-ReLU/#backward-computation-gradient","title":"Backward computation (gradient)\u00b6","text":"<p>The derivative of the ReLU function is given by</p> <p>$$ \\frac{d}{dx} \\operatorname{ReLU}(x) = \\begin{cases} 1, &amp; x &gt; 0, \\\\ 0, &amp; x \\le 0. \\end{cases} $$</p>"},{"location":"learn/layers/03-ReLU/#visualizing-relu","title":"Visualizing ReLU\u00b6","text":""},{"location":"learn/layers/03-ReLU/#functional-example","title":"Functional Example\u00b6","text":""},{"location":"learn/layers/04-Sigmoid/","title":"Sigmoid","text":"In\u00a0[9]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[10]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sorix import tensor\nfrom sorix.nn import Sigmoid\nimport sorix\n\n# Modern plot style\nplt.style.use('ggplot')\n</pre> import numpy as np import matplotlib.pyplot as plt from sorix import tensor from sorix.nn import Sigmoid import sorix  # Modern plot style plt.style.use('ggplot') In\u00a0[13]: Copied! <pre>x_vals = np.linspace(-10, 10, 100)\nX = tensor(x_vals, requires_grad=True)\nsigmoid = Sigmoid()\nY = sigmoid(X)\n\nplt.figure(figsize=(10, 5))\nplt.plot(x_vals, Y.numpy(), label='$Sigmoid(x) = \\\\sigma(x)$', color='#f39c12', lw=2)\nplt.title(\"Sigmoid Activation Function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"$\\\\sigma(x)$\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n</pre> x_vals = np.linspace(-10, 10, 100) X = tensor(x_vals, requires_grad=True) sigmoid = Sigmoid() Y = sigmoid(X)  plt.figure(figsize=(10, 5)) plt.plot(x_vals, Y.numpy(), label='$Sigmoid(x) = \\\\sigma(x)$', color='#f39c12', lw=2) plt.title(\"Sigmoid Activation Function\") plt.xlabel(\"x\") plt.ylabel(\"$\\\\sigma(x)$\") plt.grid(True, alpha=0.3) plt.legend() plt.show() In\u00a0[12]: Copied! <pre># Demonstrating stability with large values\nX_ext = tensor([-100.0, 0.0, 100.0], requires_grad=True)\nY_ext = sigmoid(X_ext)\nY_ext.backward()\n\nprint(f\"Logits:    {X_ext.numpy()}\")\nprint(f\"Probs:     {Y_ext.numpy()} (No NaNs!)\")\nprint(f\"Gradients: {X_ext.grad}\")\n</pre> # Demonstrating stability with large values X_ext = tensor([-100.0, 0.0, 100.0], requires_grad=True) Y_ext = sigmoid(X_ext) Y_ext.backward()  print(f\"Logits:    {X_ext.numpy()}\") print(f\"Probs:     {Y_ext.numpy()} (No NaNs!)\") print(f\"Gradients: {X_ext.grad}\") <pre>Logits:    [-100.    0.  100.]\nProbs:     [3.8e-44 5.0e-01 1.0e+00] (No NaNs!)\nGradients: [3.8e-44 2.5e-01 0.0e+00]\n</pre>"},{"location":"learn/layers/04-Sigmoid/#sigmoid","title":"Sigmoid\u00b6","text":"<p>The Sigmoid layer implements the logistic sigmoid activation function. It maps any real-valued input into the range $(0, 1)$, which is essential for binary classification and predicting probabilities.</p>"},{"location":"learn/layers/04-Sigmoid/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>For an input $x \\in \\mathbb{R}$, the Sigmoid function is defined as:</p> <p>$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$</p>"},{"location":"learn/layers/04-Sigmoid/#numerical-stability","title":"Numerical Stability\u00b6","text":"<p>The naive implementation of $\\sigma(x)$ can be unstable for large negative values because $e^{-x}$ will overflow. To prevent this, Sorix uses a numerically stable piecewise implementation:</p> <p>$$ \\sigma(x) =  \\begin{cases}  \\frac{1}{1 + e^{-x}} &amp; \\text{if } x \\geq 0 \\\\ \\frac{e^x}{1 + e^x} &amp; \\text{if } x &lt; 0  \\end{cases} $$</p> <p>This ensures that we always calculate $e^z$ where $z \\leq 0$, preventing overflows.</p>"},{"location":"learn/layers/04-Sigmoid/#backward-computation-gradient","title":"Backward computation (gradient)\u00b6","text":"<p>The derivative can be expressed in terms of the output $y = \\sigma(x)$: $$\\frac{d\\sigma(x)}{dx} = y(1 - y)$$</p> <p>During backpropagation, the gradient is propagated as: $$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot y(1 - y)$$</p>"},{"location":"learn/layers/04-Sigmoid/#visualizing-sigmoid","title":"Visualizing Sigmoid\u00b6","text":""},{"location":"learn/layers/04-Sigmoid/#handling-extreme-values","title":"Handling Extreme Values\u00b6","text":""},{"location":"learn/layers/05-Tanh/","title":"Tanh","text":"In\u00a0[8]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[9]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sorix import tensor\nfrom sorix.nn import Tanh\nimport sorix\n\nplt.style.use('ggplot')\n</pre> import numpy as np import matplotlib.pyplot as plt from sorix import tensor from sorix.nn import Tanh import sorix  plt.style.use('ggplot') In\u00a0[10]: Copied! <pre>x_vals = np.linspace(-5, 5, 100)\nX = tensor(x_vals, requires_grad=True)\ntanh = Tanh()\nY = tanh(X)\n\nplt.figure(figsize=(10, 5))\nplt.plot(x_vals, Y.numpy(), label='$Tanh(x) = \\\\tanh(x)$', color='#3498db', lw=2)\nplt.axhline(0, color='black', lw=1, ls='--')\nplt.title(\"Tanh Activation Function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"$\\\\tanh(x)$\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n</pre> x_vals = np.linspace(-5, 5, 100) X = tensor(x_vals, requires_grad=True) tanh = Tanh() Y = tanh(X)  plt.figure(figsize=(10, 5)) plt.plot(x_vals, Y.numpy(), label='$Tanh(x) = \\\\tanh(x)$', color='#3498db', lw=2) plt.axhline(0, color='black', lw=1, ls='--') plt.title(\"Tanh Activation Function\") plt.xlabel(\"x\") plt.ylabel(\"$\\\\tanh(x)$\") plt.grid(True, alpha=0.3) plt.legend() plt.show() In\u00a0[11]: Copied! <pre>X = tensor([-3.0, -1.0, 0.0, 1.0, 3.0], requires_grad=True)\nY = tanh(X)\nY.backward()\n\nprint(f\"Input:     {X.numpy()}\")\nprint(f\"Output:    {Y.numpy()}\")\nprint(f\"Gradients: {X.grad}\")\n</pre> X = tensor([-3.0, -1.0, 0.0, 1.0, 3.0], requires_grad=True) Y = tanh(X) Y.backward()  print(f\"Input:     {X.numpy()}\") print(f\"Output:    {Y.numpy()}\") print(f\"Gradients: {X.grad}\") <pre>Input:     [-3. -1.  0.  1.  3.]\nOutput:    [-0.9950548 -0.7615942  0.         0.7615942  0.9950548]\nGradients: [0.009866   0.41997433 1.         0.41997433 0.009866  ]\n</pre>"},{"location":"learn/layers/05-Tanh/#tanh","title":"Tanh\u00b6","text":"<p>The Tanh layer implements the hyperbolic tangent activation function. It is a symmetric transformation that maps input values into the range $(-1, 1)$. Being zero-centered, its outputs have an average mean closer to zero compared to the Sigmoid function, which often facilitates faster training of deep models.</p>"},{"location":"learn/layers/05-Tanh/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>For an input $x \\in \\mathbb{R}$, the Tanh function is defined as:</p> <p>$$ \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}. $$</p>"},{"location":"learn/layers/05-Tanh/#backward-computation-gradient","title":"Backward computation (gradient)\u00b6","text":"<p>The derivative of the hyperbolic tangent function can be expressed elegantly in terms of its output $y = \\tanh(x)$: $$\\frac{d\\tanh(x)}{dx} = 1 - y^2$$</p> <p>During backpropagation, the gradient is propagated as: $$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot (1 - y^2)$$</p>"},{"location":"learn/layers/05-Tanh/#visualizing-tanh","title":"Visualizing Tanh\u00b6","text":""},{"location":"learn/layers/05-Tanh/#functional-example","title":"Functional Example\u00b6","text":""},{"location":"learn/layers/06-Dropout/","title":"Dropout","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sorix import tensor\nfrom sorix.nn import Dropout\nimport sorix\n</pre> import numpy as np from sorix import tensor from sorix.nn import Dropout import sorix In\u00a0[3]: Copied! <pre># Create a large batch to see the dropout statistics\nX = tensor(np.ones((1, 10)))\nprint(f\"Input: {X.data}\")\n\ndropout = Dropout(p=0.5)\ndropout.train() # Default mode\nY_train = dropout(X)\nprint(f\"Output (Training): {Y_train.data}\")\n\ndropout.eval() # Change to inference mode\nY_eval = dropout(X)\nprint(f\"Output (Evaluation): {Y_eval.data}\")\n</pre> # Create a large batch to see the dropout statistics X = tensor(np.ones((1, 10))) print(f\"Input: {X.data}\")  dropout = Dropout(p=0.5) dropout.train() # Default mode Y_train = dropout(X) print(f\"Output (Training): {Y_train.data}\")  dropout.eval() # Change to inference mode Y_eval = dropout(X) print(f\"Output (Evaluation): {Y_eval.data}\") <pre>Input: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\nOutput (Training): [[2. 2. 0. 0. 2. 0. 0. 2. 0. 2.]]\nOutput (Evaluation): [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n</pre>"},{"location":"learn/layers/06-Dropout/#dropout","title":"Dropout\u00b6","text":"<p>The Dropout layer implements a powerful regularization technique widely used in deep neural networks to prevent overfitting. During training, it randomly zeroes some of the elements of the input tensor with probability $p$ using samples from a Bernoulli distribution. This forces the network to learn more robust features and prevents the co-adaptation of neurons.</p>"},{"location":"learn/layers/06-Dropout/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>During training, for each element $x$ of the input tensor, the output $y$ is computed as:</p> <p>$$ y = \\begin{cases} 0 &amp; \\text{with probability } p, \\\\ \\frac{x}{1-p} &amp; \\text{with probability } 1-p. \\end{cases} $$</p> <p>The factor $\\frac{1}{1-p}$ ensures that the expected value of the output remains the same as during inference:</p> <p>$$ \\mathbb{E}[y] = p \\cdot 0 + (1-p) \\cdot \\frac{x}{1-p} = x. $$</p>"},{"location":"learn/layers/06-Dropout/#training-vs-inference","title":"Training vs Inference\u00b6","text":"<p>Like all regularization layers, Dropout behaves differently depending on the model's mode:</p> <ul> <li>Training Mode: The mask is randomly generated and applied, and scaling is performed.</li> <li>Evaluation Mode (<code>model.train(False)</code>): Dropout acts as an identity function ($y = x$), as scaling was already handled during training.</li> </ul>"},{"location":"learn/layers/06-Dropout/#backward-computation-gradient","title":"Backward computation (gradient)\u00b6","text":"<p>The gradient is propagated through the mask used during the forward pass. If an element was zeroed out, its gradient will also be zero:</p> <p>$$ \\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\text{mask} \\cdot \\frac{1}{1-p}. $$</p>"},{"location":"learn/layers/06-Dropout/#functional-view","title":"Functional view\u00b6","text":"<p>The Dropout layer maps from $\\mathbb{R}^{N \\times d}$ to $\\mathbb{R}^{N \\times d}$. It is a zero-parameter layer (though it has the hyperparameter $p$), meaning it does not have trainable weights like Linear or BatchNorm1d.</p>"},{"location":"learn/layers/06-Dropout/#implementation-specifics","title":"Implementation specifics\u00b6","text":"<p>Sorix's Dropout implementation uses the input tensor's device to generate the random mask, ensuring that random operations are GPU-accelerated when necessary.</p>"},{"location":"learn/layers/07-Module/","title":"Module","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nfrom sorix import tensor\nfrom sorix.nn import Module\n\nclass PReLU(Module):\n    def __init__(self, size=1, initial_alpha=0.25):\n        super().__init__()\n        # alpha is a learned parameter\n        self.alpha = tensor(np.full(size, initial_alpha), requires_grad=True)\n        \n    def forward(self, x):\n        # x &gt; 0 returns a boolean mask (converted to float in operation)\n        # We use Sorix operations to stay within the autograd graph\n        pos = (x &gt; 0) * x\n        neg = (x &lt;= 0) * (self.alpha * x)\n        return pos + neg\n\nprelu = PReLU(size=1)\nx = tensor([-2.0, 1.0, -0.5])\ny = prelu(x)\n\nprint(f\"Input: {x.numpy()}\")\nprint(f\"Output with initial alpha=0.25: {y.numpy()}\")\n</pre> import numpy as np from sorix import tensor from sorix.nn import Module  class PReLU(Module):     def __init__(self, size=1, initial_alpha=0.25):         super().__init__()         # alpha is a learned parameter         self.alpha = tensor(np.full(size, initial_alpha), requires_grad=True)              def forward(self, x):         # x &gt; 0 returns a boolean mask (converted to float in operation)         # We use Sorix operations to stay within the autograd graph         pos = (x &gt; 0) * x         neg = (x &lt;= 0) * (self.alpha * x)         return pos + neg  prelu = PReLU(size=1) x = tensor([-2.0, 1.0, -0.5]) y = prelu(x)  print(f\"Input: {x.numpy()}\") print(f\"Output with initial alpha=0.25: {y.numpy()}\") <pre>Input: [-2.   1.  -0.5]\nOutput with initial alpha=0.25: [-0.5    1.    -0.125]\n</pre> In\u00a0[2]: Copied! <pre>from sorix.optim import SGD\nfrom sorix.nn import MSELoss\n\noptimizer = SGD(prelu.parameters(), lr=0.1)\ntarget = tensor([0.0, 1.0, 0.0]) # We want negative inputs to result in 0\ncriterion = MSELoss()\n\nprint(f\"Alpha before update: {prelu.alpha.item():.4f}\")\n\n# One training step\ny = prelu(x)\nloss = criterion(y, target)\nloss.backward()\noptimizer.step()\noptimizer.zero_grad()\n\nprint(f\"Alpha after update: {prelu.alpha.item():.4f}\")\nprint(f\"New output: {prelu(x).numpy()}\")\n</pre> from sorix.optim import SGD from sorix.nn import MSELoss  optimizer = SGD(prelu.parameters(), lr=0.1) target = tensor([0.0, 1.0, 0.0]) # We want negative inputs to result in 0 criterion = MSELoss()  print(f\"Alpha before update: {prelu.alpha.item():.4f}\")  # One training step y = prelu(x) loss = criterion(y, target) loss.backward() optimizer.step() optimizer.zero_grad()  print(f\"Alpha after update: {prelu.alpha.item():.4f}\") print(f\"New output: {prelu(x).numpy()}\") <pre>Alpha before update: 0.2500\nAlpha after update: 0.1792\nNew output: [-0.35833333  1.         -0.08958333]\n</pre> In\u00a0[3]: Copied! <pre>from sorix.nn import Linear, ReLU, BatchNorm1d\n\nclass ResidualBlock(Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = Linear(dim, dim)\n        self.bn1 = BatchNorm1d(dim)\n        self.relu = ReLU()\n        self.fc2 = Linear(dim, dim)\n        self.bn2 = BatchNorm1d(dim)\n        \n    def forward(self, x):\n        residual = x\n        \n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.bn2(out)\n        \n        return self.relu(out + residual)\n\nblock = ResidualBlock(10)\nprint(f\"Number of parameters in block: {len(block.parameters())}\")\n</pre> from sorix.nn import Linear, ReLU, BatchNorm1d  class ResidualBlock(Module):     def __init__(self, dim):         super().__init__()         self.fc1 = Linear(dim, dim)         self.bn1 = BatchNorm1d(dim)         self.relu = ReLU()         self.fc2 = Linear(dim, dim)         self.bn2 = BatchNorm1d(dim)              def forward(self, x):         residual = x                  out = self.fc1(x)         out = self.bn1(out)         out = self.relu(out)         out = self.fc2(out)         out = self.bn2(out)                  return self.relu(out + residual)  block = ResidualBlock(10) print(f\"Number of parameters in block: {len(block.parameters())}\") <pre>Number of parameters in block: 8\n</pre> In\u00a0[4]: Copied! <pre>class ResNetMLP(Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=2):\n        super().__init__()\n        self.stem = Linear(input_dim, hidden_dim)\n        self.blocks = [ResidualBlock(hidden_dim) for _ in range(num_blocks)]\n        self.prelu = PReLU(size=hidden_dim)\n        self.head = Linear(hidden_dim, output_dim)\n        \n    def forward(self, x):\n        x = self.stem(x)\n        for block in self.blocks:\n            x = block(x)\n        x = self.prelu(x)\n        return self.head(x)\n\nmodel = ResNetMLP(input_dim=5, hidden_dim=16, output_dim=1)\noptimizer = SGD(model.parameters(), lr=0.01)\ncriterion = MSELoss()\n\n# Create synthetic data: y = sum(x) \nX_train = tensor(np.random.randn(100, 5))\ny_train = tensor(np.sum(X_train.numpy(), axis=1, keepdims=True))\n\nprint(\"Training ResNetMLP...\")\nfor epoch in range(101):\n    model.train()\n    y_pred = model(X_train)\n    loss = criterion(y_pred, y_train)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if epoch % 20 == 0:\n        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.6f}\")\n</pre> class ResNetMLP(Module):     def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=2):         super().__init__()         self.stem = Linear(input_dim, hidden_dim)         self.blocks = [ResidualBlock(hidden_dim) for _ in range(num_blocks)]         self.prelu = PReLU(size=hidden_dim)         self.head = Linear(hidden_dim, output_dim)              def forward(self, x):         x = self.stem(x)         for block in self.blocks:             x = block(x)         x = self.prelu(x)         return self.head(x)  model = ResNetMLP(input_dim=5, hidden_dim=16, output_dim=1) optimizer = SGD(model.parameters(), lr=0.01) criterion = MSELoss()  # Create synthetic data: y = sum(x)  X_train = tensor(np.random.randn(100, 5)) y_train = tensor(np.sum(X_train.numpy(), axis=1, keepdims=True))  print(\"Training ResNetMLP...\") for epoch in range(101):     model.train()     y_pred = model(X_train)     loss = criterion(y_pred, y_train)          optimizer.zero_grad()     loss.backward()     optimizer.step()          if epoch % 20 == 0:         print(f\"Epoch {epoch:3d} | Loss: {loss.item():.6f}\") <pre>Training ResNetMLP...\nEpoch   0 | Loss: 4.135838\nEpoch  20 | Loss: 0.418998\nEpoch  40 | Loss: 0.248450\nEpoch  60 | Loss: 0.168582\nEpoch  80 | Loss: 0.120915\nEpoch 100 | Loss: 0.091177\n</pre> In\u00a0[5]: Copied! <pre>from sorix import save, load\n\n# Get state dict\nsd = model.state_dict()\nprint(\"State dict keys sample:\", list(sd.keys())[:5])\n\n# Save and Load\nsave(sd, \"resnet_model.sor\")\nloaded_weights = load(\"resnet_model.sor\")\n\nnew_model = ResNetMLP(input_dim=5, hidden_dim=16, output_dim=1)\nnew_model.load_state_dict(loaded_weights)\n\nprint(\"\\nWeights persistence verified!\")\n</pre> from sorix import save, load  # Get state dict sd = model.state_dict() print(\"State dict keys sample:\", list(sd.keys())[:5])  # Save and Load save(sd, \"resnet_model.sor\") loaded_weights = load(\"resnet_model.sor\")  new_model = ResNetMLP(input_dim=5, hidden_dim=16, output_dim=1) new_model.load_state_dict(loaded_weights)  print(\"\\nWeights persistence verified!\") <pre>State dict keys sample: ['stem.W', 'stem.b', 'prelu.alpha', 'head.W', 'head.b']\n\nWeights persistence verified!\n</pre> In\u00a0[6]: Copied! <pre>import sorix\n\n# Switch to evaluation mode (essential for BatchNorm/Dropout)\nmodel.eval()\nprint(f\"In training mode? {model.training}\")\n\n# Move to GPU if available\nif sorix.cuda.is_available():\n    model.to('cuda')\n    print(\"Entire model and its nested blocks moved to GPU memory.\")\n</pre> import sorix  # Switch to evaluation mode (essential for BatchNorm/Dropout) model.eval() print(f\"In training mode? {model.training}\")  # Move to GPU if available if sorix.cuda.is_available():     model.to('cuda')     print(\"Entire model and its nested blocks moved to GPU memory.\") <pre>In training mode? False\n</pre> <pre>\u2705 GPU basic operation passed\n\u2705 GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\nCUDA runtime version: 13000\nCuPy version: 14.0.1\nEntire model and its nested blocks moved to GPU memory.\n</pre>"},{"location":"learn/layers/07-Module/#module","title":"Module\u00b6","text":"<p>In Sorix, the <code>Module</code> class is the fundamental building block for all neural network components. Whether you are building a simple activation function, a complex layer, or an entire deep neural network, you will almost always inherit from <code>Module</code>.</p> <p>Its design is intentionally similar to PyTorch's <code>nn.Module</code>, making it intuitive for those coming from other frameworks while remaining simple enough to extend manually.</p>"},{"location":"learn/layers/07-Module/#key-features-of-module","title":"Key Features of <code>Module</code>\u00b6","text":"<ol> <li>Automatic Parameter Tracking: Any <code>Tensor</code> attribute that has <code>requires_grad=True</code> is automatically collected by the <code>.parameters()</code> method.</li> <li>Sub-module Registration: If you assign another <code>Module</code> as an attribute of your class, Sorix will recursively find its parameters as well.</li> <li>Device Management: The <code>.to(device)</code> method moves all parameters and sub-modules to CPU or GPU (via CuPy).</li> <li>Training/Evaluation Modes: The <code>.train()</code> and <code>.eval()</code> methods toggle the behavior of layers like <code>Dropout</code> and <code>BatchNorm1d</code> surface-wide.</li> <li>State Management: <code>.state_dict()</code> and <code>.load_state_dict()</code> allow for easy serialization of your model's weights.</li> </ol>"},{"location":"learn/layers/07-Module/#1-creating-a-custom-layer-with-parameters","title":"1. Creating a Custom Layer with Parameters\u00b6","text":"<p>A \"layer\" in Sorix is just a <code>Module</code> that performs a specific operation. While we have built-in layers like <code>Linear</code>, you can easily create your own.</p> <p>Let's implement a Parametric ReLU (PReLU), which is like a standard ReLU but with a learned slope $\\alpha$ for negative values:</p> <p>$$f(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ \\alpha x &amp; \\text{if } x \\leq 0 \\end{cases}$$</p>"},{"location":"learn/layers/07-Module/#verifying-autograd-in-custom-layers","title":"Verifying Autograd in Custom Layers\u00b6","text":"<p>To verify that our layer is indeed learning, we can perform a simple optimization step. If we want the output for negative numbers to be more positive, the optimizer should adjust <code>alpha</code> accordingly.</p>"},{"location":"learn/layers/07-Module/#2-advanced-composition-residual-blocks","title":"2. Advanced Composition: Residual Blocks\u00b6","text":"<p>Modern deep learning architectures (like ResNets) rely on Skip Connections. In Sorix, you can easily build complex re-usable blocks by nesting other modules.</p>"},{"location":"learn/layers/07-Module/#3-training-a-complete-architecture","title":"3. Training a Complete Architecture\u00b6","text":"<p>Let's build a ResNet-style MLP and train it on a simple synthetic regression task to prove that the entire stack (nested modules, residual connections, custom layers, and optimizers) works in harmony.</p>"},{"location":"learn/layers/07-Module/#4-parameter-and-state-management","title":"4. Parameter and State Management\u00b6","text":"<p>One of the most powerful features of <code>Module</code> is the <code>.parameters()</code> method. It automatically crawls the object's attributes (including lists, dictionaries, and sub-models) to find everything that needs to be optimized.</p> <p>The <code>state_dict()</code> returns a dictionary mapping parameter names to their current values, perfect for saving weights.</p>"},{"location":"learn/layers/07-Module/#5-device-and-mode-management","title":"5. Device and Mode Management\u00b6","text":"<p>Since our model contains <code>BatchNorm1d</code>, switching between <code>train()</code> and <code>eval()</code> is mandatory for correct inference.</p>"},{"location":"learn/layers/07-Module/#conclusion","title":"Conclusion\u00b6","text":"<p>By subclassing <code>Module</code>, you gain all the power of Sorix's ecosystem with minimal code. You can implement complex research architectures with skip connections and custom primitives, and Sorix will handle the gradients, optimization, and hardware acceleration for you.</p>"},{"location":"learn/loss/","title":"Loss Functions","text":"<p>Loss functions (also called cost functions or objective functions) are used to measure how well a neural network's predictions match the target values. The goal of training is to minimize this value.</p> <p>Sorix provides several common loss functions for different types of tasks.</p>"},{"location":"learn/loss/#available-loss-functions","title":"Available Loss Functions","text":"Loss Function Use Case Description <code>MSELoss</code> Regression Mean Squared Error between prediction and target. <code>BCEWithLogitsLoss</code> Binary Classification Binary Cross Entropy with integrated Sigmoid for stability. <code>CrossEntropyLoss</code> Multiclass Classification Measures the difference between two probability distributions."},{"location":"learn/loss/#which-loss-function-should-i-use","title":"Which loss function should I use?","text":"<ul> <li>Regression: Use <code>MSELoss</code> when predicting continuous values (e.g., house prices).</li> <li>Binary Classification: Use <code>BCEWithLogitsLoss</code> when you have two classes (0 or 1).</li> <li>Multiclass Classification: Use <code>CrossEntropyLoss</code> for multiple mutually exclusive classes (e.g., MNIST digit recognition).</li> </ul>"},{"location":"learn/loss/#basic-usage","title":"Basic Usage","text":"<p>In Sorix, loss functions are objects that you call with two tensors: the predictions (<code>y_pred</code>) and the ground truth (<code>y_true</code>).</p> <pre><code>from sorix.nn import MSELoss\nfrom sorix import tensor\n\ncriterion = MSELoss()\ny_pred = tensor([2.5, 0.0])\ny_true = tensor([3.0, 0.0])\n\nloss = criterion(y_pred, y_true)\nprint(f\"Loss: {loss.item()}\") # -&gt; 0.125\n</code></pre>"},{"location":"learn/loss/01-MSELoss/","title":"MSELoss","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sorix import tensor\nfrom sorix.nn import MSELoss\n\n# Create data\ny_pred = tensor([2.5, 0.0, 2.1], requires_grad=True)\ny_true = tensor([3.0, 0.0, 2.0])\n\ncriterion = MSELoss()\nloss = criterion(y_pred, y_true)\n\nprint(f\"Predictions: {y_pred.numpy()}\")\nprint(f\"Targets:     {y_true.numpy()}\")\nprint(f\"MSE Loss:    {loss.item():.4f}\")\n</pre> import numpy as np from sorix import tensor from sorix.nn import MSELoss  # Create data y_pred = tensor([2.5, 0.0, 2.1], requires_grad=True) y_true = tensor([3.0, 0.0, 2.0])  criterion = MSELoss() loss = criterion(y_pred, y_true)  print(f\"Predictions: {y_pred.numpy()}\") print(f\"Targets:     {y_true.numpy()}\") print(f\"MSE Loss:    {loss.item():.4f}\") <pre>Predictions: [2.5 0.  2.1]\nTargets:     [3. 0. 2.]\nMSE Loss:    0.0867\n</pre> In\u00a0[3]: Copied! <pre>loss.backward()\nprint(f\"Gradients w.r.t y_pred: {y_pred.grad}\")\n\n# Manual verification: d/dy_pred (1/n * (y_pred - y_true)^2) = 2/n * (y_pred - y_true)\nn = y_pred.data.size\nmanual_grad = 2/n * (y_pred.data - y_true.data)\nprint(f\"Manual Gradients:     {manual_grad}\")\n</pre> loss.backward() print(f\"Gradients w.r.t y_pred: {y_pred.grad}\")  # Manual verification: d/dy_pred (1/n * (y_pred - y_true)^2) = 2/n * (y_pred - y_true) n = y_pred.data.size manual_grad = 2/n * (y_pred.data - y_true.data) print(f\"Manual Gradients:     {manual_grad}\") <pre>Gradients w.r.t y_pred: [-0.33333334  0.          0.0666666 ]\nManual Gradients:     [-0.33333334  0.          0.0666666 ]\n</pre> In\u00a0[4]: Copied! <pre>from sorix.optim import SGD\n\nweight = tensor([10.0], requires_grad=True)\ntarget = tensor([42.0])\noptimizer = SGD([weight], lr=0.1)\n\nprint(f\"Initial weight: {weight.item():.2f}\")\n\nfor i in range(21):\n    loss = criterion(weight, target)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    if i % 5 == 0:\n        print(f\"Step {i:2d} | Weight: {weight.item():.4f} | Loss: {loss.item():.4f}\")\n\nprint(f\"Final weight: {weight.item():.2f}\")\n</pre> from sorix.optim import SGD  weight = tensor([10.0], requires_grad=True) target = tensor([42.0]) optimizer = SGD([weight], lr=0.1)  print(f\"Initial weight: {weight.item():.2f}\")  for i in range(21):     loss = criterion(weight, target)     loss.backward()     optimizer.step()     optimizer.zero_grad()     if i % 5 == 0:         print(f\"Step {i:2d} | Weight: {weight.item():.4f} | Loss: {loss.item():.4f}\")  print(f\"Final weight: {weight.item():.2f}\") <pre>Initial weight: 10.00\nStep  0 | Weight: 16.4000 | Loss: 1024.0000\nStep  5 | Weight: 33.6114 | Loss: 109.9512\nStep 10 | Weight: 39.2512 | Loss: 11.8059\nStep 15 | Weight: 41.0993 | Loss: 1.2676\nStep 20 | Weight: 41.7049 | Loss: 0.1361\nFinal weight: 41.70\n</pre>"},{"location":"learn/loss/01-MSELoss/#mseloss","title":"MSELoss\u00b6","text":"<p>The Mean Squared Error (MSE) loss measures the average of the squares of the errors. It is the most common loss function for Regression tasks.</p> <p>$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$</p> <p>Where:</p> <ul> <li>$n$ is the batch size.</li> <li>$y_i$ is the target value.</li> <li>$\\hat{y}_i$ is the predicted value.</li> </ul>"},{"location":"learn/loss/01-MSELoss/#verification-with-autograd","title":"Verification with Autograd\u00b6","text":"<p>MSELoss in Sorix is fully differentiable. If we compute the backward pass, we can see the gradients w.r.t the predictions.</p>"},{"location":"learn/loss/01-MSELoss/#training-example","title":"Training Example\u00b6","text":"<p>Let's see how <code>MSELoss</code> guides a single value to match a target.</p>"},{"location":"learn/loss/02-BCEWithLogitsLoss/","title":"BCEWithLogitsLoss","text":"In\u00a0[17]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[18]: Copied! <pre>import numpy as np\nfrom sorix import tensor\nfrom sorix.nn import BCEWithLogitsLoss\n\n# Create logits (+ve for class 1, -ve for class 0)\nlogits = tensor([100.0, -100.0, 0.0], requires_grad=True)\ntargets = tensor([1.0, 0.0, 1.0])\n\ncriterion = BCEWithLogitsLoss()\nloss = criterion(logits, targets)\n\nprint(f\"Logits (Extremes): {logits.numpy()}\")\nprint(f\"Targets:           {targets.numpy()}\")\nprint(f\"Stable BCE Loss:    {loss.item():.4f} (No NaNs or Warnings!)\")\n</pre> import numpy as np from sorix import tensor from sorix.nn import BCEWithLogitsLoss  # Create logits (+ve for class 1, -ve for class 0) logits = tensor([100.0, -100.0, 0.0], requires_grad=True) targets = tensor([1.0, 0.0, 1.0])  criterion = BCEWithLogitsLoss() loss = criterion(logits, targets)  print(f\"Logits (Extremes): {logits.numpy()}\") print(f\"Targets:           {targets.numpy()}\") print(f\"Stable BCE Loss:    {loss.item():.4f} (No NaNs or Warnings!)\") <pre>Logits (Extremes): [ 100. -100.    0.]\nTargets:           [1. 0. 1.]\nStable BCE Loss:    0.2310 (No NaNs or Warnings!)\n</pre> In\u00a0[19]: Copied! <pre>loss.backward()\nprint(f\"Gradients w.r.t logits: {logits.grad}\")\n\n# dL/d_logit = 1/n * (sigma(logit) - target)\nn = logits.data.size\nx = logits.data\n\n# Truly stable sigmoid reusing e^{-|x|}\nabs_x = np.abs(x)\nexp_neg_abs_x = np.exp(-abs_x)\ndenom = 1 + exp_neg_abs_x\nprobs = np.where(x &gt;= 0, 1 / denom, exp_neg_abs_x / denom)\n\nmanual_grad = (probs - targets.data) / n\nprint(f\"Manual Gradients:       {manual_grad}\")\n</pre> loss.backward() print(f\"Gradients w.r.t logits: {logits.grad}\")  # dL/d_logit = 1/n * (sigma(logit) - target) n = logits.data.size x = logits.data  # Truly stable sigmoid reusing e^{-|x|} abs_x = np.abs(x) exp_neg_abs_x = np.exp(-abs_x) denom = 1 + exp_neg_abs_x probs = np.where(x &gt;= 0, 1 / denom, exp_neg_abs_x / denom)  manual_grad = (probs - targets.data) / n print(f\"Manual Gradients:       {manual_grad}\") <pre>Gradients w.r.t logits: [ 0.          0.         -0.16666667]\nManual Gradients:       [ 0.0000000e+00  1.2611686e-44 -1.6666667e-01]\n</pre> In\u00a0[20]: Copied! <pre>from sorix.optim import SGD\n\nlogits = tensor([-5.0], requires_grad=True) # Starting at class 0\ntarget = tensor([1.0]) # Target class 1\noptimizer = SGD([logits], lr=0.5)\n\nprint(f\"Initial Logits: {logits.item():.2f}\")\n\nfor i in range(21):\n    loss = criterion(logits, target)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    if i % 5 == 0:\n        # Probability after Sigmoid\n        prob = 1 / (1 + np.exp(-logits.item()))\n        print(f\"Step {i:2d} | Logit: {logits.item():6.4f} | Prob: {prob:6.4f} | Loss: {loss.item():6.4f}\")\n\nprint(f\"\\nFinal Logit: {logits.item():.2f} (Close to +ve for class 1)\")\n</pre> from sorix.optim import SGD  logits = tensor([-5.0], requires_grad=True) # Starting at class 0 target = tensor([1.0]) # Target class 1 optimizer = SGD([logits], lr=0.5)  print(f\"Initial Logits: {logits.item():.2f}\")  for i in range(21):     loss = criterion(logits, target)     loss.backward()     optimizer.step()     optimizer.zero_grad()     if i % 5 == 0:         # Probability after Sigmoid         prob = 1 / (1 + np.exp(-logits.item()))         print(f\"Step {i:2d} | Logit: {logits.item():6.4f} | Prob: {prob:6.4f} | Loss: {loss.item():6.4f}\")  print(f\"\\nFinal Logit: {logits.item():.2f} (Close to +ve for class 1)\") <pre>Initial Logits: -5.00\nStep  0 | Logit: -4.5033 | Prob: 0.0110 | Loss: 5.0067\nStep  5 | Logit: -2.0912 | Prob: 0.1100 | Loss: 2.6300\nStep 10 | Logit: -0.1809 | Prob: 0.4549 | Loss: 0.9685\nStep 15 | Logit: 0.8868 | Prob: 0.7082 | Loss: 0.3955\nStep 20 | Logit: 1.4911 | Prob: 0.8162 | Loss: 0.2221\n\nFinal Logit: 1.49 (Close to +ve for class 1)\n</pre>"},{"location":"learn/loss/02-BCEWithLogitsLoss/#bcewithlogitsloss","title":"BCEWithLogitsLoss\u00b6","text":"<p>Binary Cross Entropy (BCE) measures the distance between the distribution of outcomes and predictions. For stability, Sorix implements <code>BCEWithLogitsLoss</code>, which includes a Sigmoid activation inside the loss function.</p> <p>The total loss is the average over all $n$ samples in the batch:</p> <p>$$L = - \\frac{1}{n} \\sum_{i=1}^{n} [y_i \\ln(\\sigma(\\hat{y}_i)) + (1 - y_i) \\ln(1 - \\sigma(\\hat{y}_i))]$$</p> <p>Where:</p> <ul> <li>$\\hat{y}_i$ are logarithmic odds (logits).</li> <li>$\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the Sigmoid function.</li> <li>$y_i$ is the target (0 or 1).</li> </ul>"},{"location":"learn/loss/02-BCEWithLogitsLoss/#numerical-stability-the-log-sum-exp-trick","title":"Numerical Stability: The Log-Sum-Exp Trick\u00b6","text":"<p>Directly calculating $\\ln(\\sigma(x))$ can lead to numerical instability. For example, if $x$ is a large positive number, $\\sigma(x) \\approx 1$, and $\\ln(1) = 0$. However, if $x$ is a large negative number, $\\sigma(x) \\approx 0$, and $\\ln(0)$ is undefined ($-\\infty$).</p> <p>To avoid this, Sorix uses a mathematically equivalent but numerically stable form for each element:</p>"},{"location":"learn/loss/02-BCEWithLogitsLoss/#mathematical-derivation","title":"Mathematical Derivation\u00b6","text":"<p>We know that $\\ln(\\sigma(x)) = \\ln(\\frac{1}{1+e^{-x}}) = -\\ln(1+e^{-x})$. And $\\ln(1-\\sigma(x)) = \\ln(\\frac{e^{-x}}{1+e^{-x}}) = -x - \\ln(1+e^{-x})$.</p> <p>Substituting these into the BCE formula for a single element: $$l = - [y (-\\ln(1+e^{-x})) + (1-y)(-x - \\ln(1+e^{-x}))]$$ $$l = y \\ln(1+e^{-x}) + (1-y)x + (1-y)\\ln(1+e^{-x})$$ $$l = (1-y)x + \\ln(1+e^{-x})$$</p> <p>To make it stable for both large positive and negative $x$, we use the identity $\\ln(1+e^{-x}) = \\max(-x, 0) + \\ln(1+e^{-|x|})$. The final stable per-element loss implemented in Sorix is:</p> <p>$$l = \\max(x, 0) - x \\cdot y + \\ln(1 + e^{-|x|})$$</p> <p>And the final loss is the mean of these values: $L = \\text{mean}(l)$.</p>"},{"location":"learn/loss/02-BCEWithLogitsLoss/#implementation-optimization","title":"Implementation Optimization\u00b6","text":"<p>Sorix further optimizes this by reusing intermediate values ($e^{-|x|}$) to calculate both the loss and the probabilities needed for the gradient, avoiding redundant exponential calculations.</p>"},{"location":"learn/loss/02-BCEWithLogitsLoss/#verification-with-autograd","title":"Verification with Autograd\u00b6","text":"<p>The gradient of this combined function is also remarkably simple and stable: $$\\frac{\\partial L}{\\partial x} = \\frac{1}{n}(\\sigma(x) - y)$$</p> <p>This prevents the \"vanishing gradient\" problem often seen when activation and loss are calculated separately.</p>"},{"location":"learn/loss/02-BCEWithLogitsLoss/#training-example","title":"Training Example\u00b6","text":"<p>Let's see how <code>BCEWithLogitsLoss</code> matches current logits to a desired outcome.</p>"},{"location":"learn/loss/03-CrossEntropyLoss/","title":"CrossEntropyLoss","text":"In\u00a0[4]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[1]: Copied! <pre>import numpy as np\nfrom sorix import tensor\nfrom sorix.nn import CrossEntropyLoss\n\n# Create logits for 3 classes\nlogits = tensor([[2.0, 1.0, 0.1], [0.0, 5.0, 0.2]], requires_grad=True)\ntargets = tensor([0, 1]) # Class 0 for sample 1, class 1 for sample 2\n\ncriterion = CrossEntropyLoss()\nloss = criterion(logits, targets)\n\nprint(f\"Logits sample 1: {logits.numpy()[0]} (Highest is label 0)\")\nprint(f\"Logits sample 2: {logits.numpy()[1]} (Highest is label 1)\")\nprint(f\"Cross Entropy Loss: {loss.item():.4f}\")\n</pre> import numpy as np from sorix import tensor from sorix.nn import CrossEntropyLoss  # Create logits for 3 classes logits = tensor([[2.0, 1.0, 0.1], [0.0, 5.0, 0.2]], requires_grad=True) targets = tensor([0, 1]) # Class 0 for sample 1, class 1 for sample 2  criterion = CrossEntropyLoss() loss = criterion(logits, targets)  print(f\"Logits sample 1: {logits.numpy()[0]} (Highest is label 0)\") print(f\"Logits sample 2: {logits.numpy()[1]} (Highest is label 1)\") print(f\"Cross Entropy Loss: {loss.item():.4f}\") <pre>Logits sample 1: [2.  1.  0.1] (Highest is label 0)\nLogits sample 2: [0.  5.  0.2] (Highest is label 1)\nCross Entropy Loss: 0.2159\n</pre> In\u00a0[2]: Copied! <pre>loss.backward()\nprint(f\"Gradients w.r.t logits:\\n{logits.grad}\")\n\n# Manual verification: dL/d_logits = 1/n * (softmax(logits) - targets_one_hot)\nbatch_size = logits.data.shape[0]\nexp_logits = np.exp(logits.data - np.max(logits.data, axis=-1, keepdims=True))\nprobs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n\nY_one_hot = np.zeros_like(probs)\nY_one_hot[np.arange(batch_size), targets.data.flatten().astype(int)] = 1\n\nmanual_grad = (probs - Y_one_hot) / batch_size\nprint(f\"\\nManual Gradients (P - Y) / n:\\n{manual_grad}\")\n</pre> loss.backward() print(f\"Gradients w.r.t logits:\\n{logits.grad}\")  # Manual verification: dL/d_logits = 1/n * (softmax(logits) - targets_one_hot) batch_size = logits.data.shape[0] exp_logits = np.exp(logits.data - np.max(logits.data, axis=-1, keepdims=True)) probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)  Y_one_hot = np.zeros_like(probs) Y_one_hot[np.arange(batch_size), targets.data.flatten().astype(int)] = 1  manual_grad = (probs - Y_one_hot) / batch_size print(f\"\\nManual Gradients (P - Y) / n:\\n{manual_grad}\") <pre>Gradients w.r.t logits:\n[[-0.17049944  0.12121648  0.04928295]\n [ 0.00331929 -0.00737348  0.00405419]]\n\nManual Gradients (P - Y) / n:\n[[-0.17049944  0.12121648  0.04928295]\n [ 0.00331929 -0.00737348  0.00405419]]\n</pre> In\u00a0[3]: Copied! <pre>from sorix.optim import SGD\nfrom sorix.nn import Linear\n\nx = tensor([[1.0, 0.0, 0.0]]) # Input data\ntarget = tensor([2]) # We want it to be class 2\n\nmodel = Linear(3, 3)\noptimizer = SGD(model.parameters(), lr=0.1)\n\nprint(f\"Initial raw scores: {model(x).numpy()}\")\n\nfor i in range(51):\n    y_pred = model(x)\n    loss = criterion(y_pred, target)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    if i % 10 == 0:\n        # The score for index 2 should increase\n        print(f\"Step {i:2d} | Loss: {loss.item():.4f} | Output: {y_pred.numpy().flatten()}\")\n\nprint(f\"\\nFinal output: {model(x).numpy().flatten()}\")\n</pre> from sorix.optim import SGD from sorix.nn import Linear  x = tensor([[1.0, 0.0, 0.0]]) # Input data target = tensor([2]) # We want it to be class 2  model = Linear(3, 3) optimizer = SGD(model.parameters(), lr=0.1)  print(f\"Initial raw scores: {model(x).numpy()}\")  for i in range(51):     y_pred = model(x)     loss = criterion(y_pred, target)     loss.backward()     optimizer.step()     optimizer.zero_grad()     if i % 10 == 0:         # The score for index 2 should increase         print(f\"Step {i:2d} | Loss: {loss.item():.4f} | Output: {y_pred.numpy().flatten()}\")  print(f\"\\nFinal output: {model(x).numpy().flatten()}\") <pre>Initial raw scores: [[-1.129986    1.8432822  -0.88651246]]\nStep  0 | Loss: 2.8399 | Output: [-1.129986    1.8432822  -0.88651246]\nStep 10 | Loss: 0.6802 | Output: [-1.2658902   0.4506843   0.64198977]\nStep 20 | Loss: 0.2603 | Output: [-1.392698   -0.12021631  1.3396983 ]\nStep 30 | Loss: 0.1513 | Output: [-1.4783971  -0.39979756  1.7049787 ]\nStep 40 | Loss: 0.1052 | Output: [-1.5416893  -0.57646394  1.944937  ]\nStep 50 | Loss: 0.0803 | Output: [-1.5918877  -0.70369416  2.1223652 ]\n\nFinal output: [-1.5963862 -0.714629   2.1377985]\n</pre>"},{"location":"learn/loss/03-CrossEntropyLoss/#crossentropyloss","title":"CrossEntropyLoss\u00b6","text":"<p>The Cross Entropy loss measures the performance of a classification model whose output is a probability distribution. The goal is to minimize the difference between the predicted distribution and the true distribution.</p> <p>In Sorix, <code>CrossEntropyLoss</code> is implemented for multiclass classification. It expects raw logits as input and applies the Softmax internally.</p> <p>The loss is calculated as the mean over $n$ samples: $$L = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\ln(p_{i,c})$$</p> <p>Where:</p> <ul> <li>$n$ is the batch size.</li> <li>$C$ is the number of classes.</li> <li>$y_{i,c}$ is 1 if class $c$ is the correct label for sample $i$, 0 otherwise.</li> <li>$p_{i,c}$ is the predicted probability for class $c$ of sample $i$ (after Softmax).</li> </ul>"},{"location":"learn/loss/03-CrossEntropyLoss/#1-internal-softmax-integration","title":"1. Internal Softmax Integration\u00b6","text":"<p>Combining the Softmax activation and the Cross Entropy loss into a single step is a standard practice in deep learning frameworks. The main reasons are:</p>"},{"location":"learn/loss/03-CrossEntropyLoss/#numerical-stability-the-log-sum-exp-trick","title":"Numerical Stability (The Log-Sum-Exp Trick)\u00b6","text":"<p>Softmax involves $e^{x_i}$, which can easily overflow for large positive $x_i$. Sorix uses the Log-Sum-Exp trick to calculate these safely: $$\\text{Softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum e^{x_j - \\max(x)}}$$</p>"},{"location":"learn/loss/03-CrossEntropyLoss/#computational-efficiency","title":"Computational Efficiency\u00b6","text":"<p>The mathematical derivative of the combined Softmax + Cross Entropy simplifies beautifully.</p> <p>If we have the loss $L = - \\ln(\\text{Softmax}(x_k))$ where $k$ is the correct class, its derivative simplified is: $$\\frac{\\partial L}{\\partial x_i} = \\frac{1}{n}(P_i - Y_i)$$</p> <p>Where:</p> <ul> <li>$P_i$ is the predicted probability for class $i$.</li> <li>$Y_i$ is 1 if class $i$ is the target, 0 otherwise.</li> <li>$n$ is the batch size.</li> </ul> <p>This means the gradient is just the difference between the prediction and the target, which is extremely cheap to calculate and numerically robust.</p>"},{"location":"learn/loss/03-CrossEntropyLoss/#gradient-verification","title":"Gradient Verification\u00b6","text":"<p>As mentioned before, the gradient is just $(P - Y) / n$. Let's verify this in Sorix.</p>"},{"location":"learn/loss/03-CrossEntropyLoss/#training-example","title":"Training Example\u00b6","text":"<p>Let's see how <code>CrossEntropyLoss</code> helps a simple layer identify the correct class.</p>"},{"location":"learn/optimizers/","title":"Optimizers","text":"<p>Optimizers are algorithms used to update the weights and biases of a neural network to minimize a specific loss function. In Sorix, all optimizers inherit from a base <code>Optimizer</code> class, which provides common functionality such as gradient zeroing and step execution.</p> <p>The general workflow for using an optimizer in Sorix is:</p> <ol> <li>Initialization: Define the optimizer by passing the model's parameters and a learning rate.</li> <li>Zero Gradients: Before each backward pass, clear the previous gradients using <code>optimizer.zero_grad()</code>.</li> <li>Step: After computing the gradients via <code>loss.backward()</code>, update the parameters using <code>optimizer.step()</code>.</li> </ol> <p>Example syntax: <pre><code>optimizer = Adam(model.parameters(), lr=1e-3)\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n</code></pre></p>"},{"location":"learn/optimizers/#available-optimizers","title":"Available Optimizers","text":"<p>Sorix provides several popular optimization algorithms:</p> <ul> <li>SGD: Standard Stochastic Gradient Descent.</li> <li>SGD with Momentum: Accelerates SGD in the relevant direction and dampens oscillations.</li> <li>RMSprop: Adapts the learning rate based on a moving average of squared gradients.</li> <li>Adam: Combines the benefits of AdaGrad and RMSProp, widely used due to its efficiency and low memory requirements.</li> </ul> <p>For a side-by-side comparison of these algorithms on non-convex landscapes, see the Optimizer Comparison guide.</p> <p>If you want to implement your own optimization algorithm, check out the Optimizer Base Class documentation.</p> <p>Detailed mathematical descriptions and implementation examples for each optimizer are provided in the notebooks linked above.</p>"},{"location":"learn/optimizers/01-SGD/","title":"SGD","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sorix import tensor\nfrom sorix.optim import SGD\nimport sorix\n</pre> import numpy as np from sorix import tensor from sorix.optim import SGD import sorix In\u00a0[3]: Copied! <pre># Simple optimization example: minimize an anisotropic parabolic function: f(x, y) = x^2 + 10*y^2\n# This surface challenges standard SGD as it tends to oscillate in the steeper y-direction\nx = tensor([5.0], requires_grad=True)\ny = tensor([5.0], requires_grad=True)\noptimizer = SGD([x, y], lr=0.01)\n\nfor epoch in range(10):\n    # compute loss: f(x, y) = x^2 + 10*y^2\n    loss = x * x + tensor([10.0]) * y * y\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")\n</pre> # Simple optimization example: minimize an anisotropic parabolic function: f(x, y) = x^2 + 10*y^2 # This surface challenges standard SGD as it tends to oscillate in the steeper y-direction x = tensor([5.0], requires_grad=True) y = tensor([5.0], requires_grad=True) optimizer = SGD([x, y], lr=0.01)  for epoch in range(10):     # compute loss: f(x, y) = x^2 + 10*y^2     loss = x * x + tensor([10.0]) * y * y          optimizer.zero_grad()     loss.backward()     optimizer.step()          print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")  <pre>Epoch 1: x = 4.9000, y = 4.0000, loss = 275.0000\nEpoch 2: x = 4.8020, y = 3.2000, loss = 184.0100\nEpoch 3: x = 4.7060, y = 2.5600, loss = 125.4592\nEpoch 4: x = 4.6118, y = 2.0480, loss = 87.6821\nEpoch 5: x = 4.5196, y = 1.6384, loss = 63.2121\nEpoch 6: x = 4.4292, y = 1.3107, loss = 47.2704\nEpoch 7: x = 4.3406, y = 1.0486, loss = 36.7978\nEpoch 8: x = 4.2538, y = 0.8389, loss = 29.8362\nEpoch 9: x = 4.1687, y = 0.6711, loss = 25.1318\nEpoch 10: x = 4.0854, y = 0.5369, loss = 21.8820\n</pre>"},{"location":"learn/optimizers/01-SGD/#sgd","title":"SGD\u00b6","text":"<p>Stochastic Gradient Descent (SGD) is a fundamental optimization algorithm used in machine learning. It updates the model parameters by taking a step in the direction of the negative gradient of the loss function.</p>"},{"location":"learn/optimizers/01-SGD/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>Let $\\theta$ represent the parameters of the model and $\\mathcal{L}$ the loss function. The update rule for SGD is defined as:</p> <p>$$ \\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla \\mathcal{L}(\\theta_t) $$</p> <p>where:</p> <ul> <li>$\\theta_t$: Parameters at time $t$</li> <li>$\\eta$: Learning rate ($lr$), a positive scalar determining the step size.</li> <li>$\\nabla \\mathcal{L}(\\theta_t)$: Gradient of the loss with respect to the parameters at time $t$.</li> </ul>"},{"location":"learn/optimizers/01-SGD/#implementation-details","title":"Implementation details\u00b6","text":"<p>In Sorix, the <code>SGD</code> optimizer iterates through the parameters and updates their <code>data</code> attribute using the calculated <code>grad</code>. This operation is performed in-place and handles both CPU and GPU tensors automatically.</p>"},{"location":"learn/optimizers/02-SGDMomentum/","title":"SGDMomentum","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sorix import tensor\nfrom sorix.optim import SGDMomentum\nimport sorix\n</pre> import numpy as np from sorix import tensor from sorix.optim import SGDMomentum import sorix In\u00a0[3]: Copied! <pre># Same minimizing problem as SGD example: f(x, y) = x^2 + 10*y^2\n# Notice how momentum accelerates the convergence despite the flat landscape in x\nx = tensor([5.0], requires_grad=True)\ny = tensor([5.0], requires_grad=True)\noptimizer = SGDMomentum([x, y], lr=0.01, momentum=0.9)\n\nfor epoch in range(10):\n    loss = x * x + tensor([10.0]) * y * y\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")\n</pre> # Same minimizing problem as SGD example: f(x, y) = x^2 + 10*y^2 # Notice how momentum accelerates the convergence despite the flat landscape in x x = tensor([5.0], requires_grad=True) y = tensor([5.0], requires_grad=True) optimizer = SGDMomentum([x, y], lr=0.01, momentum=0.9)  for epoch in range(10):     loss = x * x + tensor([10.0]) * y * y          optimizer.zero_grad()     loss.backward()     optimizer.step()          print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")  <pre>Epoch 1: x = 4.9000, y = 4.0000, loss = 275.0000\nEpoch 2: x = 4.7120, y = 2.3000, loss = 184.0100\nEpoch 3: x = 4.4486, y = 0.3100, loss = 75.1030\nEpoch 4: x = 4.1225, y = -1.5430, loss = 20.7507\nEpoch 5: x = 3.7466, y = -2.9021, loss = 40.8034\nEpoch 6: x = 3.3333, y = -3.5449, loss = 98.2587\nEpoch 7: x = 2.8947, y = -3.4144, loss = 136.7721\nEpoch 8: x = 2.4421, y = -2.6141, loss = 124.9600\nEpoch 9: x = 1.9859, y = -1.3710, loss = 74.2980\nEpoch 10: x = 1.5356, y = 0.0220, loss = 22.7398\n</pre>"},{"location":"learn/optimizers/02-SGDMomentum/#sgdmomentum","title":"SGDMomentum\u00b6","text":"<p>SGD with Momentum is an enhancement over standard SGD that helps it navigate the landscape of high-curvature regions by incorporating information from previous gradients. This reduces oscillations and speeds up the optimization process.</p>"},{"location":"learn/optimizers/02-SGDMomentum/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>Let $\\theta$ represent the parameters and $\\nabla \\mathcal{L}(\\theta_t)$ the gradient at time $t$. SGDMomentum maintains a velocity vector $v_t$:</p> <p>$$ v_{t} = \\mu \\cdot v_{t-1} + \\nabla \\mathcal{L}(\\theta_t) $$ $$ \\theta_{t+1} = \\theta_t - \\eta \\cdot v_t $$</p> <p>where:</p> <ul> <li>$v_t$: Accumulated velocity at time $t$.</li> <li>$\\mu$: Momentum coefficient (typically 0.9).</li> <li>$\\eta$: Learning rate ($lr$).</li> </ul>"},{"location":"learn/optimizers/02-SGDMomentum/#implementation-details","title":"Implementation details\u00b6","text":"<p>In Sorix, the <code>SGDMomentum</code> optimizer keeps track of the velocity vectors in a list (<code>vts</code>). These vectors are stored on the same device as the parameters, ensuring consistency across CPU and GPU setups.</p>"},{"location":"learn/optimizers/03-RMSprop/","title":"RMSprop","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sorix import tensor\nfrom sorix.optim import RMSprop\nimport sorix\n</pre> import numpy as np from sorix import tensor from sorix.optim import RMSprop import sorix In\u00a0[3]: Copied! <pre># Minimize an anisotropic function: f(x, y) = x^2 + 10*y^2\n# RMSprop normalizes the update using the moving average of squared gradients,\n# effectively equalizing the step sizes across parameters with different gradient magnitudes.\nx = tensor([5.0], requires_grad=True)\ny = tensor([5.0], requires_grad=True)\noptimizer = RMSprop([x, y], lr=0.1)\n\nfor epoch in range(10):\n    loss = x * x + tensor([10.0]) * y * y\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")\n</pre> # Minimize an anisotropic function: f(x, y) = x^2 + 10*y^2 # RMSprop normalizes the update using the moving average of squared gradients, # effectively equalizing the step sizes across parameters with different gradient magnitudes. x = tensor([5.0], requires_grad=True) y = tensor([5.0], requires_grad=True) optimizer = RMSprop([x, y], lr=0.1)  for epoch in range(10):     loss = x * x + tensor([10.0]) * y * y          optimizer.zero_grad()     loss.backward()     optimizer.step()          print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")  <pre>Epoch 1: x = 4.6838, y = 4.6838, loss = 275.0000\nEpoch 2: x = 4.4616, y = 4.4616, loss = 241.3149\nEpoch 3: x = 4.2793, y = 4.2793, loss = 218.9631\nEpoch 4: x = 4.1201, y = 4.1201, loss = 201.4354\nEpoch 5: x = 3.9762, y = 3.9762, loss = 186.7233\nEpoch 6: x = 3.8433, y = 3.8433, loss = 173.9078\nEpoch 7: x = 3.7189, y = 3.7189, loss = 162.4813\nEpoch 8: x = 3.6011, y = 3.6011, loss = 152.1306\nEpoch 9: x = 3.4887, y = 3.4887, loss = 142.6466\nEpoch 10: x = 3.3808, y = 3.3808, loss = 133.8827\n</pre>"},{"location":"learn/optimizers/03-RMSprop/#rmsprop","title":"RMSprop\u00b6","text":"<p>RMSprop adaptively adjusts the learning rate for each parameter. It divides the learning rate by an exponentially decaying average of squared gradients, which prevents the learning rate from vanishing too quickly.</p>"},{"location":"learn/optimizers/03-RMSprop/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>The update rules for RMSprop are:</p> <p>$$ v_t = \\rho \\cdot v_{t-1} + (1 - \\rho) \\cdot (\\nabla \\mathcal{L}(\\theta_t))^2 $$ $$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} \\cdot \\nabla \\mathcal{L}(\\theta_t) $$</p> <p>where:</p> <ul> <li>$v_t$: Moving average of the squared gradients at time $t$.</li> <li>$\\rho$: Decay rate (often 0.9).</li> <li>$\\epsilon$: Small constant for numerical stability.</li> <li>$\\eta$: Learning rate ($lr$).</li> </ul>"},{"location":"learn/optimizers/03-RMSprop/#implementation-details","title":"Implementation details\u00b6","text":"<p>Sorix's <code>RMSprop</code> stores the historical gradients in the <code>vts</code> list. This adaptive method is particularly useful for recurrent neural networks and handling non-stationary objectives.</p>"},{"location":"learn/optimizers/04-Adam/","title":"Adam","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sorix import tensor\nfrom sorix.optim import Adam\nimport sorix\n</pre> import numpy as np from sorix import tensor from sorix.optim import Adam import sorix In\u00a0[3]: Copied! <pre># Miniizing an anisotropic function with Adam: f(x, y) = x^2 + 10*y^2\n# Adam combines momentum with adaptive scaling, making it exceptionally reliable\n# even with high learning rates on non-homogeneous surfaces.\nx = tensor([5.0], requires_grad=True)\ny = tensor([5.0], requires_grad=True)\noptimizer = Adam([x, y], lr=0.5)\n\nfor epoch in range(10):\n    loss = x * x + tensor([10.0]) * y * y\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")\n</pre> # Miniizing an anisotropic function with Adam: f(x, y) = x^2 + 10*y^2 # Adam combines momentum with adaptive scaling, making it exceptionally reliable # even with high learning rates on non-homogeneous surfaces. x = tensor([5.0], requires_grad=True) y = tensor([5.0], requires_grad=True) optimizer = Adam([x, y], lr=0.5)  for epoch in range(10):     loss = x * x + tensor([10.0]) * y * y          optimizer.zero_grad()     loss.backward()     optimizer.step()          print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")  <pre>Epoch 1: x = 4.5000, y = 4.5000, loss = 275.0000\nEpoch 2: x = 4.0021, y = 4.0021, loss = 222.7500\nEpoch 3: x = 3.5079, y = 3.5079, loss = 176.1814\nEpoch 4: x = 3.0197, y = 3.0197, loss = 135.3614\nEpoch 5: x = 2.5398, y = 2.5398, loss = 100.3042\nEpoch 6: x = 2.0712, y = 2.0712, loss = 70.9574\nEpoch 7: x = 1.6171, y = 1.6171, loss = 47.1878\nEpoch 8: x = 1.1813, y = 1.1813, loss = 28.7653\nEpoch 9: x = 0.7679, y = 0.7679, loss = 15.3507\nEpoch 10: x = 0.3812, y = 0.3812, loss = 6.4868\n</pre>"},{"location":"learn/optimizers/04-Adam/#adam","title":"Adam\u00b6","text":"<p>Adam is a sophisticated optimization algorithm that combines the elements of RMSprop and Momentum. It computes adaptive learning rates for each parameter by incorporating both first-order moments (the mean) and second-order moments (the uncentered variance) of the gradients.</p>"},{"location":"learn/optimizers/04-Adam/#mathematical-definition","title":"Mathematical definition\u00b6","text":"<p>Adam maintains two moving averages: $m_t$ (first moment) and $v_t$ (second moment). The update rules are:</p> <p>$$ m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot \\nabla \\mathcal{L}(\\theta_t) $$ $$ v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot (\\nabla \\mathcal{L}(\\theta_t))^2 $$</p> <p>Bias-corrected estimates are calculated to account for the initialization to zero at earlier time steps:</p> <p>$$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} $$ $$ \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} $$</p> <p>The parameters are updated using:</p> <p>$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t $$</p> <p>where:</p> <ul> <li>$\\beta_1, \\beta_2$: Exponential decay rates for the moment estimates (typically 0.9 and 0.999).</li> <li>$t$: Time step (iteration count).</li> <li>$\\epsilon$: Small constant for stability.</li> <li>$\\eta$: Learning rate ($lr$).</li> </ul>"},{"location":"learn/optimizers/04-Adam/#implementation-details","title":"Implementation details\u00b6","text":"<p>In Sorix, the <code>Adam</code> optimizer provides high computational efficiency and low memory overhead. It stores the state for $m_t$ and $v_t$ as lists and automatically performs device-specific calculations (CPU or GPU).</p>"},{"location":"learn/optimizers/05-Comparison/","title":"Optimizer Comparison","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sorix import tensor\nfrom sorix.optim import SGD, SGDMomentum, RMSprop, Adam\n</pre> import numpy as np import matplotlib.pyplot as plt from sorix import tensor from sorix.optim import SGD, SGDMomentum, RMSprop, Adam In\u00a0[3]: Copied! <pre>def optimize_banana(optimizer_class, lr, epochs=1000, **kwargs):\n    # Initial position far from the minimum (1, 1)\n    x_val = -2.0\n    y_val = 2.0\n    \n    x = tensor([x_val], requires_grad=True)\n    y = tensor([y_val], requires_grad=True)\n    \n    optimizer = optimizer_class([x, y], lr=lr, **kwargs)\n    \n    history = []\n    for epoch in range(epochs):\n        # f(x, y) = (1 - x)^2 + 100 * (y - x^2)^2\n        loss = (tensor([1.0]) - x)**2 + tensor([100.0]) * (y - x**2)**2\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # record path\n        history.append((float(x.data[0]), float(y.data[0]), float(loss.data[0])))\n        \n    return np.array(history)\n\n# Hyperparameters\nEPOCHS = 2000\n\nprint(\"Running optimizations...\")\nresults = {\n    \"SGD (lr=0.001)\": optimize_banana(SGD, lr=0.001, epochs=EPOCHS),\n    \"Momentum (lr=0.001)\": optimize_banana(SGDMomentum, lr=0.001, momentum=0.9, epochs=EPOCHS),\n    \"RMSprop (lr=0.01)\": optimize_banana(RMSprop, lr=0.01, epochs=EPOCHS),\n    \"Adam (lr=0.1)\": optimize_banana(Adam, lr=0.1, epochs=EPOCHS)\n}\nprint(\"Done!\")\n</pre> def optimize_banana(optimizer_class, lr, epochs=1000, **kwargs):     # Initial position far from the minimum (1, 1)     x_val = -2.0     y_val = 2.0          x = tensor([x_val], requires_grad=True)     y = tensor([y_val], requires_grad=True)          optimizer = optimizer_class([x, y], lr=lr, **kwargs)          history = []     for epoch in range(epochs):         # f(x, y) = (1 - x)^2 + 100 * (y - x^2)^2         loss = (tensor([1.0]) - x)**2 + tensor([100.0]) * (y - x**2)**2                  optimizer.zero_grad()         loss.backward()         optimizer.step()                  # record path         history.append((float(x.data[0]), float(y.data[0]), float(loss.data[0])))              return np.array(history)  # Hyperparameters EPOCHS = 2000  print(\"Running optimizations...\") results = {     \"SGD (lr=0.001)\": optimize_banana(SGD, lr=0.001, epochs=EPOCHS),     \"Momentum (lr=0.001)\": optimize_banana(SGDMomentum, lr=0.001, momentum=0.9, epochs=EPOCHS),     \"RMSprop (lr=0.01)\": optimize_banana(RMSprop, lr=0.01, epochs=EPOCHS),     \"Adam (lr=0.1)\": optimize_banana(Adam, lr=0.1, epochs=EPOCHS) } print(\"Done!\") <pre>Running optimizations...\n</pre> <pre>Done!\n</pre> In\u00a0[4]: Copied! <pre>plt.figure(figsize=(14, 6))\n\n# 1. Loss Convergence\nplt.subplot(1, 2, 1)\nfor name, history in results.items():\n    plt.plot(history[:, 2], label=name)\nplt.yscale('log')\nplt.title(\"Loss Convergence (Log Scale)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 2. Parameter Space Path\nplt.subplot(1, 2, 2)\nX_grid, Y_grid = np.meshgrid(np.linspace(-2.5, 2.5, 100), np.linspace(-1, 4, 100))\nZ_grid = (1 - X_grid)**2 + 100 * (Y_grid - X_grid**2)**2\n\nplt.contour(X_grid, Y_grid, Z_grid, levels=np.logspace(0, 5, 20), cmap='viridis', alpha=0.4)\nplt.plot(1, 1, 'r*', markersize=15, label=\"Goal (1, 1)\") # Global minimum\n\nfor name, history in results.items():\n    plt.plot(history[:, 0], history[:, 1], label=name, linewidth=2)\n\nplt.title(\"Optimization Path in Parameter Space\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(14, 6))  # 1. Loss Convergence plt.subplot(1, 2, 1) for name, history in results.items():     plt.plot(history[:, 2], label=name) plt.yscale('log') plt.title(\"Loss Convergence (Log Scale)\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.legend() plt.grid(True, alpha=0.3)  # 2. Parameter Space Path plt.subplot(1, 2, 2) X_grid, Y_grid = np.meshgrid(np.linspace(-2.5, 2.5, 100), np.linspace(-1, 4, 100)) Z_grid = (1 - X_grid)**2 + 100 * (Y_grid - X_grid**2)**2  plt.contour(X_grid, Y_grid, Z_grid, levels=np.logspace(0, 5, 20), cmap='viridis', alpha=0.4) plt.plot(1, 1, 'r*', markersize=15, label=\"Goal (1, 1)\") # Global minimum  for name, history in results.items():     plt.plot(history[:, 0], history[:, 1], label=name, linewidth=2)  plt.title(\"Optimization Path in Parameter Space\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend() plt.grid(True, alpha=0.3)  plt.tight_layout() plt.show()"},{"location":"learn/optimizers/05-Comparison/#optimizer-comparison","title":"Optimizer Comparison\u00b6","text":"<p>Selecting the right optimizer is crucial for the success of a deep learning model. In this guide, we compare the convergence properties of the optimizers available in Sorix using a classic test problem: the Rosenbrock function (also known as the Banana function).</p>"},{"location":"learn/optimizers/05-Comparison/#the-rosenbrock-function","title":"The Rosenbrock Function\u00b6","text":"<p>The Rosenbrock function is a non-convex function often used as a performance test for optimization algorithms. It is defined as:</p> <p>$$ f(x, y) = (a - x)^2 + b(y - x^2)^2 $$</p> <p>Typically, $a=1$ and $b=100$. The global minimum is at $(x, y) = (1, 1)$, located inside a long, narrow, parabolic-shaped flat valley. Finding the valley is easy, but converging to the global minimum is difficult for simple algorithms due to the flat landscape.</p>"},{"location":"learn/optimizers/05-Comparison/#observations","title":"Observations\u00b6","text":"<ol> <li><p>SGD (Blue): Shows the highest difficulty in navigating the Rosenbrock landscape. Because it lacks momentum or adaptive scaling, it moves extremely slowly once it enters the narrow valley floor where gradients are near zero. Even after 2000 epochs, it is still far from the goal.</p> </li> <li><p>Momentum (Orange): Demonstrates high \"energy\" and speed. It reaches the lowest loss the fastest, but its path is very unstable. In the parameter space, you can see large \"zig-zag\" oscillations as the velocity carries the optimizer across the steep walls of the valley rather than following the curve smoothly.</p> </li> <li><p>RMSprop (Green) &amp; Adam (Red): These adaptive optimizers show the most efficient and smooth paths. By scaling the learning rate for each parameter individually (dividing by the moving average of squared gradients), they effectively normalize the steepness of the terrain. They quickly find the bottom of the valley and track it cleanly toward the global minimum at $(1, 1)$.</p> </li> </ol>"},{"location":"learn/optimizers/05-Comparison/#conclusion","title":"Conclusion\u00b6","text":"<p>While Momentum can achieve very low loss quickly through brute force, Adam and RMSprop provide significantly better stability and precision on complex, anisotropic surfaces. For most deep learning tasks, the adaptive scaling and bias correction of Adam make it the most robust choice.</p>"},{"location":"learn/optimizers/06-Optimizer/","title":"Optimizer","text":"In\u00a0[1]: Copied! <pre># Uncomment the next line and run this cell to install sorix\n#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn'\n</pre> # Uncomment the next line and run this cell to install sorix #!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main/docs_learn/docs_learn/docs_learn/docs_learn' In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sorix.optim import Optimizer\nfrom sorix import tensor\n\nclass SignSGD(Optimizer):\n    def __init__(self, parameters, lr=0.01):\n        # Initialize using the base class\n        super().__init__(parameters, lr)\n        \n    def step(self):\n        for param in self.parameters:\n            if param.grad is not None:\n                # Only update based on the sign of the gradient\n                # We use self.xp to correctly handle CPU or GPU\n                param.data -= self.lr * self.xp.sign(param.grad)\n\n# Create a simple model\nw = tensor([10.0], requires_grad=True)\noptim = SignSGD([w], lr=2.0)\n\nprint(f\"Initial value: {w.item():.2f}\")\nfor i in range(5):\n    loss = (w - 2.0)**2\n    loss.backward()\n    optim.step()\n    optim.zero_grad()\n    print(f\"Step {i+1} | Value: {w.item():.2f} (Moved by exactly 2.0 per step)\")\n</pre> import numpy as np from sorix.optim import Optimizer from sorix import tensor  class SignSGD(Optimizer):     def __init__(self, parameters, lr=0.01):         # Initialize using the base class         super().__init__(parameters, lr)              def step(self):         for param in self.parameters:             if param.grad is not None:                 # Only update based on the sign of the gradient                 # We use self.xp to correctly handle CPU or GPU                 param.data -= self.lr * self.xp.sign(param.grad)  # Create a simple model w = tensor([10.0], requires_grad=True) optim = SignSGD([w], lr=2.0)  print(f\"Initial value: {w.item():.2f}\") for i in range(5):     loss = (w - 2.0)**2     loss.backward()     optim.step()     optim.zero_grad()     print(f\"Step {i+1} | Value: {w.item():.2f} (Moved by exactly 2.0 per step)\") <pre>Initial value: 10.00\nStep 1 | Value: 8.00 (Moved by exactly 2.0 per step)\nStep 2 | Value: 6.00 (Moved by exactly 2.0 per step)\nStep 3 | Value: 4.00 (Moved by exactly 2.0 per step)\nStep 4 | Value: 2.00 (Moved by exactly 2.0 per step)\nStep 5 | Value: 2.00 (Moved by exactly 2.0 per step)\n</pre> In\u00a0[3]: Copied! <pre>class MovingAverageSGD(Optimizer):\n    def __init__(self, parameters, lr=0.01, beta=0.9):\n        super().__init__(parameters, lr)\n        self.beta = beta\n        # Pre-allocate a list of buffers (one for each parameter)\n        self.m = [self.xp.zeros_like(p.data) for p in self.parameters]\n        \n    def step(self):\n        for i, param in enumerate(self.parameters):\n            if param.grad is None:\n                continue\n            \n            # Update moving average: m = beta*m + (1-beta)*grad\n            self.m[i] = self.beta * self.m[i] + (1 - self.beta) * param.grad\n            \n            # Perform weight update\n            param.data -= self.lr * self.m[i]\n\nprint(\"MovingAverageSGD created successfully with List state management!\")\n</pre> class MovingAverageSGD(Optimizer):     def __init__(self, parameters, lr=0.01, beta=0.9):         super().__init__(parameters, lr)         self.beta = beta         # Pre-allocate a list of buffers (one for each parameter)         self.m = [self.xp.zeros_like(p.data) for p in self.parameters]              def step(self):         for i, param in enumerate(self.parameters):             if param.grad is None:                 continue                          # Update moving average: m = beta*m + (1-beta)*grad             self.m[i] = self.beta * self.m[i] + (1 - self.beta) * param.grad                          # Perform weight update             param.data -= self.lr * self.m[i]  print(\"MovingAverageSGD created successfully with List state management!\") <pre>MovingAverageSGD created successfully with List state management!\n</pre> In\u00a0[4]: Copied! <pre>from sorix.nn import Linear, MSELoss\n\nmodel = Linear(5, 1)\noptimizer = SignSGD(model.parameters(), lr=0.01)\ncriterion = MSELoss()\n\n# Simple linear target: y = sum(x)\nX = tensor(np.random.randn(100, 5))\ny = tensor(np.sum(X.numpy(), axis=1, keepdims=True))\n\nprint(\"Training model with custom SignSGD...\")\nfor epoch in range(101):\n    y_pred = model(X)\n    loss = criterion(y_pred, y)\n    \n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    \n    if epoch % 20 == 0:\n        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.6f}\")\n</pre> from sorix.nn import Linear, MSELoss  model = Linear(5, 1) optimizer = SignSGD(model.parameters(), lr=0.01) criterion = MSELoss()  # Simple linear target: y = sum(x) X = tensor(np.random.randn(100, 5)) y = tensor(np.sum(X.numpy(), axis=1, keepdims=True))  print(\"Training model with custom SignSGD...\") for epoch in range(101):     y_pred = model(X)     loss = criterion(y_pred, y)          loss.backward()     optimizer.step()     optimizer.zero_grad()          if epoch % 20 == 0:         print(f\"Epoch {epoch:3d} | Loss: {loss.item():.6f}\") <pre>Training model with custom SignSGD...\nEpoch   0 | Loss: 5.047922\nEpoch  20 | Loss: 3.456228\nEpoch  40 | Loss: 2.279963\nEpoch  60 | Loss: 1.405424\nEpoch  80 | Loss: 0.754761\nEpoch 100 | Loss: 0.327485\n</pre>"},{"location":"learn/optimizers/06-Optimizer/#optimizer","title":"Optimizer\u00b6","text":"<p>In Sorix, the <code>Optimizer</code> base class serves as the foundation for all optimization algorithms (like SGD, Adam, or RMSprop). It provides common utilities for updating model parameters but leaves the actual update logic to its subclasses.</p> <p>By inheriting from <code>Optimizer</code>, you can easily implement your own optimization logic for research or specialized use cases.</p>"},{"location":"learn/optimizers/06-Optimizer/#anatomy-of-the-optimizer-class","title":"Anatomy of the Optimizer Class\u00b6","text":"<p>Every optimizer in Sorix must follow a simple contract:</p> <ol> <li><code>__init__(self, parameters, lr)</code>: Receives a list of <code>Tensor</code> objects to optimize and a learning rate.</li> <li><code>zero_grad()</code>: Clears the <code>.grad</code> attribute of all parameters.</li> <li><code>step()</code>: The heart of the optimizer, where you define how to modify <code>parameter.data</code> using <code>parameter.grad</code>.</li> </ol>"},{"location":"learn/optimizers/06-Optimizer/#1-creating-a-custom-optimizer","title":"1. Creating a Custom Optimizer\u00b6","text":"<p>Let's implement a Sign Gradient Descent optimizer. Instead of scaling the gradient, it only looks at the sign (direction) of the gradient and moves by a fixed step size $\\eta$:</p> <p>$$w = w - \\eta \\cdot \\text{sign}(\\nabla w)$$</p> <p>This can be useful for robust optimization in noisy environments.</p>"},{"location":"learn/optimizers/06-Optimizer/#2-managing-internal-state","title":"2. Managing Internal State\u00b6","text":"<p>Many optimizers (like Adam or SGD with Momentum) need to track additional state for each parameter across time (e.g., historical gradients).</p> <p>For maximum efficiency, Sorix prefers using Lists to store these states. Lists allow for direct indexing, which is faster than hash-map lookups in dictionaries.</p>"},{"location":"learn/optimizers/06-Optimizer/#3-training-proof","title":"3. Training Proof\u00b6","text":"<p>Let's see if our <code>SignSGD</code> optimizer can actually train a small model to solve a problem. If the gradients correctly guide the direction, the model should converge regardless of the gradient magnitude.</p>"},{"location":"learn/optimizers/06-Optimizer/#conclusion","title":"Conclusion\u00b6","text":"<p>The <code>Optimizer</code> base class makes it incredibly easy to experiment with new learning algorithms. All you need to do is subclass it and implement the <code>step()</code> method to manipulate your parameters' data based on their gradients. Sorix handles everything else, including zeroing out gradients and managing hardware-specific operations through <code>self.xp</code>.</p>"}]}