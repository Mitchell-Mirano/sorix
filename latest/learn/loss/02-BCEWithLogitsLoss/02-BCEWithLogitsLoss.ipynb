{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCEWithLogitsLoss\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mitchell-Mirano/sorix/blob/main/docs/learn/loss/02-BCEWithLogitsLoss.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/Mitchell-Mirano/sorix/blob/main/docs/learn/loss/02-BCEWithLogitsLoss.ipynb)\n",
    "[![Open in Docs](https://img.shields.io/badge/Open%20in-Docs-blue?logo=readthedocs)](http://127.0.0.1:8000/sorix/learn/loss/02-BCEWithLogitsLoss)\n",
    "\n",
    "\n",
    "**Binary Cross Entropy** (BCE) measures the distance between the distribution of outcomes and predictions. For stability, Sorix implements `BCEWithLogitsLoss`, which includes a **Sigmoid** activation inside the loss function.\n",
    "\n",
    "The total loss is the average over all $n$ samples in the batch:\n",
    "\n",
    "$$L = - \\frac{1}{n} \\sum_{i=1}^{n} [y_i \\ln(\\sigma(\\hat{y}_i)) + (1 - y_i) \\ln(1 - \\sigma(\\hat{y}_i))]$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}_i$ are logarithmic odds (logits).\n",
    "- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the Sigmoid function.\n",
    "- $y_i$ is the target (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Stability: The Log-Sum-Exp Trick\n",
    "\n",
    "Directly calculating $\\ln(\\sigma(x))$ can lead to numerical instability. For example, if $x$ is a large positive number, $\\sigma(x) \\approx 1$, and $\\ln(1) = 0$. However, if $x$ is a large negative number, $\\sigma(x) \\approx 0$, and $\\ln(0)$ is undefined ($-\\infty$).\n",
    "\n",
    "To avoid this, Sorix uses a mathematically equivalent but numerically stable form for each element:\n",
    "\n",
    "### Mathematical Derivation\n",
    "\n",
    "We know that $\\ln(\\sigma(x)) = \\ln(\\frac{1}{1+e^{-x}}) = -\\ln(1+e^{-x})$.\n",
    "And $\\ln(1-\\sigma(x)) = \\ln(\\frac{e^{-x}}{1+e^{-x}}) = -x - \\ln(1+e^{-x})$.\n",
    "\n",
    "Substituting these into the BCE formula for a single element:\n",
    "$$l = - [y (-\\ln(1+e^{-x})) + (1-y)(-x - \\ln(1+e^{-x}))]$$\n",
    "$$l = y \\ln(1+e^{-x}) + (1-y)x + (1-y)\\ln(1+e^{-x})$$\n",
    "$$l = (1-y)x + \\ln(1+e^{-x})$$\n",
    "\n",
    "To make it stable for both large positive and negative $x$, we use the identity $\\ln(1+e^{-x}) = \\max(-x, 0) + \\ln(1+e^{-|x|})$. The final stable per-element loss implemented in Sorix is:\n",
    "\n",
    "$$l = \\max(x, 0) - x \\cdot y + \\ln(1 + e^{-|x|})$$\n",
    "\n",
    "And the final loss is the mean of these values: $L = \\text{mean}(l)$.\n",
    "\n",
    "### Implementation Optimization\n",
    "Sorix further optimizes this by reusing intermediate values ($e^{-|x|}$) to calculate both the loss and the probabilities needed for the gradient, avoiding redundant exponential calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line and run this cell to install sorix\n",
    "#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:08:08.678040Z",
     "iopub.status.busy": "2026-03-01T06:08:08.677810Z",
     "iopub.status.idle": "2026-03-01T06:08:08.864364Z",
     "shell.execute_reply": "2026-03-01T06:08:08.863857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits (Extremes): [ 100. -100.    0.]\n",
      "Targets:           [1. 0. 1.]\n",
      "Stable BCE Loss:    0.2310 (No NaNs or Warnings!)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sorix import tensor\n",
    "from sorix.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Create logits (+ve for class 1, -ve for class 0)\n",
    "logits = tensor([100.0, -100.0, 0.0], requires_grad=True)\n",
    "targets = tensor([1.0, 0.0, 1.0])\n",
    "\n",
    "criterion = BCEWithLogitsLoss()\n",
    "loss = criterion(logits, targets)\n",
    "\n",
    "print(f\"Logits (Extremes): {logits.numpy()}\")\n",
    "print(f\"Targets:           {targets.numpy()}\")\n",
    "print(f\"Stable BCE Loss:    {loss.item():.4f} (No NaNs or Warnings!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification with Autograd\n",
    "\n",
    "The gradient of this combined function is also remarkably simple and stable:\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{1}{n}(\\sigma(x) - y)$$\n",
    "\n",
    "This prevents the \"vanishing gradient\" problem often seen when activation and loss are calculated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:08:08.865952Z",
     "iopub.status.busy": "2026-03-01T06:08:08.865787Z",
     "iopub.status.idle": "2026-03-01T06:08:08.868812Z",
     "shell.execute_reply": "2026-03-01T06:08:08.868465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients w.r.t logits: [ 0.          0.         -0.16666667]\n",
      "Manual Gradients:       [ 0.0000000e+00  1.2611686e-44 -1.6666667e-01]\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(f\"Gradients w.r.t logits: {logits.grad}\")\n",
    "\n",
    "# dL/d_logit = 1/n * (sigma(logit) - target)\n",
    "n = logits.data.size\n",
    "x = logits.data\n",
    "\n",
    "# Truly stable sigmoid reusing e^{-|x|}\n",
    "abs_x = np.abs(x)\n",
    "exp_neg_abs_x = np.exp(-abs_x)\n",
    "denom = 1 + exp_neg_abs_x\n",
    "probs = np.where(x >= 0, 1 / denom, exp_neg_abs_x / denom)\n",
    "\n",
    "manual_grad = (probs - targets.data) / n\n",
    "print(f\"Manual Gradients:       {manual_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Example\n",
    "\n",
    "Let's see how `BCEWithLogitsLoss` matches current logits to a desired outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:08:08.870004Z",
     "iopub.status.busy": "2026-03-01T06:08:08.869904Z",
     "iopub.status.idle": "2026-03-01T06:08:08.874094Z",
     "shell.execute_reply": "2026-03-01T06:08:08.873814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Logits: -5.00\n",
      "Step  0 | Logit: -4.5033 | Prob: 0.0110 | Loss: 5.0067\n",
      "Step  5 | Logit: -2.0912 | Prob: 0.1100 | Loss: 2.6300\n",
      "Step 10 | Logit: -0.1809 | Prob: 0.4549 | Loss: 0.9685\n",
      "Step 15 | Logit: 0.8868 | Prob: 0.7082 | Loss: 0.3955\n",
      "Step 20 | Logit: 1.4911 | Prob: 0.8162 | Loss: 0.2221\n",
      "\n",
      "Final Logit: 1.49 (Close to +ve for class 1)\n"
     ]
    }
   ],
   "source": [
    "from sorix.optim import SGD\n",
    "\n",
    "logits = tensor([-5.0], requires_grad=True) # Starting at class 0\n",
    "target = tensor([1.0]) # Target class 1\n",
    "optimizer = SGD([logits], lr=0.5)\n",
    "\n",
    "print(f\"Initial Logits: {logits.item():.2f}\")\n",
    "\n",
    "for i in range(21):\n",
    "    loss = criterion(logits, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if i % 5 == 0:\n",
    "        # Probability after Sigmoid\n",
    "        prob = 1 / (1 + np.exp(-logits.item()))\n",
    "        print(f\"Step {i:2d} | Logit: {logits.item():6.4f} | Prob: {prob:6.4f} | Loss: {loss.item():6.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Logit: {logits.item():.2f} (Close to +ve for class 1)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}