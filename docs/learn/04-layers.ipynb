{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce70a5d8",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mitchell-Mirano/sorix/blob/qa/docs/learn/04-layers.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/Mitchell-Mirano/sorix/blob/qa/docs/learn/04-layers.ipynb)\n",
    "[![Open in Docs](https://img.shields.io/badge/Open%20in-Docs-blue?logo=readthedocs)](http://127.0.0.1:8000/learn/04-layers/)\n",
    "\n",
    "\n",
    "\n",
    "The layers are implemented as Python classes that encapsulate one or more fundamental tensor operations. Each layer defines its trainable parameters as class attributes represented by `tensor` objects, which are initialized with `requires_grad=True` by default to enable automatic differentiation.\n",
    "\n",
    "These layers are designed to be composable and device-aware, supporting execution on both CPU and GPU backends. Parameter initialization follows standard schemes (e.g., He or Xavier initialization), and all learnable parameters are exposed through a unified interface for optimization. Non-trainable quantities, such as running statistics in normalization layers, are handled as buffers and are therefore excluded from gradient computation.\n",
    "\n",
    "At a high level, the available layers include linear transformations, nonlinear activation functions, and normalization modules. Each class defines the forward computation via the `__call__` interface, while gradient propagation is handled internally through the underlying tensor autograd mechanism. Detailed mathematical formulations and implementation specifics of each layer are discussed in the following sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba096bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line and run this cell to install sorix\n",
    "#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@qa'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b415bf",
   "metadata": {},
   "source": [
    "## Import Sorix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb07aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sorix import tensor\n",
    "from sorix.nn import ReLU,Linear,BatchNorm1d\n",
    "import sorix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b58960",
   "metadata": {},
   "source": [
    "## Linear\n",
    "\n",
    "The **Linear** layer implements an affine transformation between finite-dimensional real vector spaces and constitutes a fundamental operator in deep learning architectures. Formally, it defines a linear mapping from an input feature space to an output representation space, optionally augmented by a bias term. This transformation is applied independently to each element of a batch.\n",
    "\n",
    "### Mathematical definition\n",
    "\n",
    "Let  $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$\n",
    "be an input tensor representing a batch of $N$ samples, where each sample is a vector in a $d$-dimensional feature space. The Linear layer defines the affine transformation\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}$$\n",
    "\n",
    "where the involved quantities have the following dimensions:\n",
    "\n",
    "- $\n",
    "  \\mathbf{X} \\in \\mathbb{R}^{N \\times d}\n",
    "  $ : input batch matrix  \n",
    "- $\n",
    "  \\mathbf{W} \\in \\mathbb{R}^{d \\times m}\n",
    "  $ : weight matrix (trainable parameters)  \n",
    "- $\n",
    "  \\mathbf{b} \\in \\mathbb{R}^{1 \\times m}\n",
    "  $ : bias vector associated with the output neurons  \n",
    "- $\n",
    "  \\mathbf{Y} \\in \\mathbb{R}^{N \\times m}\n",
    "  $ : output tensor  \n",
    "- $\n",
    "  m\n",
    "  $ : number of neurons, i.e., the dimensionality of the output space  \n",
    "\n",
    "From a dimensional analysis standpoint, the matrix product\n",
    "\n",
    "$$\n",
    "\\mathbf{X}\\mathbf{W} :\n",
    "\\mathbb{R}^{N \\times d} \\times \\mathbb{R}^{d \\times m}\n",
    "\\;\\longrightarrow\\;\n",
    "\\mathbb{R}^{N \\times m}\n",
    "$$\n",
    "\n",
    "is well-defined. The bias term $\\mathbf{b}$ is then added **column-wise** to the resulting matrix, meaning that each component $b_j$ is added to all entries of the $j$-th output column. Explicitly,\n",
    "\n",
    "$$\n",
    "Y_{ij} = (\\mathbf{X}\\mathbf{W})_{ij} + b_j,\n",
    "\\quad\n",
    "i = 1,\\dots,N,\\;\n",
    "j = 1,\\dots,m.\n",
    "$$\n",
    "\n",
    "### Interpretation as a linear mapping\n",
    "\n",
    "At the level of individual samples, for each\n",
    "$\n",
    "i \\in \\{1, \\dots, N\\},\n",
    "$\n",
    "the transformation can be written as\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_i = \\mathbf{x}_i \\mathbf{W} + \\mathbf{b},\n",
    "\\quad\n",
    "\\mathbf{x}_i \\in \\mathbb{R}^{1 \\times d},\\;\n",
    "\\mathbf{y}_i \\in \\mathbb{R}^{1 \\times m}.\n",
    "$$\n",
    "\n",
    "Thus, each output vector $\\mathbf{y}_i$ is obtained as a linear combination of the input features, defined by the columns of $\\mathbf{W}$, followed by a translation in the output space determined by the bias vector $\\mathbf{b}$.\n",
    "\n",
    "### Functional view\n",
    "\n",
    "The Linear layer realizes the mapping\n",
    "\n",
    "$$\n",
    "\\text{Linear}:\\;\n",
    "\\mathbb{R}^{N \\times d}\n",
    "\\;\\longrightarrow\\;\n",
    "\\mathbb{R}^{N \\times m},\n",
    "$$\n",
    "\n",
    "where the same affine transformation is applied independently to each sample in the batch. This operator forms the mathematical foundation upon which more complex nonlinear models are constructed when composed with activation and normalization layers.\n",
    "\n",
    "### Parameterization and gradients\n",
    "\n",
    "The parameters $\\mathbf{W}$ and $\\mathbf{b}$ are represented as `tensor` objects with `requires_grad=True`, enabling automatic gradient computation via automatic differentiation. During backpropagation, the following gradients are computed:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}}, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}},\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}$ denotes the global loss function of the model.\n",
    "\n",
    "### Parameter initialization\n",
    "\n",
    "The weight matrix $\\mathbf{W}$ is initialized from a zero-mean normal distribution with a standard deviation determined by the chosen initialization scheme:\n",
    "\n",
    "- **He initialization** (recommended for ReLU-like activations):\n",
    "  $$\n",
    "  \\sigma = \\sqrt{\\frac{2}{d}}.\n",
    "  $$\n",
    "\n",
    "- **Xavier initialization** (suitable for symmetric activations such as $\\tanh$):\n",
    "  $$\n",
    "  \\sigma = \\sqrt{\\frac{2}{d + m}}.\n",
    "  $$\n",
    "\n",
    "Formally,\n",
    "$$\n",
    "W_{ij} \\sim \\mathcal{N}(0, \\sigma^2).\n",
    "$$\n",
    "\n",
    "When present, the bias vector $\\mathbf{b}$ is initialized to zero.\n",
    "\n",
    "### Forward computation\n",
    "\n",
    "Given an input tensor $\\mathbf{X}$, the forward evaluation of the layer is performed through the matrix operation\n",
    "\n",
    "$$\n",
    "\\text{Linear}(\\mathbf{X}) = \\mathbf{X}\\mathbf{W} + \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "In the implementation, this computation is exposed via the `__call__` method, enabling a concise and functional syntax consistent with the rest of the framework.\n",
    "\n",
    "### Multi-device support\n",
    "\n",
    "The Linear layer is device-aware. Parameters and computations may reside on either CPU or GPU, using NumPy or CuPy as the numerical backend, respectively. The `to(device)` method ensures consistent parameter transfer across devices while preserving the mathematical semantics of the transformation.\n",
    "\n",
    "### Parameter interface\n",
    "\n",
    "The trainable parameters of the layer are exposed through the `parameters()` method, which returns the set\n",
    "\n",
    "$$\n",
    "\\{\\mathbf{W}, \\mathbf{b}\\},\n",
    "$$\n",
    "\n",
    "or only $\\mathbf{W}$ when the bias term is disabled. This abstraction allows direct integration with gradient-based optimization algorithms.\n",
    "\n",
    "### Statistical interpretation\n",
    "\n",
    "From a statistical perspective, the Linear layer can be interpreted as a multivariate linear regression model, where each output neuron represents a linear combination of the input features. In this context, the coefficients of the weight matrix $\\mathbf{W}$ and the bias vector $\\mathbf{b}$ define hyperplanes in the output space that approximate the relationship between input and output variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b7db5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[-2.11857762  1.68792676  1.31994672]\n",
       " [ 1.5472385   0.1157047  -0.47949683]\n",
       " [-0.60946832 -0.26404178  1.0974149 ]\n",
       " [-0.02944926  1.27697084  0.55260467]\n",
       " [-1.14989104  1.20767261  0.87711009]\n",
       " [-0.20930424 -1.50447589  1.70364656]\n",
       " [-0.52887809 -2.09572447  1.52516325]\n",
       " [ 0.33512794  1.30232238  2.43443063]\n",
       " [ 1.12924792 -0.34338007  0.09819372]\n",
       " [-3.17633068 -0.30102419  1.39405847]], shape=(10, 3), device=cpu, requires_grad=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create random input data\n",
    "samples = 10\n",
    "features = 3\n",
    "neurons = 2\n",
    "\n",
    "# X ∈ ℝ^(samples × features)\n",
    "X = tensor(np.random.randn(samples, features))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a6bf6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\n",
      "[[-0.07543034  0.36379951]\n",
      " [-0.64783009 -1.21172978]\n",
      " [ 0.53614753  1.63158125]], shape=(3, 2), device=cpu, requires_grad=True)\n",
      "Tensor(\n",
      "[[0. 0.]], shape=(1, 2), device=cpu, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# instantiate a Linear layer: ℝ^(samples × features) → ℝ^(samples × neurons)\n",
    "linear = Linear(features, neurons)\n",
    "\n",
    "# weight matrix W ∈ ℝ^(features × neurons)\n",
    "print(linear.W)\n",
    "\n",
    "# bias vector b ∈ ℝ^(1 × neurons)\n",
    "print(linear.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7a7330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\n",
      "[[-0.22599853 -0.66244831]\n",
      " [-0.44874675 -0.35965625]\n",
      " [ 0.8054029   1.88874458]\n",
      " [-0.52876113 -0.6564378 ]\n",
      " [-0.22536957 -0.45062629]\n",
      " [ 1.90383853  4.52651126]\n",
      " [ 2.21527932  4.83547393]\n",
      " [ 0.43625153  2.51582794]\n",
      " [ 0.18991871  0.98711474]\n",
      " [ 1.18202524  1.48373209]], shape=(10, 2), device=cpu, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# forward pass:\n",
    "# Y ∈ ℝ^(samples × neurons) = X @ W + b\n",
    "Y = linear(X)\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce7fa4",
   "metadata": {},
   "source": [
    "## Batch Normalization (BatchNorm1d)\n",
    "\n",
    "The **BatchNorm1d** layer implements *batch normalization*, a technique designed to stabilize and accelerate the training of deep neural networks by reducing internal covariate shift. This is achieved by normalizing intermediate activations across the batch dimension and subsequently applying a learnable affine transformation.\n",
    "\n",
    "### Mathematical definition\n",
    "\n",
    "Let $ \\mathbf{X} \\in \\mathbb{R}^{N \\times d} $\n",
    "be an input tensor representing a batch of $N$ samples, where each sample has $d$ features. Batch normalization operates **feature-wise**, normalizing each feature independently across the batch.\n",
    "\n",
    "During training, the batch-wise mean and variance are computed as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_B\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "\\mathbf{x}_i\n",
    "\\;\\in\\;\n",
    "\\mathbb{R}^{1 \\times d},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\sigma}_B^2\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "(\\mathbf{x}_i - \\boldsymbol{\\mu}_B)^2\n",
    "\\;\\in\\;\n",
    "\\mathbb{R}^{1 \\times d},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i \\in \\mathbb{R}^{1 \\times d}$ denotes the $i$-th sample in the batch.\n",
    "\n",
    "### Normalization step\n",
    "\n",
    "Each input sample is normalized using the batch statistics:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{X}}\n",
    "=\n",
    "\\frac{\\mathbf{X} - \\boldsymbol{\\mu}_B}\n",
    "{\\sqrt{\\boldsymbol{\\sigma}_B^2 + \\varepsilon}},\n",
    "\\quad\n",
    "\\widehat{\\mathbf{X}} \\in \\mathbb{R}^{N \\times d},\n",
    "$$\n",
    "\n",
    "where $\\varepsilon > 0$ is a small constant introduced for numerical stability.\n",
    "\n",
    "### Learnable affine transformation\n",
    "\n",
    "To preserve the representational capacity of the network, batch normalization introduces two learnable parameters:\n",
    "\n",
    "- Scale parameter:\n",
    "  $\n",
    "  \\boldsymbol{\\gamma} \\in \\mathbb{R}^{1 \\times d}\n",
    "  $\n",
    "- Shift parameter:\n",
    "  $\n",
    "  \\boldsymbol{\\beta} \\in \\mathbb{R}^{1 \\times d}\n",
    "  $\n",
    "\n",
    "The final output of the layer is given by\n",
    "\n",
    "$$\n",
    "\\mathbf{Y}\n",
    "=\n",
    "\\boldsymbol{\\gamma} \\odot \\widehat{\\mathbf{X}}\n",
    "+\n",
    "\\boldsymbol{\\beta},\n",
    "\\quad\n",
    "\\mathbf{Y} \\in \\mathbb{R}^{N \\times d},\n",
    "$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication applied **column-wise**, i.e., independently to each feature.\n",
    "\n",
    "### Running statistics and inference mode\n",
    "\n",
    "In addition to batch statistics, BatchNorm1d maintains *running estimates* of the mean and variance:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_{\\text{run}} \\in \\mathbb{R}^{1 \\times d},\n",
    "\\quad\n",
    "\\boldsymbol{\\sigma}^2_{\\text{run}} \\in \\mathbb{R}^{1 \\times d}.\n",
    "$$\n",
    "\n",
    "These statistics are updated during training using an exponential moving average:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_{\\text{run}}\n",
    "\\leftarrow\n",
    "\\alpha \\boldsymbol{\\mu}_{\\text{run}}\n",
    "+\n",
    "(1 - \\alpha)\\boldsymbol{\\mu}_B,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\sigma}^2_{\\text{run}}\n",
    "\\leftarrow\n",
    "\\alpha \\boldsymbol{\\sigma}^2_{\\text{run}}\n",
    "+\n",
    "(1 - \\alpha)\\boldsymbol{\\sigma}^2_B,\n",
    "$$\n",
    "\n",
    "where $\\alpha \\in (0,1)$ is the momentum parameter controlling the update rate.\n",
    "\n",
    "During inference (evaluation mode), normalization is performed using these accumulated running statistics instead of the batch statistics, ensuring deterministic behavior:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{X}}\n",
    "=\n",
    "\\frac{\\mathbf{X} - \\boldsymbol{\\mu}_{\\text{run}}}\n",
    "{\\sqrt{\\boldsymbol{\\sigma}^2_{\\text{run}} + \\varepsilon}}.\n",
    "$$\n",
    "\n",
    "### Functional view\n",
    "\n",
    "The BatchNorm1d layer realizes the mapping\n",
    "\n",
    "$$\n",
    "\\text{BatchNorm1d}:\\;\n",
    "\\mathbb{R}^{N \\times d}\n",
    "\\;\\longrightarrow\\;\n",
    "\\mathbb{R}^{N \\times d},\n",
    "$$\n",
    "\n",
    "where normalization and affine reparameterization are applied independently to each feature across the batch.\n",
    "\n",
    "### Parameterization and gradients\n",
    "\n",
    "The learnable parameters $\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\beta}$ are represented as `tensor` objects with `requires_grad=True`. Gradients are computed with respect to these parameters, as well as with respect to the input tensor $\\mathbf{X}$, during backpropagation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\gamma}}, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\beta}}, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}}.\n",
    "$$\n",
    "\n",
    "The running statistics are treated as buffers and do not participate in gradient computation.\n",
    "\n",
    "### Multi-device support\n",
    "\n",
    "BatchNorm1d is device-aware and supports execution on CPU and GPU backends. Learnable parameters are stored as tensors on the selected device, while running statistics are maintained as NumPy or CuPy arrays and transferred consistently when changing devices via the `to(device)` method.\n",
    "\n",
    "### Parameter interface\n",
    "\n",
    "The trainable parameters of the layer are exposed through the `parameters()` method, which returns\n",
    "\n",
    "$$\n",
    "\\{\\boldsymbol{\\gamma}, \\boldsymbol{\\beta}\\}.\n",
    "$$\n",
    "\n",
    "### Statistical interpretation\n",
    "\n",
    "From a statistical perspective, BatchNorm1d performs a feature-wise standardization of the input distribution, followed by a learned affine transformation. This can be interpreted as dynamically re-centering and re-scaling the feature space, which improves numerical conditioning and facilitates optimization in deep networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64022e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[ 1.22129142 -0.0213213  -0.29432569]\n",
       " [-0.91636576 -0.06141393 -1.13231409]\n",
       " [-2.61212584 -0.89636751 -0.9959895 ]\n",
       " [-1.6714919  -1.37353929  0.9229026 ]\n",
       " [ 0.89873948 -0.36702833 -0.64483109]\n",
       " [ 0.52349229 -0.67205028 -0.62714142]\n",
       " [-0.14046412  0.33438818 -0.96513357]\n",
       " [ 0.80115808  1.47316211  0.29733451]], shape=(8, 3), device=cpu, requires_grad=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of samples and features\n",
    "samples = 8\n",
    "features = 3\n",
    "\n",
    "# input tensor X ∈ ℝ^(samples × features)\n",
    "X = tensor(np.random.randn(samples, features))\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1756ffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\n",
      "[[1. 1. 1.]], shape=(1, 3), device=cpu, requires_grad=True)\n",
      "Tensor(\n",
      "[[0. 0. 0.]], shape=(1, 3), device=cpu, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "bn = BatchNorm1d(features)\n",
    "\n",
    "# γ ∈ ℝ^(1 × features), β ∈ ℝ^(1 × features)\n",
    "print(bn.gamma)\n",
    "print(bn.beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "395d8ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[ 1.1334296   0.21814242  0.20320982]\n",
       " [-0.52805754  0.16864665 -1.05249018]\n",
       " [-1.8460816  -0.86213323 -0.84821195]\n",
       " [-1.1149769  -1.45121875  2.02718921]\n",
       " [ 0.88272715 -0.20864519 -0.32201181]\n",
       " [ 0.59106748 -0.58520564 -0.29550437]\n",
       " [ 0.0750095   0.65727842 -0.80197528]\n",
       " [ 0.80688232  2.06313531  1.08979455]], shape=(8, 3), device=cpu, requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass (training mode)\n",
    "Y = bn(X)\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "609c50e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02369708 -0.01980213 -0.04299373]]\n",
      "[[1.06553105 0.96561242 0.94453426]]\n"
     ]
    }
   ],
   "source": [
    "# running statistics after the forward pass\n",
    "print(bn.running_mean)\n",
    "print(bn.running_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a434c15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[ 1.20609147e+00 -1.54597652e-03 -2.58604792e-01]\n",
       " [-8.64779137e-01 -4.23460407e-02 -1.12084217e+00]\n",
       " [-2.50755880e+00 -8.92032437e-01 -9.80572746e-01]\n",
       " [-1.59631297e+00 -1.37762394e+00  9.93846602e-01]\n",
       " [ 8.93616984e-01 -3.53353027e-01 -6.19252818e-01]\n",
       " [ 5.30093599e-01 -6.63757116e-01 -6.01051251e-01]\n",
       " [-1.13118899e-01  3.60440022e-01 -9.48823932e-01]\n",
       " [ 7.99084306e-01  1.51930769e+00  3.50176361e-01]], shape=(8, 3), device=cpu, requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference mode\n",
    "bn.training = False\n",
    "Y_eval = bn(X)\n",
    "Y_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8674e",
   "metadata": {},
   "source": [
    "## ReLU (Rectified Linear Unit)\n",
    "\n",
    "The **ReLU** layer implements the *Rectified Linear Unit* activation function, one of the most widely used nonlinearities in deep learning due to its simplicity, computational efficiency, and favorable gradient properties. ReLU introduces nonlinearity by applying an element-wise thresholding operation to its input.\n",
    "\n",
    "### Mathematical definition\n",
    "\n",
    "Let  \n",
    "$\n",
    "\\mathbf{X} \\in \\mathbb{R}^{N \\times d}\n",
    "$\n",
    "be an input tensor representing a batch of $N$ samples with $d$ features. The ReLU activation is defined element-wise as\n",
    "\n",
    "$$\n",
    "\\operatorname{ReLU}(x) = \\max(0, x).\n",
    "$$\n",
    "\n",
    "Applied to a tensor, this yields\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\operatorname{ReLU}(\\mathbf{X}),\n",
    "\\quad\n",
    "Y_{ij} = \\max(0, X_{ij}),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\n",
    "  \\mathbf{Y} \\in \\mathbb{R}^{N \\times d}\n",
    "  $\n",
    "  is the output tensor,\n",
    "- $\n",
    "  i = 1,\\dots,N\n",
    "  $\n",
    "  indexes the samples,\n",
    "- $\n",
    "  j = 1,\\dots,d\n",
    "  $\n",
    "  indexes the features.\n",
    "\n",
    "The transformation preserves the dimensionality of the input:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}:\\;\n",
    "\\mathbb{R}^{N \\times d}\n",
    "\\;\\longrightarrow\\;\n",
    "\\mathbb{R}^{N \\times d}.\n",
    "$$\n",
    "\n",
    "### Forward computation\n",
    "\n",
    "In the forward pass, ReLU performs a simple element-wise comparison with zero, retaining positive values and setting negative values to zero. This operation is computationally inexpensive and trivially parallelizable.\n",
    "\n",
    "Formally,\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\max(\\mathbf{0}, \\mathbf{X}),\n",
    "$$\n",
    "\n",
    "where the maximum is taken element-wise.\n",
    "\n",
    "### Backward computation (gradient)\n",
    "\n",
    "The derivative of the ReLU function is given by\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\operatorname{ReLU}(x)\n",
    "=\n",
    "\\begin{cases}\n",
    "1, & x > 0, \\\\\n",
    "0, & x \\le 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Accordingly, during backpropagation, the gradient with respect to the input tensor is computed as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial X_{ij}}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Y_{ij}}\n",
    "\\cdot\n",
    "\\mathbb{I}(X_{ij} > 0),\n",
    "$$\n",
    "\n",
    "where $\\mathbb{I}(\\cdot)$ denotes the indicator function.\n",
    "\n",
    "This matches the implementation, where the upstream gradient is masked by the condition $X > 0$.\n",
    "\n",
    "### Autograd implementation details\n",
    "\n",
    "In the provided implementation, the output tensor stores a reference to the input tensor and defines a custom backward function:\n",
    "\n",
    "- The forward pass computes $\\mathbf{Y} = \\max(0, \\mathbf{X})$.\n",
    "- The backward pass accumulates gradients in the input tensor via\n",
    "  $\n",
    "  X.\\text{grad} \\mathrel{+}= \\text{out.grad} \\cdot (X > 0).\n",
    "  $\n",
    "\n",
    "This ensures correct gradient flow through the activation during optimization.\n",
    "\n",
    "### Device awareness\n",
    "\n",
    "The ReLU layer is device-aware and supports execution on both CPU and GPU. The numerical backend (NumPy or CuPy) is selected dynamically based on the device associated with the input tensor, ensuring consistency and performance portability.\n",
    "\n",
    "### Functional role in neural networks\n",
    "\n",
    "From a functional standpoint, ReLU introduces nonlinearity while preserving sparsity in activations, as negative inputs are mapped exactly to zero. This property mitigates the vanishing gradient problem commonly observed with saturating nonlinearities and enables efficient training of deep architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "348ce6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[-0.89057293 -0.26609062 -0.07709271]\n",
       " [-0.32024358 -1.6020258   0.18186056]\n",
       " [ 0.06751868  0.94575422 -0.27170854]\n",
       " [-0.93181959  0.22443158 -1.08736002]\n",
       " [-1.66797155 -0.89579603  2.077023  ]\n",
       " [ 0.68590973 -2.62432205  1.08619278]\n",
       " [ 0.38510981 -0.49339594 -0.00567828]\n",
       " [-0.34719925  0.55854604 -0.53541034]], shape=(8, 3), device=cpu, requires_grad=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of samples and features\n",
    "samples = 8\n",
    "features = 3\n",
    "\n",
    "# input tensor X ∈ ℝ^(samples × features)\n",
    "X = tensor(np.random.randn(samples, features))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7c06bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[0.         0.         0.        ]\n",
       " [0.         0.         0.18186056]\n",
       " [0.06751868 0.94575422 0.        ]\n",
       " [0.         0.22443158 0.        ]\n",
       " [0.         0.         2.077023  ]\n",
       " [0.68590973 0.         1.08619278]\n",
       " [0.38510981 0.         0.        ]\n",
       " [0.         0.55854604 0.        ]], shape=(8, 3), device=cpu, requires_grad=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "ReLU(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df012d91",
   "metadata": {},
   "source": [
    "## ReLu + BatchNorm1d + Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00761fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[0.7899231  0.        ]\n",
       " [0.76963545 0.        ]\n",
       " [0.         1.27633776]\n",
       " [0.59711683 0.        ]\n",
       " [0.69790922 0.6917095 ]\n",
       " [0.62584084 0.        ]\n",
       " [0.         0.88076543]\n",
       " [0.         0.81695402]], shape=(8, 2), device=cpu, requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of samples and features\n",
    "samples = 8\n",
    "features = 3\n",
    "neurons = 2\n",
    "\n",
    "# input tensor X ∈ ℝ^(samples × features)\n",
    "X = tensor(np.random.randn(samples, features))\n",
    "\n",
    "#Linear: X ∈ ℝ^(samples × features) -> X ∈ ℝ^(samples × neurons)\n",
    "linear = Linear(features,neurons)\n",
    "\n",
    "#BatchNorm1d: X ∈ ℝ^(samples × neurons) -> X ∈ ℝ^(samples × neurons)\n",
    "bn = BatchNorm1d(neurons)\n",
    "\n",
    "#Relu: X ∈ ℝ^(samples × neurons) -> X ∈ ℝ^(samples × neurons)\n",
    "relu = ReLU()\n",
    "\n",
    "Y = ReLU(bn(linear(X)))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28396e24",
   "metadata": {},
   "source": [
    "## Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39f3e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU basic operation passed\n",
      "✅ GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "CUDA runtime version: 13000\n",
      "CuPy version: 13.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gpu'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'gpu' if sorix.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec5022a",
   "metadata": {},
   "source": [
    "## Linear + GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47fd2869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[-5.16703327 -3.59290969]\n",
       " [ 2.79656698  2.81765267]\n",
       " [ 1.39700557  1.11127165]\n",
       " [-1.22504713 -0.81609523]\n",
       " [ 0.15312325 -0.12838952]\n",
       " [-0.22987993 -0.93993913]\n",
       " [ 1.69610384 -1.71439391]\n",
       " [-0.35339305  0.65142666]\n",
       " [ 0.67518722 -0.39164053]\n",
       " [ 0.22420753 -0.41889924]], shape=(10, 2), device=gpu, requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = 5\n",
    "classes = 2\n",
    "examples = 10\n",
    "\n",
    "X = tensor(np.random.randn(examples, features), device=device)\n",
    "linear = Linear(features,classes).to(device)\n",
    "linear(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9524f",
   "metadata": {},
   "source": [
    "## Batch Norm + GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd888e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[-2.5330769  -2.00942872]\n",
       " [ 1.37349087  1.95325517]\n",
       " [ 0.68693186  0.89845714]\n",
       " [-0.59932389 -0.29294312]\n",
       " [ 0.07674169  0.1321616 ]\n",
       " [-0.11114166 -0.36949713]\n",
       " [ 0.8336554  -0.84822572]\n",
       " [-0.17173139  0.61420433]\n",
       " [ 0.33284172 -0.0305668 ]\n",
       " [ 0.11161229 -0.04741675]], shape=(10, 2), device=gpu, requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = BatchNorm1d(classes).to(device)\n",
    "bn(linear(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b835d",
   "metadata": {},
   "source": [
    "## ReLU + GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4525d0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[0.         0.         0.         1.42350407 0.        ]\n",
       " [1.44277295 1.88202868 0.         0.         0.6079959 ]\n",
       " [0.         1.41685226 0.         0.         0.50730439]\n",
       " [0.55991084 0.         0.         0.52641969 0.        ]\n",
       " [0.16115509 0.53765079 0.         0.         0.        ]\n",
       " [0.         0.0046861  0.23670877 0.         0.18401204]\n",
       " [1.49590944 0.         0.         0.         0.85012796]\n",
       " [0.         0.75653219 0.48502291 0.         0.        ]\n",
       " [1.22528896 0.         0.50027597 0.         0.87791816]\n",
       " [0.         0.13026655 0.         0.         0.        ]], shape=(10, 5), device=gpu, requires_grad=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "ReLU(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7255ae1e",
   "metadata": {},
   "source": [
    "## Linear + ReLU + Batch Norm + GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ead2cff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(\n",
       "[[0.         0.        ]\n",
       " [1.37349087 1.95325517]\n",
       " [0.68693186 0.89845714]\n",
       " [0.         0.        ]\n",
       " [0.07674169 0.1321616 ]\n",
       " [0.         0.        ]\n",
       " [0.8336554  0.        ]\n",
       " [0.         0.61420433]\n",
       " [0.33284172 0.        ]\n",
       " [0.11161229 0.        ]], shape=(10, 2), device=gpu, requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLU(bn(linear(X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}