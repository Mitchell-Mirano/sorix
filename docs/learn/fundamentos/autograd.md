## üß† Autograd: C√°lculo Autom√°tico de Derivadas

El motor `autograd` de Sorix te permite calcular las derivadas de las funciones de manera autom√°tica, lo que te permite enfocarte en la definici√≥n de la funci√≥n de p√©rdida y no preocuparte por la implementaci√≥n de las derivadas.

En esencia, cada operaci√≥n que realizas con un **Tensor** crea un **gr√°fico de c√≥mputo** (Computation Graph) que registra c√≥mo se lleg√≥ al valor final. El m√©todo `.backward()` recorre este gr√°fico a la inversa (de la p√©rdida a los par√°metros) aplicando la **regla de la cadena** para calcular los gradientes.

### El Ejemplo Cl√°sico: Error Cuadr√°tico Medio (MSE)

El Error Cuadr√°tico Medio (MSE) es la funci√≥n de p√©rdida m√°s com√∫n para los problemas de regresi√≥n. En un modelo lineal simple ($\hat{y} = xw$), el objetivo es calcular la derivada de la p√©rdida ($L$) con respecto al par√°metro $w$: $\frac{\partial L}{\partial w}$.

Aqu√≠ se demuestra c√≥mo realizar este c√°lculo con Sorix:

```python
# 1. Importar Tensor (asumimos que ya est√° disponible)
from sorix import Tensor
import numpy as np

# --- 1. Datos de Entrada ---
# Caracter√≠stica de entrada X
X = Tensor(np.array([1.0, 2.0]))
# Etiqueta real Y
Y_true = np.array([3.0, 4.0])

# --- 2. Par√°metro a Optimizar ---

# El tensor 'w' es el par√°metro del modelo. 
# requires_grad=True es CRUCIAL para que autograd rastree sus operaciones.
w = Tensor(1.0, requires_grad=True)

# --- 3. Pase Adelante (Forward Pass) ---

# Modelo lineal simple: Y_hat = X * w
Y_hat = X * w

# --- 4. C√°lculo de la P√©rdida (MSE) ---

# Error = (Y_hat - Y_true)
error = Y_hat - Y_true

# P√©rdida (L) = mean(error**2)
loss = (error**2).mean()

# --- 5. Backpropagation y Gradiente ---

# Ejecutar el backpropagation. Esto computa los gradientes de 'loss' 
# con respecto a todos los tensores que tengan requires_grad=True.
loss.backward()

# --- 6. Resultado ---

print(f"Valor de la p√©rdida (L): {loss.item():.4f}")
print(f"Gradiente de W (dL/dw): {w.grad:.4f}")
```

### Explicaci√≥n del Resultado

Cuando se llama a `loss.backward()`, el gr√°fico de c√≥mputo se eval√∫a. El valor final del gradiente se almacena en el atributo **`.grad`** del tensor `w`.

En este ejemplo:

  * Si $w=1.0$, la predicci√≥n es $\hat{y} = [1.0, 2.0]$.
  * El error es $Y_{true} - \hat{y} = [3, 4] - [1, 2] = [2, 2]$.
  * El gradiente $\frac{\partial L}{\partial w}$ calculado por `autograd` es **-6.0** (verificado manualmente por la regla de la cadena).

El motor `autograd` ha calculado el valor exacto que se usar√≠a para actualizar el peso `w` en un algoritmo de optimizaci√≥n como el Descenso de Gradiente.