{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mitchell-Mirano/sorix/blob/qa/docs/learn/optimizers/06-Optimizer.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/Mitchell-Mirano/sorix/blob/qa/docs/learn/optimizers/06-Optimizer.ipynb)\n",
    "[![Open in Docs](https://img.shields.io/badge/Open%20in-Docs-blue?logo=readthedocs)](http://127.0.0.1:8000/sorix/learn/optimizers/06-Optimizer)\n",
    "\n",
    "In Sorix, the `Optimizer` base class serves as the foundation for all optimization algorithms (like SGD, Adam, or RMSprop). It provides common utilities for updating model parameters but leaves the actual update logic to its subclasses.\n",
    "\n",
    "By inheriting from `Optimizer`, you can easily implement your own optimization logic for research or specialized use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of the Optimizer Class\n",
    "\n",
    "Every optimizer in Sorix must follow a simple contract:\n",
    "\n",
    "1. **`__init__(self, parameters, lr)`**: Receives a list of `Tensor` objects to optimize and a learning rate.\n",
    "2. **`zero_grad()`**: Clears the `.grad` attribute of all parameters.\n",
    "3. **`step()`**: The heart of the optimizer, where you define how to modify `parameter.data` using `parameter.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Custom Optimizer\n",
    "\n",
    "Let's implement a **Sign Gradient Descent** optimizer. Instead of scaling the gradient, it only looks at the sign (direction) of the gradient and moves by a fixed step size $\\eta$:\n",
    "\n",
    "$$w = w - \\eta \\cdot \\text{sign}(\\nabla w)$$\n",
    "\n",
    "This can be useful for robust optimization in noisy environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:47.561574Z",
     "iopub.status.busy": "2026-03-01T06:33:47.561247Z",
     "iopub.status.idle": "2026-03-01T06:33:47.565253Z",
     "shell.execute_reply": "2026-03-01T06:33:47.564464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line and run this cell to install sorix\n",
    "#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@qa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:47.567755Z",
     "iopub.status.busy": "2026-03-01T06:33:47.567509Z",
     "iopub.status.idle": "2026-03-01T06:33:47.754622Z",
     "shell.execute_reply": "2026-03-01T06:33:47.753991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value: 10.00\n",
      "Step 1 | Value: 8.00 (Moved by exactly 2.0 per step)\n",
      "Step 2 | Value: 6.00 (Moved by exactly 2.0 per step)\n",
      "Step 3 | Value: 4.00 (Moved by exactly 2.0 per step)\n",
      "Step 4 | Value: 2.00 (Moved by exactly 2.0 per step)\n",
      "Step 5 | Value: 2.00 (Moved by exactly 2.0 per step)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sorix.optim import Optimizer\n",
    "from sorix import tensor\n",
    "\n",
    "class SignSGD(Optimizer):\n",
    "    def __init__(self, parameters, lr=0.01):\n",
    "        # Initialize using the base class\n",
    "        super().__init__(parameters, lr)\n",
    "        \n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                # Only update based on the sign of the gradient\n",
    "                # We use self.xp to correctly handle CPU or GPU\n",
    "                param.data -= self.lr * self.xp.sign(param.grad)\n",
    "\n",
    "# Create a simple model\n",
    "w = tensor([10.0], requires_grad=True)\n",
    "optim = SignSGD([w], lr=2.0)\n",
    "\n",
    "print(f\"Initial value: {w.item():.2f}\")\n",
    "for i in range(5):\n",
    "    loss = (w - 2.0)**2\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    print(f\"Step {i+1} | Value: {w.item():.2f} (Moved by exactly 2.0 per step)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Managing Internal State\n",
    "\n",
    "Many optimizers (like Adam or SGD with Momentum) need to track additional state for each parameter across time (e.g., historical gradients). \n",
    "\n",
    "For maximum efficiency, Sorix prefers using **Lists** to store these states. Lists allow for direct indexing, which is faster than hash-map lookups in dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:47.756281Z",
     "iopub.status.busy": "2026-03-01T06:33:47.756115Z",
     "iopub.status.idle": "2026-03-01T06:33:47.759512Z",
     "shell.execute_reply": "2026-03-01T06:33:47.759002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovingAverageSGD created successfully with List state management!\n"
     ]
    }
   ],
   "source": [
    "class MovingAverageSGD(Optimizer):\n",
    "    def __init__(self, parameters, lr=0.01, beta=0.9):\n",
    "        super().__init__(parameters, lr)\n",
    "        self.beta = beta\n",
    "        # Pre-allocate a list of buffers (one for each parameter)\n",
    "        self.m = [self.xp.zeros_like(p.data) for p in self.parameters]\n",
    "        \n",
    "    def step(self):\n",
    "        for i, param in enumerate(self.parameters):\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "            \n",
    "            # Update moving average: m = beta*m + (1-beta)*grad\n",
    "            self.m[i] = self.beta * self.m[i] + (1 - self.beta) * param.grad\n",
    "            \n",
    "            # Perform weight update\n",
    "            param.data -= self.lr * self.m[i]\n",
    "\n",
    "print(\"MovingAverageSGD created successfully with List state management!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Proof\n",
    "\n",
    "Let's see if our `SignSGD` optimizer can actually train a small model to solve a problem. If the gradients correctly guide the direction, the model should converge regardless of the gradient magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:47.760901Z",
     "iopub.status.busy": "2026-03-01T06:33:47.760769Z",
     "iopub.status.idle": "2026-03-01T06:33:47.788282Z",
     "shell.execute_reply": "2026-03-01T06:33:47.787682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with custom SignSGD...\n",
      "Epoch   0 | Loss: 5.047922\n",
      "Epoch  20 | Loss: 3.456228\n",
      "Epoch  40 | Loss: 2.279963\n",
      "Epoch  60 | Loss: 1.405424\n",
      "Epoch  80 | Loss: 0.754761\n",
      "Epoch 100 | Loss: 0.327485\n"
     ]
    }
   ],
   "source": [
    "from sorix.nn import Linear, MSELoss\n",
    "\n",
    "model = Linear(5, 1)\n",
    "optimizer = SignSGD(model.parameters(), lr=0.01)\n",
    "criterion = MSELoss()\n",
    "\n",
    "# Simple linear target: y = sum(x)\n",
    "X = tensor(np.random.randn(100, 5))\n",
    "y = tensor(np.sum(X.numpy(), axis=1, keepdims=True))\n",
    "\n",
    "print(\"Training model with custom SignSGD...\")\n",
    "for epoch in range(101):\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The `Optimizer` base class makes it incredibly easy to experiment with new learning algorithms. All you need to do is subclass it and implement the `step()` method to manipulate your parameters' data based on their gradients. Sorix handles everything else, including zeroing out gradients and managing hardware-specific operations through `self.xp`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}