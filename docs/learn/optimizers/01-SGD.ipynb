{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sgd_header",
   "metadata": {},
   "source": [
    "# SGD\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mitchell-Mirano/sorix/blob/qa/docs/learn/optimizers/01-SGD.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/Mitchell-Mirano/sorix/blob/qa/docs/learn/optimizers/01-SGD.ipynb)\n",
    "[![Open in Docs](https://img.shields.io/badge/Open%20in-Docs-blue?logo=readthedocs)](http://127.0.0.1:8000/sorix/learn/optimizers/01-SGD)\n",
    "\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** is a fundamental optimization algorithm used in machine learning. It updates the model parameters by taking a step in the direction of the negative gradient of the loss function.\n",
    "\n",
    "## Mathematical definition\n",
    "\n",
    "Let $\\theta$ represent the parameters of the model and $\\mathcal{L}$ the loss function. The update rule for SGD is defined as:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla \\mathcal{L}(\\theta_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\theta_t$: Parameters at time $t$\n",
    "- $\\eta$: Learning rate ($lr$), a positive scalar determining the step size.\n",
    "- $\\nabla \\mathcal{L}(\\theta_t)$: Gradient of the loss with respect to the parameters at time $t$.\n",
    "\n",
    "## Implementation details\n",
    "\n",
    "In Sorix, the `SGD` optimizer iterates through the parameters and updates their `data` attribute using the calculated `grad`. This operation is performed in-place and handles both CPU and GPU tensors automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sgd_install",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:37.426782Z",
     "iopub.status.busy": "2026-03-01T06:33:37.426516Z",
     "iopub.status.idle": "2026-03-01T06:33:37.430439Z",
     "shell.execute_reply": "2026-03-01T06:33:37.429778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line and run this cell to install sorix\n",
    "#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@qa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sgd_imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:37.432892Z",
     "iopub.status.busy": "2026-03-01T06:33:37.432692Z",
     "iopub.status.idle": "2026-03-01T06:33:37.643011Z",
     "shell.execute_reply": "2026-03-01T06:33:37.642555Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sorix import tensor\n",
    "from sorix.optim import SGD\n",
    "import sorix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sgd_example",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:37.644629Z",
     "iopub.status.busy": "2026-03-01T06:33:37.644495Z",
     "iopub.status.idle": "2026-03-01T06:33:37.648351Z",
     "shell.execute_reply": "2026-03-01T06:33:37.647903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: x = 4.9000, y = 4.0000, loss = 275.0000\n",
      "Epoch 2: x = 4.8020, y = 3.2000, loss = 184.0100\n",
      "Epoch 3: x = 4.7060, y = 2.5600, loss = 125.4592\n",
      "Epoch 4: x = 4.6118, y = 2.0480, loss = 87.6821\n",
      "Epoch 5: x = 4.5196, y = 1.6384, loss = 63.2121\n",
      "Epoch 6: x = 4.4292, y = 1.3107, loss = 47.2704\n",
      "Epoch 7: x = 4.3406, y = 1.0486, loss = 36.7978\n",
      "Epoch 8: x = 4.2538, y = 0.8389, loss = 29.8362\n",
      "Epoch 9: x = 4.1687, y = 0.6711, loss = 25.1318\n",
      "Epoch 10: x = 4.0854, y = 0.5369, loss = 21.8820\n"
     ]
    }
   ],
   "source": [
    "# Simple optimization example: minimize an anisotropic parabolic function: f(x, y) = x^2 + 10*y^2\n",
    "# This surface challenges standard SGD as it tends to oscillate in the steeper y-direction\n",
    "x = tensor([5.0], requires_grad=True)\n",
    "y = tensor([5.0], requires_grad=True)\n",
    "optimizer = SGD([x, y], lr=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # compute loss: f(x, y) = x^2 + 10*y^2\n",
    "    loss = x * x + tensor([10.0]) * y * y\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}