{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rmsprop_header",
   "metadata": {},
   "source": [
    "# RMSprop\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mitchell-Mirano/sorix/blob/develop/docs/learn/optimizers/03-RMSprop.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/Mitchell-Mirano/sorix/blob/develop/docs/learn/optimizers/03-RMSprop.ipynb)\n",
    "[![Open in Docs](https://img.shields.io/badge/Open%20in-Docs-blue?logo=readthedocs)](http://127.0.0.1:8000/sorix/learn/optimizers/03-RMSprop)\n",
    "\n",
    "\n",
    "**RMSprop** adaptively adjusts the learning rate for each parameter. It divides the learning rate by an exponentially decaying average of squared gradients, which prevents the learning rate from vanishing too quickly.\n",
    "\n",
    "## Mathematical definition\n",
    "\n",
    "The update rules for RMSprop are:\n",
    "\n",
    "$$\n",
    "v_t = \\rho \\cdot v_{t-1} + (1 - \\rho) \\cdot (\\nabla \\mathcal{L}(\\theta_t))^2\n",
    "$$\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} \\cdot \\nabla \\mathcal{L}(\\theta_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $v_t$: Moving average of the squared gradients at time $t$.\n",
    "- $\\rho$: Decay rate (often 0.9).\n",
    "- $\\epsilon$: Small constant for numerical stability.\n",
    "- $\\eta$: Learning rate ($lr$).\n",
    "\n",
    "## Implementation details\n",
    "\n",
    "Sorix's `RMSprop` stores the historical gradients in the `vts` **list**. This adaptive method is particularly useful for recurrent neural networks and handling non-stationary objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rmsprop_install",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:39.668976Z",
     "iopub.status.busy": "2026-03-01T06:33:39.668714Z",
     "iopub.status.idle": "2026-03-01T06:33:39.672274Z",
     "shell.execute_reply": "2026-03-01T06:33:39.671666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line and run this cell to install sorix\n",
    "#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@develop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rmsprop_imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:39.674476Z",
     "iopub.status.busy": "2026-03-01T06:33:39.674310Z",
     "iopub.status.idle": "2026-03-01T06:33:39.857071Z",
     "shell.execute_reply": "2026-03-01T06:33:39.856507Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sorix import tensor\n",
    "from sorix.optim import RMSprop\n",
    "import sorix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rmsprop_example",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:39.858575Z",
     "iopub.status.busy": "2026-03-01T06:33:39.858432Z",
     "iopub.status.idle": "2026-03-01T06:33:39.862685Z",
     "shell.execute_reply": "2026-03-01T06:33:39.862286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: x = 4.6838, y = 4.6838, loss = 275.0000\n",
      "Epoch 2: x = 4.4616, y = 4.4616, loss = 241.3149\n",
      "Epoch 3: x = 4.2793, y = 4.2793, loss = 218.9631\n",
      "Epoch 4: x = 4.1201, y = 4.1201, loss = 201.4354\n",
      "Epoch 5: x = 3.9762, y = 3.9762, loss = 186.7233\n",
      "Epoch 6: x = 3.8433, y = 3.8433, loss = 173.9078\n",
      "Epoch 7: x = 3.7189, y = 3.7189, loss = 162.4813\n",
      "Epoch 8: x = 3.6011, y = 3.6011, loss = 152.1306\n",
      "Epoch 9: x = 3.4887, y = 3.4887, loss = 142.6466\n",
      "Epoch 10: x = 3.3808, y = 3.3808, loss = 133.8827\n"
     ]
    }
   ],
   "source": [
    "# Minimize an anisotropic function: f(x, y) = x^2 + 10*y^2\n",
    "# RMSprop normalizes the update using the moving average of squared gradients,\n",
    "# effectively equalizing the step sizes across parameters with different gradient magnitudes.\n",
    "x = tensor([5.0], requires_grad=True)\n",
    "y = tensor([5.0], requires_grad=True)\n",
    "optimizer = RMSprop([x, y], lr=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = x * x + tensor([10.0]) * y * y\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
