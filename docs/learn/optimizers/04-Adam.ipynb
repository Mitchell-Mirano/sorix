{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adam_header",
   "metadata": {},
   "source": [
    "# Adam\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mitchell-Mirano/sorix/blob/main/docs/learn/optimizers/04-Adam.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/Mitchell-Mirano/sorix/blob/main/docs/learn/optimizers/04-Adam.ipynb)\n",
    "[![Open in Docs](https://img.shields.io/badge/Open%20in-Docs-blue?logo=readthedocs)](http://127.0.0.1:8000/sorix/learn/optimizers/04-Adam)\n",
    "\n",
    "\n",
    "**Adam** is a sophisticated optimization algorithm that combines the elements of RMSprop and Momentum. It computes adaptive learning rates for each parameter by incorporating both first-order moments (the mean) and second-order moments (the uncentered variance) of the gradients.\n",
    "\n",
    "## Mathematical definition\n",
    "\n",
    "Adam maintains two moving averages: $m_t$ (first moment) and $v_t$ (second moment). The update rules are:\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot \\nabla \\mathcal{L}(\\theta_t)\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot (\\nabla \\mathcal{L}(\\theta_t))^2\n",
    "$$\n",
    "\n",
    "Bias-corrected estimates are calculated to account for the initialization to zero at earlier time steps:\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "$$\n",
    "$$\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "The parameters are updated using:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\beta_1, \\beta_2$: Exponential decay rates for the moment estimates (typically 0.9 and 0.999).\n",
    "- $t$: Time step (iteration count).\n",
    "- $\\epsilon$: Small constant for stability.\n",
    "- $\\eta$: Learning rate ($lr$).\n",
    "\n",
    "## Implementation details\n",
    "\n",
    "In Sorix, the `Adam` optimizer provides high computational efficiency and low memory overhead. It stores the state for $m_t$ and $v_t$ as **lists** and automatically performs device-specific calculations (CPU or GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adam_install",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:43.823513Z",
     "iopub.status.busy": "2026-03-01T06:33:43.823243Z",
     "iopub.status.idle": "2026-03-01T06:33:43.827167Z",
     "shell.execute_reply": "2026-03-01T06:33:43.826275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line and run this cell to install sorix\n",
    "#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adam_imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:43.829817Z",
     "iopub.status.busy": "2026-03-01T06:33:43.829637Z",
     "iopub.status.idle": "2026-03-01T06:33:44.014200Z",
     "shell.execute_reply": "2026-03-01T06:33:44.013647Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sorix import tensor\n",
    "from sorix.optim import Adam\n",
    "import sorix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adam_example",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:33:44.016085Z",
     "iopub.status.busy": "2026-03-01T06:33:44.015935Z",
     "iopub.status.idle": "2026-03-01T06:33:44.019781Z",
     "shell.execute_reply": "2026-03-01T06:33:44.019544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: x = 4.5000, y = 4.5000, loss = 275.0000\n",
      "Epoch 2: x = 4.0021, y = 4.0021, loss = 222.7500\n",
      "Epoch 3: x = 3.5079, y = 3.5079, loss = 176.1814\n",
      "Epoch 4: x = 3.0197, y = 3.0197, loss = 135.3614\n",
      "Epoch 5: x = 2.5398, y = 2.5398, loss = 100.3042\n",
      "Epoch 6: x = 2.0712, y = 2.0712, loss = 70.9574\n",
      "Epoch 7: x = 1.6171, y = 1.6171, loss = 47.1878\n",
      "Epoch 8: x = 1.1813, y = 1.1813, loss = 28.7653\n",
      "Epoch 9: x = 0.7679, y = 0.7679, loss = 15.3507\n",
      "Epoch 10: x = 0.3812, y = 0.3812, loss = 6.4868\n"
     ]
    }
   ],
   "source": [
    "# Miniizing an anisotropic function with Adam: f(x, y) = x^2 + 10*y^2\n",
    "# Adam combines momentum with adaptive scaling, making it exceptionally reliable\n",
    "# even with high learning rates on non-homogeneous surfaces.\n",
    "x = tensor([5.0], requires_grad=True)\n",
    "y = tensor([5.0], requires_grad=True)\n",
    "optimizer = Adam([x, y], lr=0.5)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = x * x + tensor([10.0]) * y * y\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: x = {x.data[0]:.4f}, y = {y.data[0]:.4f}, loss = {loss.data[0]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}