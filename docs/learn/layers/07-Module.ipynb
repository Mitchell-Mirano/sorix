{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mitchell-Mirano/sorix/blob/qa/docs/learn/layers/06-Module.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/Mitchell-Mirano/sorix/blob/qa/docs/learn/layers/06-Module.ipynb)\n",
    "[![Open in Docs](https://img.shields.io/badge/Open%20in-Docs-blue?logo=readthedocs)](https://mitchell-mirano.github.io/sorix/latest/learn/layers/06-Module)\n",
    "\n",
    "In Sorix, the `Module` class is the fundamental building block for all neural network components. Whether you are building a simple activation function, a complex layer, or an entire deep neural network, you will almost always inherit from `Module`.\n",
    "\n",
    "Its design is intentionally similar to PyTorch's `nn.Module`, making it intuitive for those coming from other frameworks while remaining simple enough to extend manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features of `Module`\n",
    "\n",
    "1. **Automatic Parameter Tracking**: Any `Tensor` attribute that has `requires_grad=True` is automatically collected by the `.parameters()` method.\n",
    "2. **Sub-module Registration**: If you assign another `Module` as an attribute of your class, Sorix will recursively find its parameters as well.\n",
    "3. **Device Management**: The `.to(device)` method moves all parameters and sub-modules to CPU or GPU (via CuPy).\n",
    "4. **Training/Evaluation Modes**: The `.train()` and `.eval()` methods toggle the behavior of layers like `Dropout` and `BatchNorm1d` surface-wide.\n",
    "5. **State Management**: `.state_dict()` and `.load_state_dict()` allow for easy serialization of your model's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Custom Layer with Parameters\n",
    "\n",
    "A \"layer\" in Sorix is just a `Module` that performs a specific operation. While we have built-in layers like `Linear`, you can easily create your own. \n",
    "\n",
    "Let's implement a **Parametric ReLU (PReLU)**, which is like a standard ReLU but with a learned slope $\\alpha$ for negative values:\n",
    "\n",
    "$$f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T03:04:36.204497Z",
     "iopub.status.busy": "2026-03-01T03:04:36.204301Z",
     "iopub.status.idle": "2026-03-01T03:04:36.478190Z",
     "shell.execute_reply": "2026-03-01T03:04:36.477374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [-2.   1.  -0.5]\n",
      "Output with initial alpha=0.25: [-0.5    1.    -0.125]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sorix import tensor\n",
    "from sorix.nn import Module\n",
    "\n",
    "class PReLU(Module):\n",
    "    def __init__(self, size=1, initial_alpha=0.25):\n",
    "        super().__init__()\n",
    "        # alpha is a learned parameter\n",
    "        self.alpha = tensor(np.full(size, initial_alpha), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x > 0 returns a boolean mask (converted to float in operation)\n",
    "        # We use Sorix operations to stay within the autograd graph\n",
    "        pos = (x > 0) * x\n",
    "        neg = (x <= 0) * (self.alpha * x)\n",
    "        return pos + neg\n",
    "\n",
    "prelu = PReLU(size=1)\n",
    "x = tensor([-2.0, 1.0, -0.5])\n",
    "y = prelu(x)\n",
    "\n",
    "print(f\"Input: {x.numpy()}\")\n",
    "print(f\"Output with initial alpha=0.25: {y.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Autograd in Custom Layers\n",
    "\n",
    "To verify that our layer is indeed learning, we can perform a simple optimization step. If we want the output for negative numbers to be more positive, the optimizer should adjust `alpha` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T03:04:36.497131Z",
     "iopub.status.busy": "2026-03-01T03:04:36.496972Z",
     "iopub.status.idle": "2026-03-01T03:04:36.500736Z",
     "shell.execute_reply": "2026-03-01T03:04:36.500489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha before update: 0.2500\n",
      "Alpha after update: 0.1792\n",
      "New output: [-0.35833333  1.         -0.08958333]\n"
     ]
    }
   ],
   "source": [
    "from sorix.optim import SGD\n",
    "from sorix.nn import MSELoss\n",
    "\n",
    "optimizer = SGD(prelu.parameters(), lr=0.1)\n",
    "target = tensor([0.0, 1.0, 0.0]) # We want negative inputs to result in 0\n",
    "criterion = MSELoss()\n",
    "\n",
    "print(f\"Alpha before update: {prelu.alpha.item():.4f}\")\n",
    "\n",
    "# One training step\n",
    "y = prelu(x)\n",
    "loss = criterion(y, target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(f\"Alpha after update: {prelu.alpha.item():.4f}\")\n",
    "print(f\"New output: {prelu(x).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Composition: Residual Blocks\n",
    "\n",
    "Modern deep learning architectures (like ResNets) rely on **Skip Connections**. In Sorix, you can easily build complex re-usable blocks by nesting other modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T03:04:36.501948Z",
     "iopub.status.busy": "2026-03-01T03:04:36.501857Z",
     "iopub.status.idle": "2026-03-01T03:04:36.504685Z",
     "shell.execute_reply": "2026-03-01T03:04:36.504419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in block: 8\n"
     ]
    }
   ],
   "source": [
    "from sorix.nn import Linear, ReLU, BatchNorm1d\n",
    "\n",
    "class ResidualBlock(Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.bn1 = BatchNorm1d(dim)\n",
    "        self.relu = ReLU()\n",
    "        self.fc2 = Linear(dim, dim)\n",
    "        self.bn2 = BatchNorm1d(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        return self.relu(out + residual)\n",
    "\n",
    "block = ResidualBlock(10)\n",
    "print(f\"Number of parameters in block: {len(block.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Complete Architecture\n",
    "\n",
    "Let's build a ResNet-style MLP and train it on a simple synthetic regression task to prove that the entire stack (nested modules, residual connections, custom layers, and optimizers) works in harmony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T03:04:36.505874Z",
     "iopub.status.busy": "2026-03-01T03:04:36.505782Z",
     "iopub.status.idle": "2026-03-01T03:04:36.633362Z",
     "shell.execute_reply": "2026-03-01T03:04:36.632728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNetMLP...\n",
      "Epoch   0 | Loss: 4.135838\n",
      "Epoch  20 | Loss: 0.418998\n",
      "Epoch  40 | Loss: 0.248450\n",
      "Epoch  60 | Loss: 0.168582\n",
      "Epoch  80 | Loss: 0.120915\n",
      "Epoch 100 | Loss: 0.091177\n"
     ]
    }
   ],
   "source": [
    "class ResNetMLP(Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=2):\n",
    "        super().__init__()\n",
    "        self.stem = Linear(input_dim, hidden_dim)\n",
    "        self.blocks = [ResidualBlock(hidden_dim) for _ in range(num_blocks)]\n",
    "        self.prelu = PReLU(size=hidden_dim)\n",
    "        self.head = Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.prelu(x)\n",
    "        return self.head(x)\n",
    "\n",
    "model = ResNetMLP(input_dim=5, hidden_dim=16, output_dim=1)\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "criterion = MSELoss()\n",
    "\n",
    "# Create synthetic data: y = sum(x) \n",
    "X_train = tensor(np.random.randn(100, 5))\n",
    "y_train = tensor(np.sum(X_train.numpy(), axis=1, keepdims=True))\n",
    "\n",
    "print(\"Training ResNetMLP...\")\n",
    "for epoch in range(101):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameter and State Management\n",
    "\n",
    "One of the most powerful features of `Module` is the `.parameters()` method. It automatically crawls the object's attributes (including lists, dictionaries, and sub-models) to find everything that needs to be optimized.\n",
    "\n",
    "The `state_dict()` returns a dictionary mapping parameter names to their current values, perfect for saving weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T03:04:36.635037Z",
     "iopub.status.busy": "2026-03-01T03:04:36.634907Z",
     "iopub.status.idle": "2026-03-01T03:04:36.638183Z",
     "shell.execute_reply": "2026-03-01T03:04:36.637808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dict keys sample: ['stem.W', 'stem.b', 'prelu.alpha', 'head.W', 'head.b']\n",
      "\n",
      "Weights persistence verified!\n"
     ]
    }
   ],
   "source": [
    "from sorix import save, load\n",
    "\n",
    "# Get state dict\n",
    "sd = model.state_dict()\n",
    "print(\"State dict keys sample:\", list(sd.keys())[:5])\n",
    "\n",
    "# Save and Load\n",
    "save(sd, \"resnet_model.sor\")\n",
    "loaded_weights = load(\"resnet_model.sor\")\n",
    "\n",
    "new_model = ResNetMLP(input_dim=5, hidden_dim=16, output_dim=1)\n",
    "new_model.load_state_dict(loaded_weights)\n",
    "\n",
    "print(\"\\nWeights persistence verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Device and Mode Management\n",
    "\n",
    "Since our model contains `BatchNorm1d`, switching between `train()` and `eval()` is mandatory for correct inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T03:04:36.639507Z",
     "iopub.status.busy": "2026-03-01T03:04:36.639408Z",
     "iopub.status.idle": "2026-03-01T03:04:38.624142Z",
     "shell.execute_reply": "2026-03-01T03:04:38.623762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In training mode? False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU basic operation passed\n",
      "âœ… GPU available: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "CUDA runtime version: 13000\n",
      "CuPy version: 14.0.1\n",
      "Entire model and its nested blocks moved to GPU memory.\n"
     ]
    }
   ],
   "source": [
    "import sorix\n",
    "\n",
    "# Switch to evaluation mode (essential for BatchNorm/Dropout)\n",
    "model.eval()\n",
    "print(f\"In training mode? {model.training}\")\n",
    "\n",
    "# Move to GPU if available\n",
    "if sorix.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "    print(\"Entire model and its nested blocks moved to GPU memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By subclassing `Module`, you gain all the power of Sorix's ecosystem with minimal code. You can implement complex research architectures with skip connections and custom primitives, and Sorix will handle the gradients, optimization, and hardware acceleration for you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}