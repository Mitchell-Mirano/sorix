{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dropout_header",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mitchell-Mirano/sorix/blob/qa/docs/learn/layers/06-Dropout.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/Mitchell-Mirano/sorix/blob/qa/docs/learn/layers/06-Dropout.ipynb)\n",
    "[![Open in Docs](https://img.shields.io/badge/Open%20in-Docs-blue?logo=readthedocs)](https://mitchell-mirano.github.io/sorix/latest/learn/layers/06-Dropout)\n",
    "\n",
    "\n",
    "The **Dropout** layer implements a powerful regularization technique widely used in deep neural networks to prevent overfitting. During training, it randomly zeroes some of the elements of the input tensor with probability $p$ using samples from a Bernoulli distribution. This forces the network to learn more robust features and prevents the co-adaptation of neurons.\n",
    "\n",
    "## Mathematical definition\n",
    "\n",
    "During training, for each element $x$ of the input tensor, the output $y$ is computed as:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "0 & \\text{with probability } p, \\\\\n",
    "\\frac{x}{1-p} & \\text{with probability } 1-p.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The factor $\\frac{1}{1-p}$ ensures that the expected value of the output remains the same as during inference:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[y] = p \\cdot 0 + (1-p) \\cdot \\frac{x}{1-p} = x.\n",
    "$$\n",
    "\n",
    "## Training vs Inference\n",
    "\n",
    "Like all regularization layers, Dropout behaves differently depending on the model's mode:\n",
    "\n",
    "*   **Training Mode**: The mask is randomly generated and applied, and scaling is performed.\n",
    "*   **Evaluation Mode** (`model.train(False)`): Dropout acts as an identity function ($y = x$), as scaling was already handled during training.\n",
    "\n",
    "## Backward computation (gradient)\n",
    "\n",
    "The gradient is propagated through the mask used during the forward pass. If an element was zeroed out, its gradient will also be zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\text{mask} \\cdot \\frac{1}{1-p}.\n",
    "$$\n",
    "\n",
    "## Functional view\n",
    "\n",
    "The Dropout layer maps from $\\mathbb{R}^{N \\times d}$ to $\\mathbb{R}^{N \\times d}$. It is a zero-parameter layer (though it has the hyperparameter $p$), meaning it does not have trainable weights like Linear or BatchNorm1d.\n",
    "\n",
    "## Implementation specifics\n",
    "\n",
    "Sorix's Dropout implementation uses the input tensor's device to generate the random mask, ensuring that random operations are GPU-accelerated when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dropout_install",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T02:56:13.477058Z",
     "iopub.status.busy": "2026-03-01T02:56:13.476721Z",
     "iopub.status.idle": "2026-03-01T02:56:13.481986Z",
     "shell.execute_reply": "2026-03-01T02:56:13.480781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line and run this cell to install sorix\n",
    "#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@qa/docs_learn/docs_learn/docs_learn/docs_learn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dropout_imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T02:56:13.484908Z",
     "iopub.status.busy": "2026-03-01T02:56:13.484622Z",
     "iopub.status.idle": "2026-03-01T02:56:13.763384Z",
     "shell.execute_reply": "2026-03-01T02:56:13.762862Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sorix import tensor\n",
    "from sorix.nn import Dropout\n",
    "import sorix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dropout_example",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T02:56:13.764981Z",
     "iopub.status.busy": "2026-03-01T02:56:13.764841Z",
     "iopub.status.idle": "2026-03-01T02:56:13.768240Z",
     "shell.execute_reply": "2026-03-01T02:56:13.767791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "Output (Training): [[2. 2. 0. 0. 2. 0. 0. 2. 0. 2.]]\n",
      "Output (Evaluation): [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Create a large batch to see the dropout statistics\n",
    "X = tensor(np.ones((1, 10)))\n",
    "print(f\"Input: {X.data}\")\n",
    "\n",
    "dropout = Dropout(p=0.5)\n",
    "dropout.train() # Default mode\n",
    "Y_train = dropout(X)\n",
    "print(f\"Output (Training): {Y_train.data}\")\n",
    "\n",
    "dropout.eval() # Change to inference mode\n",
    "Y_eval = dropout(X)\n",
    "print(f\"Output (Evaluation): {Y_eval.data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}