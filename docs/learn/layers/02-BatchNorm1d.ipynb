{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3126ec69",
   "metadata": {},
   "source": [
    "# BatchNorm1d\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mitchell-Mirano/sorix/blob/develop/docs/learn/layers/02-BatchNorm1d.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/Mitchell-Mirano/sorix/blob/develop/docs/learn/layers/02-BatchNorm1d.ipynb)\n",
    "[![Open in Docs](https://img.shields.io/badge/Open%20in-Docs-blue?logo=readthedocs)](http://127.0.0.1:8000/sorix/learn/layers/02-BatchNorm1d)\n",
    "\n",
    "\n",
    "The **BatchNorm1d** layer implements *batch normalization*, a technique designed to stabilize and accelerate the training of deep neural networks by reducing internal covariate shift. This is achieved by normalizing intermediate activations across the batch dimension and subsequently applying a learnable affine transformation.\n",
    "\n",
    "## Mathematical definition\n",
    "\n",
    "Let $ \\mathbf{X} \\in \\mathbb{R}^{N \\times d} $\n",
    "be an input tensor representing a batch of $N$ samples, where each sample has $d$ features. Batch normalization operates **feature-wise**, normalizing each feature independently across the batch.\n",
    "\n",
    "During training, the batch-wise mean and variance are computed as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_B\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "\\mathbf{x}_i\n",
    "\\;\\in\\;\n",
    "\\mathbb{R}^{1 \\times d},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\sigma}_B^2\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "(\\mathbf{x}_i - \\boldsymbol{\\mu}_B)^2\n",
    "\\;\\in\\;\n",
    "\\mathbb{R}^{1 \\times d},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i \\in \\mathbb{R}^{1 \\times d}$ denotes the $i$-th sample in the batch.\n",
    "\n",
    "## Normalization step\n",
    "\n",
    "Each input sample is normalized using the batch statistics:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{X}}\n",
    "=\n",
    "\\frac{\\mathbf{X} - \\boldsymbol{\\mu}_B}\n",
    "{\\sqrt{\\boldsymbol{\\sigma}_B^2 + \\varepsilon}},\n",
    "\\quad\n",
    "\\widehat{\\mathbf{X}} \\in \\mathbb{R}^{N \\times d},\n",
    "$$\n",
    "\n",
    "where $\\varepsilon > 0$ is a small constant introduced for numerical stability.\n",
    "\n",
    "## Learnable affine transformation\n",
    "\n",
    "To preserve the representational capacity of the network, batch normalization introduces two learnable parameters:\n",
    "\n",
    "- Scale parameter:\n",
    "  $\n",
    "  \\boldsymbol{\\gamma} \\in \\mathbb{R}^{1 \\times d}\n",
    "  $\n",
    "- Shift parameter:\n",
    "  $\n",
    "  \\boldsymbol{\\beta} \\in \\mathbb{R}^{1 \\times d}\n",
    "  $\n",
    "\n",
    "The final output of the layer is given by\n",
    "\n",
    "$$\n",
    "\\mathbf{Y}\n",
    "=\n",
    "\\boldsymbol{\\gamma} \\odot \\widehat{\\mathbf{X}}\n",
    "+\n",
    "\\boldsymbol{\\beta},\n",
    "\\quad\n",
    "\\mathbf{Y} \\in \\mathbb{R}^{N \\times d},\n",
    "$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication applied **column-wise**, i.e., independently to each feature.\n",
    "\n",
    "## Running statistics and inference mode\n",
    "\n",
    "In addition to batch statistics, BatchNorm1d maintains *running estimates* of the mean and variance:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_{\\text{run}} \\in \\mathbb{R}^{1 \\times d},\n",
    "\\quad\n",
    "\\boldsymbol{\\sigma}^2_{\\text{run}} \\in \\mathbb{R}^{1 \\times d}.\n",
    "$$\n",
    "\n",
    "These statistics are updated during training using an exponential moving average:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_{\\text{run}}\n",
    "\\leftarrow\n",
    "\\alpha \\boldsymbol{\\mu}_{\\text{run}}\n",
    "+\n",
    "(1 - \\alpha)\\boldsymbol{\\mu}_B,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\sigma}^2_{\\text{run}}\n",
    "\\leftarrow\n",
    "\\alpha \\boldsymbol{\\sigma}^2_{\\text{run}}\n",
    "+\n",
    "(1 - \\alpha)\\boldsymbol{\\sigma}^2_B,\n",
    "$$\n",
    "\n",
    "where $\\alpha \\in (0,1)$ is the momentum parameter controlling the update rate.\n",
    "\n",
    "During inference (evaluation mode), normalization is performed using these accumulated running statistics instead of the batch statistics, ensuring deterministic behavior:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{X}}\n",
    "=\n",
    "\\frac{\\mathbf{X} - \\boldsymbol{\\mu}_{\\text{run}}}\n",
    "{\\sqrt{\\boldsymbol{\\sigma}^2_{\\text{run}} + \\varepsilon}}.\n",
    "$$\n",
    "\n",
    "## Functional view\n",
    "\n",
    "The BatchNorm1d layer realizes the mapping\n",
    "\n",
    "$$\n",
    "\\text{BatchNorm1d}:\\;\n",
    "\\mathbb{R}^{N \\times d}\n",
    "\\;\\longrightarrow\\;\n",
    "\\mathbb{R}^{N \\times d},\n",
    "$$\n",
    "\n",
    "where normalization and affine reparameterization are applied independently to each feature across the batch.\n",
    "\n",
    "## Parameterization and gradients\n",
    "\n",
    "The learnable parameters $\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\beta}$ are represented as `tensor` objects with `requires_grad=True`. Gradients are computed with respect to these parameters, as well as with respect to the input tensor $\\mathbf{X}$, during backpropagation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\gamma}}, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\beta}}, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}}.\n",
    "$$\n",
    "\n",
    "The running statistics are treated as buffers and do not participate in gradient computation.\n",
    "\n",
    "## Multi-device support\n",
    "\n",
    "BatchNorm1d is device-aware and supports execution on CPU and GPU backends. Learnable parameters are stored as tensors on the selected device, while running statistics are maintained as NumPy or CuPy arrays and transferred consistently when changing devices via the `to(device)` method.\n",
    "\n",
    "## Parameter interface\n",
    "\n",
    "The trainable parameters of the layer are exposed through the `parameters()` method, which returns\n",
    "\n",
    "$$\n",
    "\\{\\boldsymbol{\\gamma}, \\boldsymbol{\\beta}\\}.\n",
    "$$\n",
    "\n",
    "## Statistical interpretation\n",
    "\n",
    "From a statistical perspective, BatchNorm1d performs a feature-wise standardization of the input distribution, followed by a learned affine transformation. This can be interpreted as dynamically re-centering and re-scaling the feature space, which improves numerical conditioning and facilitates optimization in deep networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a519189b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T02:55:59.962611Z",
     "iopub.status.busy": "2026-03-01T02:55:59.962409Z",
     "iopub.status.idle": "2026-03-01T02:55:59.965468Z",
     "shell.execute_reply": "2026-03-01T02:55:59.964861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line and run this cell to install sorix\n",
    "#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@develop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4bbeb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T02:55:59.967162Z",
     "iopub.status.busy": "2026-03-01T02:55:59.967054Z",
     "iopub.status.idle": "2026-03-01T02:56:00.254901Z",
     "shell.execute_reply": "2026-03-01T02:56:00.254296Z"
    }
   },
   "outputs": [],
   "source": [
    "from sorix import tensor\n",
    "from sorix.nn import BatchNorm1d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42553a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T02:56:00.256651Z",
     "iopub.status.busy": "2026-03-01T02:56:00.256487Z",
     "iopub.status.idle": "2026-03-01T02:56:00.260879Z",
     "shell.execute_reply": "2026-03-01T02:56:00.260338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.79076557, -0.09530421, -2.24122608],\n",
       "        [ 0.48085172, -0.62549223, -2.1529319 ],\n",
       "        [-0.13736248,  0.21993719,  0.82125192],\n",
       "        [-0.33432386, -0.21491704,  0.07399757],\n",
       "        [-0.3230639 ,  0.97823966,  0.14454357],\n",
       "        [-0.24306372, -1.85875525,  0.32994193],\n",
       "        [ 1.22507434,  0.33410779, -1.34611515],\n",
       "        [-0.16913842,  0.05868427, -0.04777623]], dtype=sorix.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of samples and features\n",
    "samples = 8\n",
    "features = 3\n",
    "\n",
    "# input tensor X ∈ ℝ^(samples × features)\n",
    "X = tensor(np.random.randn(samples, features))\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b51c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T02:56:00.262246Z",
     "iopub.status.busy": "2026-03-01T02:56:00.262137Z",
     "iopub.status.idle": "2026-03-01T02:56:00.264356Z",
     "shell.execute_reply": "2026-03-01T02:56:00.264089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.]], requires_grad=True)\n",
      "tensor([[0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "bn = BatchNorm1d(features)\n",
    "\n",
    "# γ ∈ ℝ^(1 × features), β ∈ ℝ^(1 × features)\n",
    "print(bn.gamma)\n",
    "print(bn.beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5c7408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T02:56:00.265450Z",
     "iopub.status.busy": "2026-03-01T02:56:00.265361Z",
     "iopub.status.idle": "2026-03-01T02:56:00.267619Z",
     "shell.execute_reply": "2026-03-01T02:56:00.267411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.30578473,  0.07087535, -1.52270086],\n",
       "        [ 0.89556349, -0.61069615, -1.44309716],\n",
       "        [-0.17465216,  0.47612697,  1.23834854],\n",
       "        [-0.51561998, -0.08289027,  0.56464372],\n",
       "        [-0.49612741,  1.45094598,  0.62824614],\n",
       "        [-0.35763586, -2.19609021,  0.79539641],\n",
       "        [ 2.18391742,  0.62289647, -0.71569245],\n",
       "        [-0.22966077,  0.26883185,  0.45485567]], dtype=sorix.float64, requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass (training mode)\n",
    "Y = bn(X)\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e38d44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T02:56:00.268829Z",
     "iopub.status.busy": "2026-03-01T02:56:00.268743Z",
     "iopub.status.idle": "2026-03-01T02:56:00.270510Z",
     "shell.execute_reply": "2026-03-01T02:56:00.270304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0036474 , -0.01504375, -0.05522893]], dtype=sorix.float64)\n",
      "tensor([[0.93336737, 0.96051034, 1.02302517]], dtype=sorix.float64)\n"
     ]
    }
   ],
   "source": [
    "# running statistics after the forward pass\n",
    "print(bn.running_mean)\n",
    "print(bn.running_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbcbc3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T02:56:00.272191Z",
     "iopub.status.busy": "2026-03-01T02:56:00.272101Z",
     "iopub.status.idle": "2026-03-01T02:56:00.274247Z",
     "shell.execute_reply": "2026-03-01T02:56:00.274015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.81472549, -0.0818933 , -2.16124653],\n",
       "        [ 0.5014924 , -0.62286759, -2.07395204],\n",
       "        [-0.13840499,  0.23976144,  0.86655703],\n",
       "        [-0.34227458, -0.20393956,  0.12776335],\n",
       "        [-0.33061969,  1.013491  ,  0.19751061],\n",
       "        [-0.24781359, -1.88122041,  0.38080982],\n",
       "        [ 1.27181782,  0.35625476, -1.27627035],\n",
       "        [-0.17129544,  0.07522796,  0.00736832]], dtype=sorix.float64, requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference mode\n",
    "bn.training = False\n",
    "Y_eval = bn(X)\n",
    "Y_eval\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
