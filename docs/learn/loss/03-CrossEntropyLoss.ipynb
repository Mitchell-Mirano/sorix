{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropyLoss\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mitchell-Mirano/sorix/blob/main/docs/learn/loss/03-CrossEntropyLoss.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/Mitchell-Mirano/sorix/blob/main/docs/learn/loss/03-CrossEntropyLoss.ipynb)\n",
    "[![Open in Docs](https://img.shields.io/badge/Open%20in-Docs-blue?logo=readthedocs)](http://127.0.0.1:8000/sorix/learn/loss/03-CrossEntropyLoss)\n",
    "\n",
    "\n",
    "\n",
    "The **Cross Entropy** loss measures the performance of a classification model whose output is a probability distribution. The goal is to minimize the difference between the predicted distribution and the true distribution.\n",
    "\n",
    "In Sorix, `CrossEntropyLoss` is implemented for multiclass classification. It expects **raw logits** as input and applies the **Softmax** internally.\n",
    "\n",
    "The loss is calculated as the mean over $n$ samples:\n",
    "$$L = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\ln(p_{i,c})$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the batch size.\n",
    "- $C$ is the number of classes.\n",
    "- $y_{i,c}$ is 1 if class $c$ is the correct label for sample $i$, 0 otherwise.\n",
    "- $p_{i,c}$ is the predicted probability for class $c$ of sample $i$ (after Softmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Internal Softmax Integration\n",
    "\n",
    "Combining the Softmax activation and the Cross Entropy loss into a single step is a standard practice in deep learning frameworks. The main reasons are:\n",
    "\n",
    "### Numerical Stability (The Log-Sum-Exp Trick)\n",
    "Softmax involves $e^{x_i}$, which can easily overflow for large positive $x_i$. Sorix uses the Log-Sum-Exp trick to calculate these safely:\n",
    "$$\\text{Softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum e^{x_j - \\max(x)}}$$\n",
    "\n",
    "### Computational Efficiency\n",
    "The mathematical derivative of the combined Softmax + Cross Entropy simplifies beautifully. \n",
    "\n",
    "If we have the loss $L = - \\ln(\\text{Softmax}(x_k))$ where $k$ is the correct class, its derivative simplified is:\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\frac{1}{n}(P_i - Y_i)$$\n",
    "\n",
    "Where:\n",
    "- $P_i$ is the predicted probability for class $i$.\n",
    "- $Y_i$ is 1 if class $i$ is the target, 0 otherwise.\n",
    "- $n$ is the batch size.\n",
    "\n",
    "This means the gradient is just the difference between the prediction and the target, which is extremely cheap to calculate and numerically robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line and run this cell to install sorix\n",
    "#!pip install 'sorix @ git+https://github.com/Mitchell-Mirano/sorix.git@main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:08:09.723338Z",
     "iopub.status.busy": "2026-03-01T06:08:09.723137Z",
     "iopub.status.idle": "2026-03-01T06:08:09.942292Z",
     "shell.execute_reply": "2026-03-01T06:08:09.941703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits sample 1: [2.  1.  0.1] (Highest is label 0)\n",
      "Logits sample 2: [0.  5.  0.2] (Highest is label 1)\n",
      "Cross Entropy Loss: 0.2159\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sorix import tensor\n",
    "from sorix.nn import CrossEntropyLoss\n",
    "\n",
    "# Create logits for 3 classes\n",
    "logits = tensor([[2.0, 1.0, 0.1], [0.0, 5.0, 0.2]], requires_grad=True)\n",
    "targets = tensor([0, 1]) # Class 0 for sample 1, class 1 for sample 2\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(logits, targets)\n",
    "\n",
    "print(f\"Logits sample 1: {logits.numpy()[0]} (Highest is label 0)\")\n",
    "print(f\"Logits sample 2: {logits.numpy()[1]} (Highest is label 1)\")\n",
    "print(f\"Cross Entropy Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Verification\n",
    "\n",
    "As mentioned before, the gradient is just $(P - Y) / n$. Let's verify this in Sorix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:08:09.943730Z",
     "iopub.status.busy": "2026-03-01T06:08:09.943596Z",
     "iopub.status.idle": "2026-03-01T06:08:09.946611Z",
     "shell.execute_reply": "2026-03-01T06:08:09.946201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients w.r.t logits:\n",
      "[[-0.17049944  0.12121648  0.04928295]\n",
      " [ 0.00331929 -0.00737348  0.00405419]]\n",
      "\n",
      "Manual Gradients (P - Y) / n:\n",
      "[[-0.17049944  0.12121648  0.04928295]\n",
      " [ 0.00331929 -0.00737348  0.00405419]]\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(f\"Gradients w.r.t logits:\\n{logits.grad}\")\n",
    "\n",
    "# Manual verification: dL/d_logits = 1/n * (softmax(logits) - targets_one_hot)\n",
    "batch_size = logits.data.shape[0]\n",
    "exp_logits = np.exp(logits.data - np.max(logits.data, axis=-1, keepdims=True))\n",
    "probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "\n",
    "Y_one_hot = np.zeros_like(probs)\n",
    "Y_one_hot[np.arange(batch_size), targets.data.flatten().astype(int)] = 1\n",
    "\n",
    "manual_grad = (probs - Y_one_hot) / batch_size\n",
    "print(f\"\\nManual Gradients (P - Y) / n:\\n{manual_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Example\n",
    "\n",
    "Let's see how `CrossEntropyLoss` helps a simple layer identify the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-03-01T06:08:09.947801Z",
     "iopub.status.busy": "2026-03-01T06:08:09.947708Z",
     "iopub.status.idle": "2026-03-01T06:08:09.955318Z",
     "shell.execute_reply": "2026-03-01T06:08:09.955083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial raw scores: [[-1.129986    1.8432822  -0.88651246]]\n",
      "Step  0 | Loss: 2.8399 | Output: [-1.129986    1.8432822  -0.88651246]\n",
      "Step 10 | Loss: 0.6802 | Output: [-1.2658902   0.4506843   0.64198977]\n",
      "Step 20 | Loss: 0.2603 | Output: [-1.392698   -0.12021631  1.3396983 ]\n",
      "Step 30 | Loss: 0.1513 | Output: [-1.4783971  -0.39979756  1.7049787 ]\n",
      "Step 40 | Loss: 0.1052 | Output: [-1.5416893  -0.57646394  1.944937  ]\n",
      "Step 50 | Loss: 0.0803 | Output: [-1.5918877  -0.70369416  2.1223652 ]\n",
      "\n",
      "Final output: [-1.5963862 -0.714629   2.1377985]\n"
     ]
    }
   ],
   "source": [
    "from sorix.optim import SGD\n",
    "from sorix.nn import Linear\n",
    "\n",
    "x = tensor([[1.0, 0.0, 0.0]]) # Input data\n",
    "target = tensor([2]) # We want it to be class 2\n",
    "\n",
    "model = Linear(3, 3)\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(f\"Initial raw scores: {model(x).numpy()}\")\n",
    "\n",
    "for i in range(51):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if i % 10 == 0:\n",
    "        # The score for index 2 should increase\n",
    "        print(f\"Step {i:2d} | Loss: {loss.item():.4f} | Output: {y_pred.numpy().flatten()}\")\n",
    "\n",
    "print(f\"\\nFinal output: {model(x).numpy().flatten()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}